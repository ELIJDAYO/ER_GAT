{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAFHRFWHRUaXRsZQBwcmlzbSBjb2xvcm1hcPS0PdwAAAAadEVYdERlc2NyaXB0aW9uAHByaXNtIGNvbG9ybWFwCuuUOgAAADB0RVh0QXV0aG9yAE1hdHBsb3RsaWIgdjMuOS4wLCBodHRwczovL21hdHBsb3RsaWIub3Jn8f/jDgAAADJ0RVh0U29mdHdhcmUATWF0cGxvdGxpYiB2My45LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmffWXwpAAAD7ElEQVR4nO3W2W8XZRQG4FckKBcIUSOusaLiQqFFY1xi3JV/2X2LokYDtGUtiCy9kYS4JBCVi3oxZ37SkUbvz/PcnLSZb+bMfHPm9962nqznZo9VXax6sOqBqk9VfXQol7cP9Vj9+2jV5TrrqbWhXlh9OUly/fg7teDVWrAw1KXdSZIHav0TVeervcWcqXa+T5Is5IskybZ8UEfWhe6qP+u0eW7y97NVHx/K9Xtv3f9S1RO/DPXc+YeTJFdOvVcHvFELXqgT7E2S7Li6Jck/j/GZqgfyc/W/VPWrJMn9+biO+CYbjH2Oz3+x6v6qe6s+MpTVrRv7H+vKX0M9c2mol84cSpLcWHmrDnyl+h9OfPvqzptPO7vMfP6odk5V/TZJsi+f1RHjPvw+lN2Tvsf7GPdhfDD1oK7Wvh2b9D/uw8nh8eX8j8OD+fXEu7XgtVrwfC0YXsx7rg9/7qn1+2aXv1jtHKn2vkyS7MxHdcSJoWyd9Dvdh/mqTw5l/cGN/Y79j/dz/NpQz17YkSRZOz3sw/ryuA8vVh3u7461O5PMxixPVz2Q36qN49XW4brPT+uI97PB3Cb9j/M8nrgutLZ9Y//TeT49zvPZl5Ik18Z5PjrOc11guea51m0+zz8kSRbyeZJkWz6sIy8PZcek7+l7NG7sZJ4324fZPP/0UJLkyslDteD1WjDO8/ChG+d5rtaPr+1CrlQ7x6q9r4f7nc3z4WwwLjw4qf8xz2Pfm83zxdXhe3Rj+e1aUPN8dHhBt6zuuvm0s8vsz5/V98lq57skyfy/5nl433LfJv2P79F0nnfeuv/ZPA+PL+fPTeZ5qeb5SH24l+aSJHfXPNc2zz6Pi7lU7QzzvFDzvGs2z8Oc/O95rhd1fXg9sjzpf7yflU3n+c1aUPN8rOa5fijHeR5/Rg/U9/JgVqoOvwN78kkdMe5DDdLcpO/pPmwyz2Pf03neEgCgHQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgob8BkSHN0SfthVwAAAAASUVORK5CYII=\n",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>prism</strong> </div><div class=\"cmap\"><img alt=\"prism colormap\" title=\"prism\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAFHRFWHRUaXRsZQBwcmlzbSBjb2xvcm1hcPS0PdwAAAAadEVYdERlc2NyaXB0aW9uAHByaXNtIGNvbG9ybWFwCuuUOgAAADB0RVh0QXV0aG9yAE1hdHBsb3RsaWIgdjMuOS4wLCBodHRwczovL21hdHBsb3RsaWIub3Jn8f/jDgAAADJ0RVh0U29mdHdhcmUATWF0cGxvdGxpYiB2My45LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmffWXwpAAAD7ElEQVR4nO3W2W8XZRQG4FckKBcIUSOusaLiQqFFY1xi3JV/2X2LokYDtGUtiCy9kYS4JBCVi3oxZ37SkUbvz/PcnLSZb+bMfHPm9962nqznZo9VXax6sOqBqk9VfXQol7cP9Vj9+2jV5TrrqbWhXlh9OUly/fg7teDVWrAw1KXdSZIHav0TVeervcWcqXa+T5Is5IskybZ8UEfWhe6qP+u0eW7y97NVHx/K9Xtv3f9S1RO/DPXc+YeTJFdOvVcHvFELXqgT7E2S7Li6Jck/j/GZqgfyc/W/VPWrJMn9+biO+CYbjH2Oz3+x6v6qe6s+MpTVrRv7H+vKX0M9c2mol84cSpLcWHmrDnyl+h9OfPvqzptPO7vMfP6odk5V/TZJsi+f1RHjPvw+lN2Tvsf7GPdhfDD1oK7Wvh2b9D/uw8nh8eX8j8OD+fXEu7XgtVrwfC0YXsx7rg9/7qn1+2aXv1jtHKn2vkyS7MxHdcSJoWyd9Dvdh/mqTw5l/cGN/Y79j/dz/NpQz17YkSRZOz3sw/ryuA8vVh3u7461O5PMxixPVz2Q36qN49XW4brPT+uI97PB3Cb9j/M8nrgutLZ9Y//TeT49zvPZl5Ik18Z5PjrOc11guea51m0+zz8kSRbyeZJkWz6sIy8PZcek7+l7NG7sZJ4324fZPP/0UJLkyslDteD1WjDO8/ChG+d5rtaPr+1CrlQ7x6q9r4f7nc3z4WwwLjw4qf8xz2Pfm83zxdXhe3Rj+e1aUPN8dHhBt6zuuvm0s8vsz5/V98lq57skyfy/5nl433LfJv2P79F0nnfeuv/ZPA+PL+fPTeZ5qeb5SH24l+aSJHfXPNc2zz6Pi7lU7QzzvFDzvGs2z8Oc/O95rhd1fXg9sjzpf7yflU3n+c1aUPN8rOa5fijHeR5/Rg/U9/JgVqoOvwN78kkdMe5DDdLcpO/pPmwyz2Pf03neEgCgHQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgIQEAABoSAACgob8BkSHN0SfthVwAAAAASUVORK5CYII=\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#ff0000ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #ff0000ff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#54ff00ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #54ff00ff;\"></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.LinearSegmentedColormap at 0x1d1ef5c88e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "from vocab import Vocab, Vectors\n",
    "from wordebd import WORDEBD\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.colormaps.get_cmap('prism')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12840, 12)\n",
      "(3400, 12)\n",
      "(1462, 12)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset_drop_noise\"\n",
    "\n",
    "# Read the CSV file\n",
    "TrainData = pd.read_csv('data/dataset_original/NOT_FOR_TRAINING/train_sent_emo_dya.csv', encoding='shift_jis')\n",
    "TestData = pd.read_csv('data/dataset_original/NOT_FOR_TRAINING/test_sent_emo_dya.csv', encoding='utf-8')\n",
    "DevData = pd.read_csv('data/dataset_original/NOT_FOR_TRAINING/dev_sent_emo_dya.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first three rows\n",
    "print(TrainData.shape)\n",
    "print(TestData.shape)\n",
    "print(DevData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Old_Dialogue_ID, Old_Utterance_ID, Season, Episode, StartTime, and EndTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Old_Dialogue_ID',\n",
       " 'Old_Utterance_ID',\n",
       " 'Season',\n",
       " 'Episode',\n",
       " 'StartTime',\n",
       " 'EndTime']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features to drop\n",
    "drop_features = list(TrainData.columns[6:]) \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features from X_train DataFrame\n",
    "TrainData = TrainData.drop(drop_features, axis=1)\n",
    "TestData = TestData.drop(drop_features, axis=1)\n",
    "DevData = DevData.drop(drop_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = True\n",
    "\n",
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_encoder.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_decoder.pkl\")\n",
    "\n",
    "if not (checkFile1 and checkFile2):\n",
    "    labels = sorted(set(TrainData.Emotion))\n",
    "    labelEncoder = {label: i for i, label in enumerate(labels)}\n",
    "    labelDecoder = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    pickle.dump(labelEncoder, open('data/dump/' + dataset_path + '/label_encoder.pkl', 'wb'))\n",
    "    pickle.dump(labelDecoder, open('data/dump/' + dataset_path + '/label_decoder.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('data/dump/' + dataset_path + '/label_encoder.pkl', 'rb')\n",
    "    file2 = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "    labelEncoder = pickle.load(file1)\n",
    "    labelDecoder = pickle.load(file2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'joy': 3,\n",
       " 'neutral': 4,\n",
       " 'sadness': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the \"Emotion\" column in y_train\n",
    "TrainData[\"Emotion\"] = TrainData[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "TestData[\"Emotion\"] = TestData[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "DevData[\"Emotion\"] = DevData[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_value_ranges(lst):\n",
    "    value_ranges = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            value_ranges.append((start_index, i - 1))\n",
    "            start_index = i\n",
    "\n",
    "    # Add the last range\n",
    "    value_ranges.append((start_index, len(lst) - 1))\n",
    "\n",
    "    return value_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(TrainData[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(TestData[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(DevData[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying orginal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will copy the current dataframe to be ready for export and stored at dataset_original since dataset_original will NOT have any pre-processing of utterances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData_Original =  TrainData.copy()\n",
    "TestData_Original = TestData.copy()\n",
    "DevData_Original = DevData.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edayo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edayo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\edayo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Make a copy of X_train_utterances\n",
    "train_utterances = TrainData['Utterance']\n",
    "test_utterances = TestData['Utterance']\n",
    "dev_utterances = DevData['Utterance']\n",
    "\n",
    "# Initialize the lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_stop_words = set([\n",
    "    'a', 'an', 'the', 'and', 'but', 'or', 'nor', 'for', 'yet', 'so',\n",
    "    'at', 'by', 'for', 'in', 'of', 'on', 'to', 'with', 'about', 'above',\n",
    "    'across', 'after', 'against', 'along', 'among', 'around', 'as', 'before',\n",
    "    'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'during',\n",
    "    'except', 'from', 'inside', 'into', 'near', 'outside', 'over', 'through',\n",
    "    'under', 'until', 'up', 'upon', 'within', 'without', 'I', 'me', 'my',\n",
    "    'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "    'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "    'themselves', 'am', 'is', 'are', 'was', 'were', 'be', 'being', 'been',\n",
    "    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'get',\n",
    "    'go', 'make', 'take', 'want', 'know', 'see', 'come', 'think', 'look',\n",
    "    'use', 'find', 'give', 'tell', 'work', 'call', 'try', 'ask', 'need',\n",
    "    'feel', 'become', 'leave', 'put', 'mean', 'keep', 'let', 'begin',\n",
    "    'seem', 'help', 'talk', 'turn', 'start', 'show', 'hear', 'play', 'run',\n",
    "    'move', 'like', 'live', 'believe', 'hold', 'bring', 'happen', 'write',\n",
    "    'provide', 'sit', 'stand', 'lose', 'pay', 'meet', 'include', 'continue',\n",
    "    'set', 'learn', 'change', 'lead', 'understand', 'watch', 'follow',\n",
    "    'stop', 'create', 'speak', 'read', 'allow', 'add', 'spend', 'grow',\n",
    "    'open', 'walk', 'win', 'offer', 'remember', 'love', 'consider',\n",
    "    'appear', 'buy', 'serve', 'die', 'send', 'expect', 'build', 'stay',\n",
    "    'fall', 'cut', 'reach', 'kill', 'remain', 'suggest', 'raise', 'pass',\n",
    "    'sell', 'require', 'report', 'decide', 'pull', 'become', 'very',\n",
    "    'really', 'too', 'quite', 'much', 'so', 'just', 'even', 'now', 'then',\n",
    "    'here', 'there', 'when', 'where', 'why', 'how', 'well', 'also', 'such',\n",
    "    'still', 'only', 'always', 'never', 'ever', 'often', 'usually',\n",
    "    'sometimes', 'once', 'this', 'that', 'these', 'those', 'each', 'every',\n",
    "    'all', 'some', 'any', 'both', 'few', 'many', 'much', 'more', 'most',\n",
    "    'other', 'another', 'such', 'same', 'own', 'what', 'which', 'either',\n",
    "    'neither', 'not', 'no', \"n't\", 'nor', 'oh', 'ah', 'uh', 'um'\n",
    "])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters, and stop words\n",
    "    words = [word for word in words if word.isalpha() and word not in custom_stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Apply the clean_text function to each utterance\n",
    "train_utterances = train_utterances.apply(clean_text)\n",
    "test_utterances = test_utterances.apply(clean_text)\n",
    "dev_utterances = dev_utterances.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i point person company s transition system\n",
       "1                             must ve hand full\n",
       "2                                           i i\n",
       "3                             s little bit duty\n",
       "4                                    duty right\n",
       "Name: Utterance, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_utterances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          re coffee mug number bottom\n",
       "1    s monica can track way if one missing can s nu...\n",
       "2                                                     \n",
       "3                                                 okay\n",
       "4                                    ross say elevator\n",
       "Name: Utterance, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_utterances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              god s lost s totally lost\n",
       "1                                       \n",
       "2    could bank close account off source\n",
       "3                              re genius\n",
       "4                              re genius\n",
       "Name: Utterance, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_utterances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty in Train Utterances: 940\n",
      "Empty in Test Utterances: 239\n",
      "Empty in Dev Utterances: 122\n"
     ]
    }
   ],
   "source": [
    "# Count the empty string values in X_train_utterances\n",
    "empty_string_count_train = (train_utterances == '').sum()\n",
    "empty_string_count_test = (test_utterances == '').sum()\n",
    "empty_string_count_dev = (dev_utterances == '').sum()\n",
    "\n",
    "print(f\"Empty in Train Utterances: {empty_string_count_train}\")\n",
    "print(f\"Empty in Test Utterances: {empty_string_count_test}\")\n",
    "print(f\"Empty in Dev Utterances: {empty_string_count_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update 'Utterance' column and remove all empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (11900, 6)\n",
      "Test data shape: (3161, 6)\n",
      "Dev data shape: (1340, 6)\n"
     ]
    }
   ],
   "source": [
    "# Update X_train with the cleaned utterances\n",
    "TrainData['Utterance'] = train_utterances\n",
    "TestData['Utterance'] = test_utterances\n",
    "DevData['Utterance'] = dev_utterances\n",
    "\n",
    "# Drop rows where Utterance is an empty string. Empty string value is ''.\n",
    "TrainData = TrainData[TrainData['Utterance'] != '']\n",
    "TestData = TestData[TestData['Utterance'] != '']\n",
    "DevData = DevData[DevData['Utterance'] != '']\n",
    "\n",
    "# Reset the index\n",
    "TrainData.reset_index(drop=True, inplace=True)\n",
    "TestData.reset_index(drop=True, inplace=True)\n",
    "DevData.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Train data shape: {TrainData.shape}\")\n",
    "print(f\"Test data shape: {TestData.shape}\")\n",
    "print(f\"Dev data shape: {DevData.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Take note of new shape of Train and Test Data after dropping the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with *n* or below number of words and create a new **Dropped Words Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing rows that is 2 words or below:\n",
      "Train shape : (7359, 6)\n",
      "Test shape: (1981, 6)\n",
      "Test shape: (858, 6)\n"
     ]
    }
   ],
   "source": [
    "n = 2 # This number removes rows with n or less words. (Retains rows with more than n words)\n",
    "\n",
    "# Retain rows with more than n words\n",
    "TrainData = TrainData[TrainData['Utterance'].apply(lambda x: len(x.split()) > n)]\n",
    "TestData = TestData[TestData['Utterance'].apply(lambda x: len(x.split()) > n)]\n",
    "DevData = DevData[DevData['Utterance'].apply(lambda x: len(x.split()) > n)]\n",
    "\n",
    "TrainData.reset_index(drop=True, inplace=True)\n",
    "TestData.reset_index(drop=True, inplace=True)\n",
    "DevData.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"After removing rows that is {n} words or below:\")\n",
    "print(f\"Train shape : {TrainData.shape}\")\n",
    "print(f\"Test shape: {TestData.shape}\")\n",
    "print(f\"Test shape: {DevData.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get random sample from train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5813</th>\n",
       "      <td>i m gon na joey re back i worried</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>gon na gon na</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>d lot trouble don t country</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>i m hoping uterus next nine month</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6957</th>\n",
       "      <td>say used eat snack</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>i down store twenty minute trying tie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>re gentleman re going place</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>y i couldn t concentrate i blushed time looked</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7108</th>\n",
       "      <td>pheebs who george snuffalopagus</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>i d ya i tucked</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance  Emotion\n",
       "5813               i m gon na joey re back i worried        0\n",
       "3956                                   gon na gon na        2\n",
       "3501                     d lot trouble don t country        4\n",
       "5396               i m hoping uterus next nine month        3\n",
       "6957                              say used eat snack        4\n",
       "1032           i down store twenty minute trying tie        0\n",
       "6146                     re gentleman re going place        3\n",
       "5195  y i couldn t concentrate i blushed time looked        4\n",
       "7108                 pheebs who george snuffalopagus        4\n",
       "1291                                 i d ya i tucked        4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 10 rows\n",
    "random_sample = TrainData.sample(n=10)[[\"Utterance\", \"Emotion\"]]\n",
    "\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>guy i m sorry could please little slower</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>okay okay rach down scrunchy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>photographer who seemed dull</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>sure s hard forget</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>s pregnant s rachel rachel s one who s pregnant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>i ca lokk nice might doctor</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>thing going paul</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>hospital i m going sue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>will guy who peed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>hey chandler don t ranger game tomorrow</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Utterance  Emotion\n",
       "1382         guy i m sorry could please little slower        4\n",
       "1111                     okay okay rach down scrunchy        4\n",
       "1745                     photographer who seemed dull        4\n",
       "1804                               sure s hard forget        4\n",
       "1228  s pregnant s rachel rachel s one who s pregnant        4\n",
       "1826                      i ca lokk nice might doctor        4\n",
       "1573                                 thing going paul        4\n",
       "533                            hospital i m going sue        0\n",
       "425                                 will guy who peed        1\n",
       "1820          hey chandler don t ranger game tomorrow        4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 10 rows\n",
    "random_sample = TestData.sample(n=10)[[\"Utterance\", \"Emotion\"]]\n",
    "\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>i great julie i found out</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>junior miss i started i sleep</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>guy cute guy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>pete becker circling ring look s trying out br...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>doesn t smell opium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>check out s winning pete s winning</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>pizza pizza out</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>ross silly i tried big deal taste</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>child coming world building negative fighting ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>ralph mumble re paying attention</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Utterance  Emotion\n",
       "38                           i great julie i found out        0\n",
       "503                      junior miss i started i sleep        4\n",
       "522                                       guy cute guy        4\n",
       "311  pete becker circling ring look s trying out br...        4\n",
       "728                                doesn t smell opium        1\n",
       "317                 check out s winning pete s winning        6\n",
       "399                                    pizza pizza out        0\n",
       "742                  ross silly i tried big deal taste        3\n",
       "380  child coming world building negative fighting ...        0\n",
       "531                   ralph mumble re paying attention        4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 10 rows\n",
    "random_sample = DevData.sample(n=10)[[\"Utterance\", \"Emotion\"]]\n",
    "\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cm' has no attribute 'get_cmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_words\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Corrected WordCloud generation for all datasets\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m WC_Train \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(TrainData\u001b[38;5;241m.\u001b[39mUtterance))\n\u001b[0;32m     13\u001b[0m WC_Test \u001b[38;5;241m=\u001b[39m WordCloud(\n\u001b[0;32m     14\u001b[0m     max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, min_font_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1600\u001b[39m, background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m )\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(TestData\u001b[38;5;241m.\u001b[39mUtterance))\n\u001b[0;32m     17\u001b[0m WC_Dev \u001b[38;5;241m=\u001b[39m WordCloud(\n\u001b[0;32m     18\u001b[0m     max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, min_font_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1600\u001b[39m, background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(DevData\u001b[38;5;241m.\u001b[39mUtterance))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wordcloud\\wordcloud.py:337\u001b[0m, in \u001b[0;36mWordCloud.__init__\u001b[1;34m(self, font_path, width, height, margin, ranks_only, prefer_horizontal, mask, scale, color_func, max_words, min_font_size, stopwords, random_state, background_color, max_font_size, font_step, mode, relative_scaling, regexp, collocations, colormap, normalize_plurals, contour_width, contour_color, repeat, include_numbers, min_word_length, collocation_threshold)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontour_width \u001b[38;5;241m=\u001b[39m contour_width\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m scale\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolor_func \u001b[38;5;241m=\u001b[39m color_func \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mcolormap_color_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolormap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_words \u001b[38;5;241m=\u001b[39m max_words\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m stopwords \u001b[38;5;28;01mif\u001b[39;00m stopwords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m STOPWORDS\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wordcloud\\wordcloud.py:106\u001b[0m, in \u001b[0;36mcolormap_color_func.__init__\u001b[1;34m(self, colormap)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, colormap):\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolormap \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cmap\u001b[49m(colormap)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.cm' has no attribute 'get_cmap'"
     ]
    }
   ],
   "source": [
    "# WORD CLOUDS\n",
    "# Function to get the top 10 words from a word cloud\n",
    "def get_top_words(wordcloud):\n",
    "    word_freq = wordcloud.words_\n",
    "    top_words = [word for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]]\n",
    "    return top_words\n",
    "\n",
    "# Corrected WordCloud generation for all datasets\n",
    "WC_Train = WordCloud(\n",
    "    max_words=2000, min_font_size=10, height=800, width=1600, background_color=\"white\"\n",
    ").generate(\" \".join(TrainData.Utterance))\n",
    "\n",
    "WC_Test = WordCloud(\n",
    "    max_words=2000, min_font_size=10, height=800, width=1600, background_color=\"white\"\n",
    ").generate(\" \".join(TestData.Utterance))\n",
    "\n",
    "WC_Dev = WordCloud(\n",
    "    max_words=2000, min_font_size=10, height=800, width=1600, background_color=\"white\"\n",
    ").generate(\" \".join(DevData.Utterance))\n",
    "\n",
    "# Get top 10 words for each dataset\n",
    "top_words_train = get_top_words(WC_Train)\n",
    "top_words_test = get_top_words(WC_Test)\n",
    "top_words_dev = get_top_words(WC_Dev)\n",
    "\n",
    "# Print the top words\n",
    "print(\"Top words in TrainData:\", top_words_train)\n",
    "print(\"Top words in TestData:\", top_words_test)\n",
    "print(\"Top words in DevData:\", top_words_dev)\n",
    "\n",
    "# PLOT\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 30))  # Adjust the layout and size as needed\n",
    "axes = axes.flatten()  # Flatten the 2D array to 1D for easier indexing\n",
    "\n",
    "wordclouds = [WC_Train, WC_Test, WC_Dev]\n",
    "titles = ['TrainData', 'TestData', 'DevData']\n",
    "\n",
    "# Plot each WordCloud\n",
    "for idx, wc in enumerate(wordclouds):\n",
    "    axes[idx].imshow(wc, interpolation='bilinear')\n",
    "    axes[idx].set_title(titles[idx], fontsize=16)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Remove any empty subplots (if applicable)\n",
    "for idx in range(len(wordclouds), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division of X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for target labels\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "y_dev = pd.DataFrame()\n",
    "\n",
    "X_train = TrainData\n",
    "X_test = TestData\n",
    "X_dev = DevData\n",
    "    \n",
    "y_train[\"Emotion\"] = TrainData[\"Emotion\"].copy()\n",
    "y_test[\"Emotion\"] = TestData[\"Emotion\"].copy()\n",
    "y_dev[\"Emotion\"] = DevData[\"Emotion\"].copy()\n",
    "\n",
    "y_train[\"Dialogue_ID\"] = TrainData[\"Dialogue_ID\"].copy()\n",
    "y_test[\"Dialogue_ID\"] = TestData[\"Dialogue_ID\"].copy()\n",
    "y_dev[\"Dialogue_ID\"] = DevData[\"Dialogue_ID\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (7359, 6)\n",
      "Shape of y_train: (7359, 2)\n",
      "--\n",
      "Shape of X_test: (1981, 6)\n",
      "Shape of y_test: (1981, 2)\n",
      "--\n",
      "Shape of X_dev: (858, 6)\n",
      "Shape of y_dev: (858, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print('--')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')\n",
    "print('--')\n",
    "print(f'Shape of X_dev: {X_dev.shape}')\n",
    "print(f'Shape of y_dev: {y_dev.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data to new csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label Encoding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the file already exists\n",
    "# checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_train.pkl\")\n",
    "# checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_test.pkl\")\n",
    "\n",
    "# if key:\n",
    "#     pickle.dump(X_train[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_train.pkl', 'wb'))\n",
    "#     pickle.dump(X_test[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Noise Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportDataToCSV(df, name):\n",
    "    path = \"data/\" + dataset_path + \"/\" + name + \".csv\"\n",
    "    df.to_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i point person company s transition system</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>must ve hand full</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s little bit duty</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ll heading whole division ll lot duty</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ll perhaps people can dump certain amount</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7354</th>\n",
       "      <td>guy messing right</td>\n",
       "      <td>Joey</td>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "      <td>2158</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>i got ta side chandler one</td>\n",
       "      <td>Joey</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>i first moved city i went out couple time girl...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>2159</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7357</th>\n",
       "      <td>guy messing right</td>\n",
       "      <td>Joey</td>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "      <td>2159</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>good one second i whoa</td>\n",
       "      <td>Joey</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>2159</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7359 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Utterance          Speaker  \\\n",
       "0            i point person company s transition system         Chandler   \n",
       "1                                     must ve hand full  The Interviewer   \n",
       "2                                     s little bit duty  The Interviewer   \n",
       "3                 ll heading whole division ll lot duty  The Interviewer   \n",
       "4             ll perhaps people can dump certain amount  The Interviewer   \n",
       "...                                                 ...              ...   \n",
       "7354                                  guy messing right             Joey   \n",
       "7355                         i got ta side chandler one             Joey   \n",
       "7356  i first moved city i went out couple time girl...             Joey   \n",
       "7357                                  guy messing right             Joey   \n",
       "7358                             good one second i whoa             Joey   \n",
       "\n",
       "      Emotion Sentiment  Dialogue_ID  Utterance_ID  \n",
       "0           4   neutral            0             0  \n",
       "1           4   neutral            0             1  \n",
       "2           4   neutral            0             3  \n",
       "3           4   neutral            0             5  \n",
       "4           4   neutral            0             7  \n",
       "...       ...       ...          ...           ...  \n",
       "7354        6  positive         2158             5  \n",
       "7355        4   neutral         2159             1  \n",
       "7356        1  negative         2159             2  \n",
       "7357        6  positive         2159             4  \n",
       "7358        3  positive         2159             6  \n",
       "\n",
       "[7359 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportDataToCSV(X_train, \"X_train\")\n",
    "exportDataToCSV(y_train, \"y_train\")\n",
    "\n",
    "exportDataToCSV(X_test, \"X_test\")\n",
    "exportDataToCSV(y_test, \"y_test\")\n",
    "\n",
    "exportDataToCSV(X_dev, \"X_dev\")\n",
    "exportDataToCSV(y_dev, \"y_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportDataToCSV(TrainData_Original, \"X_train\")\n",
    "# exportDataToCSV(TrainData_Original['Emotion'], \"y_train\")\n",
    "\n",
    "# exportDataToCSV(TestData_Original, \"X_test\")\n",
    "# exportDataToCSV(TestData_Original['Emotion'], \"y_test\")\n",
    "\n",
    "# exportDataToCSV(DevData_Original, \"X_dev\")\n",
    "# exportDataToCSV(DevData_Original['Emotion'], \"y_dev\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
