{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3cd42",
   "metadata": {},
   "source": [
    "Put libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a88b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "553916bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch, os, pickle, sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# from GAT import GAT\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a0066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Path: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\\utils\\constans.py\n",
      "Project Directory: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\n"
     ]
    }
   ],
   "source": [
    "script_path = os.path.abspath(\"utils\\\\constans.py\")  # Replace __file__ with the path to your script if in a notebook\n",
    "\n",
    "# Determine the project directory by moving up two levels (adjust as needed)\n",
    "project_directory = os.path.dirname(os.path.dirname(script_path))\n",
    "\n",
    "print(\"Script Path:\", script_path)\n",
    "print(\"Project Directory:\", project_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc1152",
   "metadata": {},
   "source": [
    "Code related to GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecf3858",
   "metadata": {
    "code_folding": [
     1,
     6,
     17
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class DialogueGraphDataLoader(DataLoader):\n",
    "    def __init__(self, node_features_list, edge_index_list, batch_size=1, shuffle=False):\n",
    "        graph_dataset = DialogueGraphDataset(node_features_list, edge_index_list)\n",
    "        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=dialogue_graph_collate_fn)\n",
    "\n",
    "class DialogueGraphDataset(Dataset):\n",
    "    def __init__(self, node_features_list, edge_index_list):\n",
    "        self.node_features_list = node_features_list\n",
    "        self.edge_index_list = edge_index_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.edge_index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.node_features_list[idx], self.edge_index_list[idx]\n",
    "\n",
    "def dialogue_graph_collate_fn(batch):\n",
    "    node_features_list, edge_index_list = zip(*batch)\n",
    "    \n",
    "    node_features_list_combined = []\n",
    "    num_nodes_seen = 0\n",
    "\n",
    "    for node_features, edge_index in zip(node_features_list, edge_index_list):\n",
    "        # Assuming node_features is a tuple (text_embeddings, speakers_list)\n",
    "        text_embeddings, speakers_list = node_features\n",
    "        combined_features = (text_embeddings, speakers_list)\n",
    "\n",
    "        node_features_list_combined.append(combined_features)\n",
    "\n",
    "        # Translate the range of edge_index\n",
    "        edge_index_list.append(edge_index + num_nodes_seen)\n",
    "        num_nodes_seen += len(text_embeddings)\n",
    "\n",
    "    # Merge the dialogue graphs into a single graph with multiple connected components\n",
    "    node_features_combined = [torch.cat(features, 1) for features in zip(*node_features_list_combined)]\n",
    "    edge_index = torch.cat(edge_index_list, 1)\n",
    "\n",
    "    return node_features_combined, edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "75f4cb1c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features, num_out_features, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "\n",
    "        self.num_in_features_per_head = num_in_features // num_heads\n",
    "        self.num_out_features_per_head = num_out_features // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for each head\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head, bias=False)\n",
    "        # Define the final linear projection layer\n",
    "        self.final_linear_proj = nn.Linear(num_heads * self.num_out_features_per_head, num_in_features)\n",
    "\n",
    "        # Edge type embedding\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.edge_type_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.final_linear_proj.weight)\n",
    "        nn.init.zeros_(self.attention_weights_self)\n",
    "        nn.init.zeros_(self.attention_weights_edge_types)\n",
    "\n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Assuming the last dimension of h_linear is the one to be multiplied with attention_coefficients\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).unsqueeze(1)\n",
    "\n",
    "        # Ensure that the dimensions are compatible for broadcasting\n",
    "        # Expand the dimensions of edge_type_embedding to match h_linear\n",
    "        edge_type_embedding = edge_type_embedding.unsqueeze(-1).expand(-1, -1, -1, h_linear.size(-1))\n",
    "\n",
    "        # Perform element-wise multiplication\n",
    "        attention_scores = (h_linear.unsqueeze(2) * edge_type_embedding.unsqueeze(1)).sum(dim=-1)\n",
    "\n",
    "        # Continue with the rest of your attention mechanism\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Adjust dimensions for torch.matmul\n",
    "        attention_coefficients = attention_coefficients.unsqueeze(-2)  # Add an extra dimension before the last dimension\n",
    "\n",
    "        print(\"attention_coefficients size:\", attention_coefficients.size())\n",
    "        print(\"h_linear size:\", h_linear.size())\n",
    "        # Weighted sum using attention coefficients\n",
    "        # Assuming attention_coefficients and h_linear have correct dimensions\n",
    "        # Perform matrix multiplication step by step\n",
    "\n",
    "        # Transpose h_linear to make it compatible with matmul\n",
    "        h_linear_transposed = h_linear.transpose(-2, -1)\n",
    "\n",
    "        \n",
    "        # Reshape attention_coefficients for matmul\n",
    "#         attention_coefficients_reshaped = attention_coefficients.view(-1, attention_coefficients.size(-1))\n",
    "        \n",
    "        # Convert tensors to float16\n",
    "        attention_coefficients_reshaped_transposed = attention_coefficients.transpose(-2, -1).unsqueeze(-3).to(torch.float16)\n",
    "        h_linear_reshaped = h_linear_reshaped.unsqueeze(-1).to(torch.float16)\n",
    "\n",
    "        # Perform batch matrix multiplication\n",
    "        h_prime_flat = torch.bmm(attention_coefficients_reshaped_transposed, h_linear_reshaped).squeeze(-1)\n",
    "\n",
    "        # Convert back to float32 if necessary\n",
    "        h_prime_flat = h_prime_flat.to(torch.float32)\n",
    "\n",
    "        # Reshape h_prime to the desired shape\n",
    "        h_prime = h_prime_flat.view(attention_coefficients.size(0), attention_coefficients.size(1), attention_coefficients.size(2), -1)\n",
    "\n",
    "\n",
    "        # Final linear projection\n",
    "        h_out = self.final_linear_proj(h_prime.view(-1, self.num_heads * self.num_out_features_per_head))\n",
    "\n",
    "\n",
    "        return h_out, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e3ed3",
   "metadata": {},
   "source": [
    "<h3>Methods definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c53e5417",
   "metadata": {
    "code_folding": [
     0,
     20,
     42,
     58
    ]
   },
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    # Initialize an empty list to store pairs\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "#     node_pairs_dict = {}\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 3\n",
    "        while(val >= 0):\n",
    "            target_idx = i-val\n",
    "#                 print(target_idx)\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "#                 node_pairs_dict[i] = target_idx\n",
    "            val = val-1\n",
    "    \n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "\n",
    "    # Iterate through pairs of nodes\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "\n",
    "#         # Add source node to target node's neighbors\n",
    "#         if target_node not in adjacency_list_dict:\n",
    "#             adjacency_list_dict[target_node] = [source_node]\n",
    "#         else:\n",
    "#             adjacency_list_dict[target_node].append(source_node)\n",
    "\n",
    "        # Add target node to source node's neighbors\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "\n",
    "    return adjacency_list_dict\n",
    "# print(ranges[:1])\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        \n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    \n",
    "    for i in range(dialogs_len): #2140 dialogs\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "#         print(i, \" th dialogue\")\n",
    "#         print(i, dialog_speakers)\n",
    "        for j in range(dialog_len): #num utterances\n",
    "            src_node = dialog_nodes_pairs[j] # j = key = src node\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k] # k = value = targ node\n",
    "                # edge_types = torch.tensor([0, 1, 2]) \n",
    "                # 0: cur-self, 1: past-self, 2: past-other/past-inter\n",
    "                                \n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "#                     print(\"This is 0 \", node_i_idx, node_j_idx)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "#                         print(\"This is 1 \", node_i_idx, node_j_idx)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "#                         print(\"This is 2 \", node_i_idx, node_j_idx)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))  \n",
    "                    \n",
    "    return whole_edge_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a199d8f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print(edge_indices[0][0][3])\n",
    "# len(edge_indices[0].keys())\n",
    "# list(encoded_speaker_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbacda",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# assume this is working\n",
    "# edge_indices = get_all_adjacency_list(ranges)\n",
    "# edge_types = get_all_edge_type_list(edge_indices, encoded_speaker_list)\n",
    "# edge_indices = get_all_adjacency_list(ranges, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2ae80",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print((edge_types[:10]))\n",
    "# edge_indices[:10]\n",
    "# (updated_representations[0].shape)\n",
    "# edge_indices[0]\n",
    "# edge_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20cb34a8",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder.pkl\")\n",
    "encoded_speaker_list = []\n",
    "if checkFile is False:\n",
    "    print(\"Run first the prototype_context_encoder to generate this file\")\n",
    "else:\n",
    "    file = open('data/dump/speaker_encoder.pkl', \"rb\")\n",
    "    encoded_speaker_list, ranges = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bc790",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# need update\n",
    "# checkFile = os.path.isfile(\"data/dump/all_adjacency_list.pkl\")\n",
    "# adjacency_list = []\n",
    "# if checkFile is False:\n",
    "#     adjacency_list = get_all_adjacency_list(ranges)\n",
    "# else:\n",
    "#     file = open('data/dump/all_adjacency_list.pkl', \"rb\")\n",
    "#     adjacency_list = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacency_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f02158e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "file_path = 'embed/updated_representation_list.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    updated_representations = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "293f6d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 300])\n",
      "tensor([[-2.8721e-01,  5.8134e-01, -1.3142e-01,  ...,  1.8101e-02,\n",
      "         -4.6824e-04,  1.9901e-02],\n",
      "        [-1.6920e-01,  1.8220e-01, -1.2245e-01,  ...,  1.3620e-02,\n",
      "         -2.0732e-03,  8.3473e-03],\n",
      "        [-8.1502e-02,  7.7161e-02, -6.6144e-02,  ...,  1.3882e-02,\n",
      "          3.4588e-03, -1.4834e-03],\n",
      "        ...,\n",
      "        [-4.1162e-03,  2.6335e-02,  2.8706e-02,  ..., -1.6475e-01,\n",
      "         -1.3978e-01,  2.8344e-02],\n",
      "        [-1.7579e-02,  1.8380e-02,  3.3130e-02,  ..., -2.5659e-01,\n",
      "         -2.2489e-01,  1.5857e-02],\n",
      "        [-2.9680e-02,  8.5039e-03,  3.3814e-02,  ..., -3.8804e-01,\n",
      "         -2.8153e-01,  1.1250e-03]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(updated_representations[0].shape)\n",
    "print(updated_representations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48400c3a",
   "metadata": {},
   "source": [
    "<h3> Making Progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3af5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = get_all_adjacency_list(ranges)\n",
    "edge_types = get_all_edge_type_list(edge_indices, encoded_speaker_list)\n",
    "edge_indices = get_all_adjacency_list(ranges, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_heads_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ed5fed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_coefficients size: torch.Size([50, 56, 4, 1, 4])\n",
      "h_linear size: torch.Size([56, 4, 300])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'h_linear_reshaped' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [302]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m gat_model \u001b[38;5;241m=\u001b[39m GATWithEdgeType(num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types)\n\u001b[0;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m output, attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mgat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_representations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43medge_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43medge_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [301]\u001b[0m, in \u001b[0;36mGATWithEdgeType.forward\u001b[1;34m(self, node_features, edge_indices, edge_types)\u001b[0m\n\u001b[0;32m     94\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat_net:\n\u001b[1;32m---> 97\u001b[0m     h, attention_coefficients \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     attention_scores\u001b[38;5;241m.\u001b[39mappend(attention_coefficients)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h, attention_scores\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [301]\u001b[0m, in \u001b[0;36mGATLayerWithEdgeType.forward\u001b[1;34m(self, input_data, edge_type)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;66;03m# Reshape attention_coefficients for matmul\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#         attention_coefficients_reshaped = attention_coefficients.view(-1, attention_coefficients.size(-1))\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         \n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# Convert tensors to float16\u001b[39;00m\n\u001b[0;32m     61\u001b[0m         attention_coefficients_reshaped_transposed \u001b[38;5;241m=\u001b[39m attention_coefficients\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m---> 62\u001b[0m         h_linear_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mh_linear_reshaped\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;66;03m# Perform batch matrix multiplication\u001b[39;00m\n\u001b[0;32m     65\u001b[0m         h_prime_flat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attention_coefficients_reshaped_transposed, h_linear_reshaped)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'h_linear_reshaped' referenced before assignment"
     ]
    }
   ],
   "source": [
    "num_of_layers = 2\n",
    "num_heads_per_layer = [4, 2]  # Adjusted for 2 layers\n",
    "num_features_per_layer = [300, 150]\n",
    "num_edge_types = 3\n",
    "\n",
    "gat_model = GATWithEdgeType(num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "output, attention_scores = gat_model(updated_representations[0], \n",
    "                                     edge_indices[0], \n",
    "                                     edge_types[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f3aa6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_representations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4f1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# num_of_layers = 2\n",
    "# num_heads_per_layer = [4, 2]\n",
    "# num_features_per_layer = [300, 150, 64]\n",
    "# num_edge_types = 4  # Change this according to your specific edge types\n",
    "\n",
    "# gat_model = GATWithEdgeType(num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types)\n",
    "\n",
    "# # Assuming you have input data 'node_features', 'edge_indices', and 'edge_types'\n",
    "# output, attention_scores = gat_model(updated_representations, edge_indices, edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80221e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_reps = updated_representations[:3]\n",
    "# sample_edge_idx_list = ranges[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86db236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of DialogueGraphDataset and DialogueGraphDataLoader\n",
    "# dataset = DialogueGraphDataset(node_features_list= [updated_representations, encoded_speaker_list],\n",
    "# #                                node_labels_list, \n",
    "#                                edge_index_list = adjacency_list,\n",
    "#                               )\n",
    "# dataloader = DialogueGraphDataLoader(node_features_list = dataset, \n",
    "#                                      edge_index_list = adjacency_list, \n",
    "#                                      batch_size=2, \n",
    "#                                      shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ee705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize your GAT model\n",
    "# gat_model = GAT(\n",
    "#     num_of_layers=3,\n",
    "#     num_heads_per_layer=[4, 4, 6],\n",
    "#     num_features_per_layer=[len(dataset.node_features_list), 64, 64, dataset.num_classes],\n",
    "#     add_skip_connection=True,\n",
    "#     bias=True,\n",
    "#     dropout=0.6,\n",
    "# #     layer_type=\"your_layer_type\",  default 3\n",
    "#     log_attention_weights=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e99ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST_VAL_ACC = 0\n",
    "# BEST_VAL_LOSS = 0\n",
    "# PATIENCE_CNT = 0\n",
    "\n",
    "# BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\n",
    "# CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\n",
    "\n",
    "# # Make sure these exist as the rest of the code assumes it\n",
    "# os.makedirs(BINARIES_PATH, exist_ok=True)\n",
    "# os.makedirs(CHECKPOINTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (dataloader.sampler.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c10858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90979bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define hyperparameters\n",
    "# num_layers = 2\n",
    "# num_heads_per_layer = [8, 8]\n",
    "# num_features_per_layer = [300, 128, num_classes]  # Adjust num_classes based on your task\n",
    "# add_skip_connection = True\n",
    "# bias = True\n",
    "# dropout = 0.6\n",
    "# layer_type = LayerType.IMP3  # Choose the desired implementation\n",
    "# log_attention_weights = False  # Set to True if you want to log attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomGATLayer(nn.Module):\n",
    "#     def __init__(self, in_features, out_features, num_edge_types, dropout=0.6, alpha=0.2):\n",
    "#         super(CustomGATLayer, self).__init__()\n",
    "#         self.num_edge_types = num_edge_types\n",
    "\n",
    "#         # Node feature transformation\n",
    "#         self.W = nn.Linear(in_features, out_features)\n",
    "\n",
    "#         # Edge attention mechanism for each edge type\n",
    "#         self.attention_weights = nn.ModuleList([nn.Linear(2 * out_features, 1) for _ in range(num_edge_types)])\n",
    "#         self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, node_features, edge_index, edge_type):\n",
    "#         # Node feature transformation\n",
    "#         h = self.W(node_features)\n",
    "\n",
    "#         # Attention mechanism for each edge type\n",
    "#         attention_weights = [torch.exp(self.leaky_relu(att(torch.cat([h[edge_index[0]], h[edge_index[1]]], dim=-1))))\n",
    "#                              for att in self.attention_weights]\n",
    "\n",
    "#         # Compute weighted sum of neighbor features\n",
    "#         aggregated_features = sum(attention_weights[i] * h[edge_index[1]] for i in range(self.num_edge_types))\n",
    "\n",
    "#         # Apply dropout\n",
    "#         aggregated_features = self.dropout(aggregated_features)\n",
    "\n",
    "#         return aggregated_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GATLayerWithEdgeType(GATLayer):\n",
    "#     def __init__(self, num_in_features, num_out_features, num_of_heads, num_edge_types, concat=True, activation=nn.ELU(),\n",
    "#                  dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "#         super().__init__(num_in_features, num_out_features, num_of_heads, concat, activation, dropout_prob,\n",
    "#                          add_skip_connection, bias, log_attention_weights)\n",
    "\n",
    "#         # New trainable parameters for edge type embeddings\n",
    "#         self.edge_type_embeddings = nn.Parameter(torch.Tensor(num_edge_types, num_of_heads, num_out_features))\n",
    "#         self.init_params(LayerType.WITH_EDGE_TYPE)\n",
    "\n",
    "#     def forward(self, data, edge_type):\n",
    "#         in_nodes_features, edge_index = data  # unpack data\n",
    "#         num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "\n",
    "#         in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "#         # Project node features to NH independent output features\n",
    "#         nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "#         nodes_features_proj = self.dropout(nodes_features_proj)\n",
    "\n",
    "#         # Calculate attention scores for source and target nodes based on edge type\n",
    "#         scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "#         scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "#         # Lift the scores based on edge index\n",
    "#         scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "\n",
    "#         # Embedding for edge type\n",
    "#         edge_type_embedding = self.edge_type_embeddings[edge_type]\n",
    "\n",
    "#         # Apply the scoring function with edge type embeddings\n",
    "#         scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted + edge_type_embedding)\n",
    "\n",
    "#         # Neighborhood-aware softmax\n",
    "#         attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "#         attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "#         # Element-wise product with weighted and projected neighborhood feature vectors\n",
    "#         nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "#         # Aggregate neighbors\n",
    "#         out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "#         # Residual/skip connections, concat, and bias\n",
    "#         out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "#         return (out_nodes_features, edge_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "119px",
    "width": "235px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
