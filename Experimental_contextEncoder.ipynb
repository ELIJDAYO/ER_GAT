{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3274b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "import keras\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LayerNormalization, Dropout, Layer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b678671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # # Load BERT models from TensorFlow Hub\n",
    "# bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "# bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# # Example input text\n",
    "# text_input = [\"This is a test sentence for BERT model.\"]\n",
    "\n",
    "# # Preprocess the input text\n",
    "# preprocessed_text = bert_preprocess(text_input)\n",
    "\n",
    "# # Encode the preprocessed text\n",
    "# outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# # Extract the pooled_output (used for classification tasks)\n",
    "# pooled_output = outputs[\"pooled_output\"]\n",
    "\n",
    "# print(\"Pooled Output:\", pooled_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7856ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, pickle, collections, importlib, datetime, torch, nltk, pandas as pd, numpy as np, random\n",
    "from chardet import detect\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from wordebd import WORDEBD\n",
    "from vocab import Vocab, Vectors\n",
    "from munch import Munch\n",
    "from cnnlstmseq import CNNLSTMseq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "from torch.nn import MaxPool2d\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from model import batch_graphify, LSTMModel,MaskedEdgeAttention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import  BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.layers import Concatenate, Reshape, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.python.keras.backend as K\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f23de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d7c41",
   "metadata": {},
   "source": [
    "<b>Make sure to specify which dataset to use\n",
    "<br>\n",
    " - dataset_original\n",
    "<br>\n",
    " - dataset_drop_noise\n",
    "<br>\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee5dd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5414b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_data(data):\n",
    "    data = data.apply(lambda x: x.lower())\n",
    "    data = data.apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fbba9e3",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, filters, kernel_sizes, dropout):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=K) for K in kernel_sizes])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * filters, output_size)\n",
    "        self.feature_dim = output_size\n",
    "\n",
    "    def init_pretrained_embeddings_from_numpy(self, pretrained_word_vectors):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x, umask):\n",
    "        if len(x.size()) == 2:  # Check if x has only 2 dimensions\n",
    "            num_utt, num_words = x.size()\n",
    "            batch = 1\n",
    "        else:\n",
    "            num_utt, batch, num_words = x.size()\n",
    "\n",
    "        x = x.type(torch.LongTensor)\n",
    "        x = x.view(-1, num_words)  # Flatten to (num_utt * batch, num_words)\n",
    "        torch.manual_seed(SEED)\n",
    "        emb = self.embedding(x)  # Embed (num_utt * batch, num_words) -> (num_utt * batch, num_words, embedding_dim)\n",
    "        emb = emb.transpose(-2, -1).contiguous()  # (num_utt * batch, num_words, embedding_dim) -> (num_utt * batch, embedding_dim, num_words)\n",
    "\n",
    "        convoluted = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in convoluted]\n",
    "        concated = torch.cat(pooled, 1)\n",
    "        features = F.relu(self.fc(self.dropout(concated)))  # Apply dropout and fully connected layer\n",
    "        features = features.view(num_utt, batch, -1)  # Reshape back to (num_utt, batch, feature_dim)\n",
    "\n",
    "        mask = umask.unsqueeze(-1).type(torch.FloatTensor)  # (batch, num_utt) -> (batch, num_utt, 1)\n",
    "        mask = mask.transpose(0, 1)  # (batch, num_utt, 1) -> (num_utt, batch, 1)\n",
    "        mask = mask.repeat(1, 1, self.feature_dim)  # (num_utt, batch, 1) -> (num_utt, batch, feature_dim)\n",
    "        features = features * mask  # Apply mask\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a273b63b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08e24b0d",
   "metadata": {
    "code_folding": [
     0,
     19,
     32,
     46
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_glove():\n",
    "    print(\"Loading GloVe...\")\n",
    "    glv_vector = {}\n",
    "    f = open('embed/glove/glove.840B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float')\n",
    "            glv_vector[word] = coefs\n",
    "        except ValueError:\n",
    "            continue\n",
    "    f.close()\n",
    "    print(\"Completed loading pretrained GloVe model.\")\n",
    "    return glv_vector\n",
    "\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "def _read_words(data, convmode=None):\n",
    "    '''    \n",
    "    Count the occurrences of all words\n",
    "    @param convmode: str, None for non conversational scope, 'naive' for classic or naive approach, 'conv' for conversation depth into account (one additional dim and nested values)\n",
    "    @param data: list of examples\n",
    "    @return words: list of words (with duplicates)\n",
    "    '''    \n",
    "    words = []\n",
    "    if convmode is None:\n",
    "        for example in data:\n",
    "            words += example.split()\n",
    "    return words\n",
    "\n",
    "def find_value_ranges(lst):\n",
    "    value_ranges = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            value_ranges.append((start_index, i - 1))\n",
    "            start_index = i\n",
    "\n",
    "    # Add the last range\n",
    "    value_ranges.append((start_index, len(lst) - 1))\n",
    "\n",
    "    return value_ranges\n",
    "\n",
    "def seed_everything(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd611917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/DatasetPreparation/X_train.csv', encoding='shift_jis')\n",
    "\n",
    "# # Print the column names\n",
    "# print(df.columns)\n",
    "\n",
    "# columns_to_use = df.columns[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d2cf5f5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "MAX_PADDING = 100\n",
    "def x_value(data1, data2):\n",
    "    tokenizer = Tokenizer(num_words = 5000, split=\" \")\n",
    "    tokenizer.fit_on_texts(data2['Utterance'].values)\n",
    "    x = tokenizer.texts_to_sequences(data1['Utterance'].values)\n",
    "    x = pad_sequences(x, padding='post', maxlen=MAX_PADDING)\n",
    "    return x\n",
    "\n",
    "def prev_utt_generator(x):\n",
    "    shape = x.shape \n",
    "    zeros = shape[1]\n",
    "    zero_array = np.zeros((1, zeros))\n",
    "    array = np.concatenate((zero_array, x))\n",
    "    req_array = array[:-1]\n",
    "    return req_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bf1a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12840, 12)\n",
      "(3400, 12)\n",
      "(1462, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('data/' + dataset_path + '/train_sent_emo_dya.csv', encoding='shift_jis')\n",
    "X_test = pd.read_csv('data/' + dataset_path+ '/test_sent_emo_dya.csv', encoding='utf-8')\n",
    "X_dev = pd.read_csv('data/' + dataset_path + '/dev_sent_emo_dya.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "# Display the first three rows\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_dev.shape)\n",
    "\n",
    "# Define features to drop\n",
    "drop_features = list(X_train.columns[6:]) \n",
    "\n",
    "# Create DataFrame for target labels\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "y_dev = pd.DataFrame()\n",
    "\n",
    "y_train[\"Emotion\"] = X_train[\"Emotion\"].copy()\n",
    "y_test[\"Emotion\"] = X_test[\"Emotion\"].copy()\n",
    "y_dev[\"Emotion\"] = X_dev[\"Emotion\"].copy()\n",
    "\n",
    "y_train[\"Dialogue_ID\"] = X_train[\"Dialogue_ID\"].copy()\n",
    "y_test[\"Dialogue_ID\"] = X_test[\"Dialogue_ID\"].copy()\n",
    "y_dev[\"Dialogue_ID\"] = X_dev[\"Dialogue_ID\"].copy()\n",
    "\n",
    "\n",
    "\n",
    "# Drop features from X_train DataFrame\n",
    "X_train = X_train.drop(drop_features, axis=1)\n",
    "X_train[\"Utterance\"] = processed_data(X_train[\"Utterance\"]) \n",
    "x_train = x_value(X_train, X_train)\n",
    "\n",
    "X_test = X_test.drop(drop_features, axis=1)\n",
    "X_test[\"Utterance\"] = processed_data(X_test[\"Utterance\"])\n",
    "x_test = x_value(X_test, X_train)\n",
    "\n",
    "X_dev = X_dev.drop(drop_features, axis=1)\n",
    "X_dev[\"Utterance\"] = processed_data(X_dev[\"Utterance\"])\n",
    "x_dev = x_value(X_dev, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14993bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_prev = prev_utt_generator(x_train)\n",
    "# x_dev_prev = prev_utt_generator(x_dev)\n",
    "# x_test_prev = prev_utt_generator(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7322ae",
   "metadata": {},
   "source": [
    "#### Try the CNN version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d795009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 256)     3287296     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 100, 256, 1)  0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 98, 1, 64)    49216       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 97, 1, 64)    65600       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 96, 1, 64)    81984       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 98, 1, 64)   256         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 97, 1, 64)   256         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 96, 1, 64)   256         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 1, 64)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 64)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 64)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1, 1, 192)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]',        \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 192)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 7)            1351        ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,486,215\n",
      "Trainable params: 3,485,831\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "num_filters = 64\n",
    "drop = 0.2\n",
    "VOCAB_SIZE = len(x_train) # 43,731\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "EMBED_SIZE = 256 # arbitary\n",
    "UNIQUE_DA = 5\n",
    "\n",
    "# CNN model\n",
    "inputs = Input(shape=(MAX_LENGTH, ), dtype='int32')\n",
    "embedding = Embedding(input_dim=VOCAB_SIZE+1, \n",
    "                      output_dim=EMBED_SIZE, \n",
    "                      input_length=MAX_LENGTH)(inputs)\n",
    "reshape = Reshape((MAX_LENGTH, EMBED_SIZE, 1))(embedding)\n",
    "\n",
    "# 3 convolutions\n",
    "conv_0 = Conv2D(num_filters, \n",
    "                kernel_size=(filter_sizes[0], EMBED_SIZE), \n",
    "                strides=1, \n",
    "                padding='valid', \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu')(reshape)\n",
    "\n",
    "bn_0 = BatchNormalization()(conv_0)\n",
    "\n",
    "conv_1 = Conv2D(num_filters, \n",
    "                kernel_size=(filter_sizes[1], EMBED_SIZE), \n",
    "                strides=1, \n",
    "                padding='valid', \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu')(reshape)\n",
    "bn_1 = BatchNormalization()(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(num_filters, \n",
    "                kernel_size=(filter_sizes[2], EMBED_SIZE), \n",
    "                strides=1, \n",
    "                padding='valid', \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu')(reshape)\n",
    "bn_2 = BatchNormalization()(conv_2)\n",
    "\n",
    "# maxpool for 3 layers\n",
    "maxpool_0 = MaxPooling2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), padding='valid')(bn_0)\n",
    "maxpool_1 = MaxPooling2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), padding='valid')(bn_1)\n",
    "maxpool_2 = MaxPooling2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), padding='valid')(bn_2)\n",
    "\n",
    "# concatenate tensors\n",
    "merged_1 = Concatenate(axis=-1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "\n",
    "reshape_1 = Reshape((192,), input_shape=(1,1,192))(merged_1)\n",
    "\n",
    "output = Dense(7, activation='softmax')(reshape_1)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b3d410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', \n",
    "                               mode='auto', \n",
    "                               patience=3, \n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86036b44",
   "metadata": {
    "code_folding": [
     3,
     10
    ]
   },
   "outputs": [],
   "source": [
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_encoder.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_decoder.pkl\")\n",
    "\n",
    "if not(checkFile1 and checkFile2):\n",
    "    labels = sorted(set(y_train.Emotion))\n",
    "    labelEncoder = {label: i for i, label in enumerate(labels)}\n",
    "    labelDecoder = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    pickle.dump(labelEncoder, open('data/dump/' + dataset_path + '/label_encoder.pkl', 'wb'))\n",
    "    pickle.dump(labelDecoder, open('data/dump/' + dataset_path + '/label_decoder.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('data/dump/' + dataset_path +'/label_encoder.pkl', 'rb')\n",
    "    file2 = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "    labelEncoder = pickle.load(file1)\n",
    "    labelDecoder = pickle.load(file2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a15940aa",
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_train.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_test.pkl\")\n",
    "checkFile3 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_dev.pkl\")\n",
    "\n",
    "if not (checkFile1 or checkFile2 or checkFile3):\n",
    "    pickle.dump(X_train[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_train.pkl', 'wb'))\n",
    "    pickle.dump(X_test[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(X_dev[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_dev.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7594afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "# Encode training labels\n",
    "y_train['Label'] = encoder.fit_transform(y_train['Emotion'])\n",
    "y_train_encoded = to_categorical(y_train['Label'], num_classes=7)\n",
    "\n",
    "# Encode development labels\n",
    "y_dev['Label'] = encoder.transform(y_dev['Emotion'])\n",
    "y_dev_encoded = to_categorical(y_dev['Label'], num_classes=7)\n",
    "\n",
    "# Encode test labels\n",
    "y_test['Label'] = encoder.transform(y_test['Emotion'])\n",
    "y_test_encoded = to_categorical(y_test['Label'], num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca597143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "402/402 [==============================] - 29s 71ms/step - loss: 1.3833 - accuracy: 0.5470 - val_loss: 1.6140 - val_accuracy: 0.4371\n",
      "Epoch 2/20\n",
      "402/402 [==============================] - 30s 74ms/step - loss: 0.5031 - accuracy: 0.8350 - val_loss: 1.8187 - val_accuracy: 0.3967\n",
      "Epoch 3/20\n",
      "402/402 [==============================] - 29s 71ms/step - loss: 0.2948 - accuracy: 0.9030 - val_loss: 2.1602 - val_accuracy: 0.4302\n",
      "Epoch 4/20\n",
      "402/402 [==============================] - 30s 75ms/step - loss: 0.2307 - accuracy: 0.9222 - val_loss: 2.3512 - val_accuracy: 0.4186\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train_encoded, \n",
    "                 epochs=20, \n",
    "                 batch_size=32, \n",
    "                 verbose=1,\n",
    "                 callbacks=[early_stopping],\n",
    "                 validation_data=(x_dev, y_dev_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af8224da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEvCAYAAAB2Xan3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAAsTAAALEwEAmpwYAABClklEQVR4nO3deXhU1cE/8O+ZPfsK2SELYZEdwg5hsYoCgqKISy3oK1RFcOlrtdatP2lrq6+1uKJW0FZF61YFra2tIQRBWWRfwpaQhC1kD2Qms5zfH3cymclCEpjkTma+n+fJMzP3nrlz5jDMd865594rpJQgIiIi9WjUrgAREVGgYxgTERGpjGFMRESkMoYxERGRyhjGREREKmMYExERqUyn1gvHxsbK1NRUr23v3LlzCAkJ8dr2uju2hye2RyO2hSe2hye2R6POaItt27adlVL2aLpctTBOTU3F1q1bvba9nJwcTJkyxWvb6+7YHp7YHo3YFp7YHp7YHo06oy2EEIUtLecwNRERkcoYxkRERCpjGBMREalMtX3GLbFarSguLobZbO7wcyMiIrB///5OqFX35I32MJlMSE5Ohl6v91KtiIioJT4VxsXFxQgLC0NqaiqEEB16bk1NDcLCwjqpZt3PpbaHlBJlZWUoLi5GWlqaF2tGRERN+dQwtdlsRkxMTIeDmLxPCIGYmJiLGqUgIqKO8akwBsAg9iH8tyAi6ho+F8ZqCw0NVbsKREQUYBjGREREKmMYt0JKiYceegiDBg3C4MGD8cEHHwAATp48iezsbAwbNgyDBg3Chg0bYLfbsXDhQlfZP/3pTyrXnoiILoaUEqfOncLGko1YX70eUsoueV2fmk3tSz755BPs2LEDO3fuxNmzZzFq1ChkZ2fjvffew/Tp0/HrX/8adrsd58+fx44dO1BSUoI9e/YAACorK9WtPBERXZBDOnDy3EkcqTyCo5VHcaSq8fac9Zyr3BLzEsQGxXZ6fXw2jH/zxV7sO1Hd7vJ2ux1arfaCZS5LDMeT1wxs1/by8vJw8803Q6vVIi4uDpMnT8aWLVswatQo3HHHHbBarbj22msxbNgwpKen4+jRo1i6dClmzpyJK6+8st31JiKizmN32HGi9gSOVB1RgrfqKA5XHsaxqmOos9W5ysUGxSIjIgOzM2YjIyID6ZHpOLX3VJcEMeDDYeyrsrOzkZubi3Xr1mHhwoV48MEH8bOf/Qw7d+7E119/jddeew0ffvgh3nrrLbWrSkQUMGwOG4prinGk8ohH8B6rOgaL3eIq1zO4JzIiMnB95vVIj0xHRkQGMiIzEGGMaLbNnAM5XVZ/nw3j9vZgG3j7pB+TJk3CypUrsWDBApSXlyM3NxfPPvssCgsLkZycjEWLFsFisWD79u2YMWMGDAYDrr/+evTr1w8//elPvVYPIiJqZLVbcbzmuCt0G4aWC6oKYHVYXeUSQhKQHpmOMfFjkBGp9HTTI9IRZvDNk0P5bBir7brrrsOmTZswdOhQCCHwxz/+EfHx8Xj77bfx7LPPQq/XIzQ0FO+88w5KSkpw++23w+FwAAB+//vfq1x7IqLurd5ej4LqAlfYNuzbLawuhE3aAAACAkmhSciIzMDEpImuXm5aRBpC9N3rmswM4yZqa2sBKCe8ePbZZ/Hss896rF+wYAEWLFjQ7Hnbt2/vkvoREfkTs82MguoCHK48rASvc3j5eM1xOKTSwdEIDVLCUpAekY6pvaYiPSLdFbpBuiCV34F3MIyJiKjTnbeex7GqYx693CNVR1BcUwwJ5fAhrdCiV3gv9InsgytTr0SfyD5Ij0hHakQqjFqjyu+gczGMiYjIa2rra3G0qrGH23BbUlviKqPT6JAanorLYi7DNenXuCZS9Q7vDb02MK8SxzAmIqIOq7JUKT3dyiPKELMzeE+fP+0qY9AYkBaRhiE9huC6Pte5JlKlhKVArwnM0G0Nw5iIiFpVaa70CNuGGcyldaWuMiatCWkRaRgVPwoZkRmuiVRJoUnQai58/gdSMIyJiAKclBJl5jIcrTyK3Jpc5G3Oc4VvubncVS5YF4yMyAyMTxyvhG5kBtIj0pEYmgiN4NmVLwXDmIgoQEgpUVpX6rE/t6G3W2WpcpULrQlFRmQGpqRMcc1czojIQHxIPC+t2kkYxkREfqbhYgfuZ6JqmMFcY61xlQs3hKNPZB9c0fsK19DymX1nMPvy2QzdLsYwVonNZoNOx+YnoovnkA6cqD3h0cttuH/edt5VLtoUjYzIDMxIn+Hq5aZHpiPGFNMsdHPycxjEKmAatODaa69FUVERzGYz7rvvPixevBj//Oc/8eijj8JutyM2Nhb/+c9/UFtbi6VLl2Lr1q0QQuDJJ5/E9ddfj9DQUNfJQz766COsXbsWq1evxsKFC2EymfDjjz9iwoQJuOmmm3DffffBbDYjKCgIq1atQr9+/WC32/Hwww/jn//8JzQaDRYtWoSBAwdixYoV+OyzzwAA//73v/HKK6/g008/VbGliKgr2B12lNSWeEygOlx5GAXVBR4XO+gR1APpkem4ts+1rv256ZHpiDZFq1h7ag+GcQveeustREdHo66uDqNGjcKcOXOwaNEi5ObmIi0tDeXlyoSGp59+GhEREdi9ezcAoKKios1tFxcX47vvvoNWq0V1dTU2bNgAnU6Hb775Bo8++ig+/vhjvP766ygoKMCOHTug0+lQXl6OqKgo3HPPPSgtLUWPHj2watUq3HHHHZ3aDkTUtWwOG4pqijz25R6tVC52UO+od5WLC45DRmQGRsaNREZkBvpE9kFaRFqLFzug7sF3w/irR4BTu9tdPMhuA7RtvJ34wcDVz7S5rRUrVrh6nEVFRXj99deRnZ2NtLQ0AEB0tPIr85tvvsGaNWtcz4uKimpz2/PmzXNd6rGqqgoLFizAoUOHIISA1Wp1bfeuu+5yDWM3vN5tt92Gv/3tb7j99tuxadMmvPPOO22+HhH5HqvdisLqQo8LHRypPIKC6gLYHDZXucSQRKRHpmNc4jjXRKr0iHSEGkJVrD11Bt8NY5Xk5OTgm2++waZNmxAcHIwpU6Zg2LBhOHDgQLu34b6/xWw2e6wLCWk8efnjjz+OqVOn4tNPP0VBQQGmTJlywe3efvvtuOaaa2AymTBv3jzucybycRa7BQVVBc3OSHW8+rjHxQ6Sw5KREZGB7ORs1z7dtIg0BOuDVX4H1FV899u8HT1Yd3VeuoRiVVUVoqKiEBwcjAMHDmDz5s0wm83Izc3FsWPHXMPU0dHRuOKKK/Dyyy/jhRdeAKAMU0dFRSEuLg779+9Hv3798Omnn7Zar6qqKiQlJQEAVq9e7Vp+xRVXYOXKlZg6daprmDo6OhqJiYlITEzE8uXL8c0331zyeyUi76iz1aGgqsDj5BhHq46iqKbI42IHvcJ6IT0iHZf3uhzpkenoE9kHqeGpMOlMKr8DUpvvhrFKrrrqKrz22msYMGAA+vXrh7Fjx6JHjx54/fXXMXfuXDgcDvTs2RP//ve/8dhjj2HJkiUYNGgQtFotnnzyScydOxfPPPMMZs2ahR49eiArK8s1maupX/7yl1iwYAGWL1+OmTNnupbfeeedyM/Px5AhQ6DX67Fo0SLce++9AIBbb70VpaWlGDBgQJe0BxE1Om893+xMVEcqj6CktsR1sQOd0KFXeC/0jeqLq9Ouds1cTg1PhUFrUPkdkK9iGDdhNBrx1Vdftbju6quv9ngcGhqKt99+u1m5G264ATfccEOz5e69XwAYN24c8vPzXY+XL18OANDpdHj++efx/PPPN9tGXl4eFi1a1Ob7IKKLV1Nfg6NVR11h2xC8J86dcJXRa/RIjUjFoNhBmN1ntus43V5hvQL2Ygd08RjG3cjIkSMREhKC//u//1O7KkR+4bz1PI6aj+Js/lmPGcxnzp9xlTFqjUiLSMOwnsNwfeT1rp5uSlgKdBp+hZJ38JPUjWzbtk3tKhB1e8erjyO3OBe5xbnYenorrA4rcBoI0gUhLSINYxPGepwCMjE0kRc7oE7HMCYiv2a1W7H9zHZXABdUFwAA0iLScEv/W2AqNeG67OuQEJLAix2QahjGROR3yurKsKFkA3KLc7HpxCbUWmuh1+gxKn4Ubup/E7KTspESngJAOZwxKTRJ5RpToGMYE1G3J6XE/vL9WF+8HhuKN2DP2T2QkOgR1APTU6djUvIkjEsYx+N2yWcxjImoWzpvPY9NJzchtzgXG4o3oLSuFAICg2MH455h9yA7ORsDogfwogfULTCMiajbKKouQm5JLtYXrXdNvgrVh2J84nhkJ2djYtJExATFqF1Nog5jGF8C96szNVVQUIBZs2Zhz549XVwrIv9hdVjx4+kfkVuci/XF65tNvspOzsbwuOHQa3hcL3VvDGMi8illdWXIK8nD+uL1bU6+IvIXDGM3jzzyCFJSUrBkyRIAwFNPPQWdTodvv/0WFRUVsFqtWL58OebMmdOh7ZrNZtx9993YunWr6+xaU6dOxd69e3H77bejvr4eDocDH3/8MRITE3HjjTeiuLgYdrsdjz/+OObPn98Zb5fIJzRMvmo49IiTrygQ+WwY/+GHP+BAefuvlGS3212XJmxN/+j+eHj0w62unz9/Pu6//35XGH/44Yf4+uuvsWzZMoSHh+Ps2bMYO3YsZs+e3aFJIS+//DKEENi9ezcOHDiAK6+8Evn5+Xjttddw33334dZbb0V9fT3sdju+/PJLJCYmYt26dQCUi0kQ+ZuGyVcbipXDjxomXw2KHcTJVxSQfDaM1TB8+HCcOXMGJ06cQGlpKaKiohAfH48HHngAubm50Gg0KCkpwenTpxEfH9/u7ebl5WHp0qUAgP79+6N3797Iz8/HuHHj8Nvf/hbFxcWYO3cuMjMzMXjwYPziF7/Aww8/jFmzZmHSpEmd9XaJulTD5Kvc4lxsObXFNflqXOI4TE6ezMlXFNB8Nowv1INtSY2XLqE4b948fPTRRzh16hTmz5+Pd999F6Wlpdi2bRv0ej1SU1ObXaP4Yt1yyy0YM2YM1q1bhxkzZmDlypWYNm0atm/fji+//BKPPfYYLr/8cjzxxBNeeT2iruQ++Sq3JBfHqo4BAFLDUzn5iqgJnw1jtcyfPx+LFi3C2bNnsX79enz44Yfo2bMn9Ho9vv32WxQWFnZ4m5MmTcK7776LadOmIT8/H8ePH0e/fv1w9OhRpKenY9myZTh+/Dh27dqF/v37Izo6Gj/96U8RGRmJN998sxPeJVHnaJh8lVuci+9OfOeafJUVl4X5/eZz8hVRKxjGTQwcOBA1NTVISkpCQkICbr31VlxzzTUYPHgwsrKy0L9//w5v85577sHdd9+NwYMHQ6fTYfXq1TAajfjwww/x17/+FXq9HvHx8Xj00UexZcsWPPTQQ9BoNNDr9Xj11Vc74V0SeYeUEgfKD7jOfLX77G7X5KsrU69EdnI2J18RtQPDuAW7d+923Y+NjcWmTZtaLNfaMcYAkJqa6jrG2GQyYdWqVc3KPPLII3jkkUc8lk2fPh3Tp0+/mGoTdYnz1vPYfHKz68xXZ+rOuCZf3T3sbkxOnoz+0f150QWiDmgzjIUQKQDeARAHQAJ4XUr55yZlBIA/A5gB4DyAhVLK7d6vLhGpoaimyHXoESdfEXlfe3rGNgC/kFJuF0KEAdgmhPi3lHKfW5mrAWQ6/8YAeNV56/d2796N2267zWOZ0WjE999/r1KNiC6d1WHFjjM7XGe+cp98dXP/mzE5eTKG9xwOvZaTr4i8oc0wllKeBHDSeb9GCLEfQBIA9zCeA+AdKaUEsFkIESmESHA+168NHjwYO3bsULsaRJes3FzeOPmq5DvUWGtck69u7HsjspOz0Su8l9rVJPJLHdpnLIRIBTAcQNNuXxKAIrfHxc5lfh/GRN1Vw+SrhkOPdpcqk69ig2JxReoVyE7KxtjEsQjRh6hdVSK/J5TObDsKChEKYD2A30opP2mybi2AZ6SUec7H/wHwsJRya5NyiwEsBoC4uLiRa9as8XiNiIgI9OnT56LeSHvOwBVIvNUehw8f9ouzgNXW1iI0NFTtaqjO4rBgZ+VOHJFHsK9uHyrtlQCA3obeGBg0EAODBiLZkBxQk6/42fDE9mjUGW0xderUbVLKrKbL29UzFkLoAXwM4N2mQexUAsD94MFk5zIPUsrXAbwOAFlZWXLKlCke6/fv33/RJ+7w1kk//IW32sNkMmH48OFeqJG6cnJy0PTzFigaJl9tKN6ALae2oN5RjxB9CMYnN152MDYoVu1qqiaQPxstYXs06sq2aM9sagHgLwD2Symfb6XY5wDuFUKsgTJxqyoQ9hcT+SL3yVe5xbk4WnUUgDL56qb+NyH8bDjuuPIOTr4i8iHt6RlPAHAbgN1CiB3OZY8C6AUAUsrXAHwJ5bCmw1AObbrd6zX1QRe6njFRV2pp8pVOo0NWXBbm9Z3nMfkqJyeHQUzkY9ozmzoPwAUvneKcRb3EW5WijrHZbNDpeP6WQMLJV0T+xWe/wU/97new7G//JRRtdjvK25iwZBzQH/GPPtrqem9ez7i2thZz5sxp8XnvvPMOnnvuOQghMGTIEPz1r3/F6dOncdddd+HoUWVI8dVXX0ViYiJmzZrlOpPXc889h9raWjz11FOYMmUKhg0bhry8PNx8883o27cvli9fjvr6esTExGDlypUICwtDbW0tli5diq1bt0IIgSeffBJVVVXYtWsXXnjhBQDAG2+8gX379uFPf/pTm++L1ONx5quSDThz/gwAYFCMcuarhssOBtLkKyJ/4bNhrAZvXs/YZDLh008/bfa8ffv2Yfny5fjuu+8QGxuL8vJyAMCyZcswefJkfPrpp7Db7aitrUVFRcUFX6O+vh5btyoT1isqKrB582YIIfDmm2/ihRdewIsvvoinn34aERERrlN8VlRUQK/X47e//S2effZZ6PV6rFq1CitXrrzU5qNO0Orkq0ROviLyJz4bxhfqwbbEG7OHvXk9YyklHn300WbP++9//4t58+YhNlb5Ao2OjgYA/Pe//8U777wDANBqtYiIiGgzjOfPn++6X1xcjPnz5+PkyZOor69HSooyuf2bb76B+yFkUVFRAIBp06Zh7dq1GDBgAKxWKwYPHtzB1qLOcKHJV/P7z8fk5MkY0XME9/kS+RmfDWO1eOt6xt64DrJOp4PD4XA9bvr8kJDG/YFLly7Fgw8+iNmzZyMnJwePP/74Bbd955134ne/+x369++P228PiPl2PqvCXIG8kjysL17fbPLVDX1vQHZyNnqH91a7mkTUiRjGTXjresZVVVUtPm/atGm47rrr8OCDDyImJgbl5eWIjo7G5ZdfjldffRX333+/a5g6Li4OZ86cQVlZGUJDQ7F27VpcddVVrb5eUlISAODtt992Lb/iiivw8ssvu/YPV1RUICoqCmPGjEFRURG2b9+OXbt2XUKLUUdJKXGw4qDrvM/uk69+0vsnymUHE8dx8hVRAGEYN+Gt6xm39ryBAwfi17/+NSZPngytVovhw4dj9erV+POf/4zFixfjL3/5C7RaLV599VWMGzcOTzzxBEaPHo2kpKQLvvZTTz2FefPmISoqCtOmTcPhw4cBAI899hiWLFmCQYMGQavV4sknn8TcuXMBADfeeCN27NjhGrqmznPeeh7fn/weuSXK8LPH5KuhzslXMZx8RRSoGMYt8Mb1jC/0vAULFmDBggUey+Li4vCPf/yjWdlly5Zh2bJlzZbn5OR4PJ4zZ47HLO+amhoAyrHQ7j1ld3l5eXjggQdafQ90aYpril2HHm056Tn5alLSJExKnsTJV0QEgGEckCorKzF69GgMHToUl19+udrV8RsNk682FG9AbnEujlQdAdA4+So7ORsje47k5CsiaoZhfIm64/WMIyMjkZ+fr3Y1/ELD5Kvc4lxsPLERNfWNk6+u73s9J18RUbswjC8Rr2ccWKSUyK/Ix/ri9cgtzsWu0l2QkIgxxeAnvTj5iogujs+FsZSyzRNqUNdo7+U1/d1563n8cOoHrC9ejw3FG3D6/GkAwMCYgZx8RURe4VNhbDKZUFZWhpiYGAayyqSUKCsrg8lkUrsqqiipLXEdetQw+SpYF4zxieOxJHkJJ18RkVf5VBgnJyejuLgYpaWlHX6u2WwO2OBoiTfaw2QyITk52Us18m02h00581VJLnKLGidf9Q7vjRv73YjJKZM5+YqIOo1PhbFer0daWtpFPTcnJwfDhw/3co26L7ZH2xomX20o3oC8E3muyVcj40ZibuZcZCdnIzUiVe1qElEA8KkwJupMDZOvXGe+OrsbDulAjCkGl/e6XJl8lTAOoYZQtatKRAGGYUx+yyEdKKkpwcGKg/io7CMs/2i5x+Srnw/5OSYnT+bkKyJSHcOY/EKFuQKHKg7hUOUh5dZ5v85WBwAwCiMmpUzi5Csi8kkMY+pWLHYLjlQe8Qjc/Ip8nK076yoTZYxCZlQm5mbORWZkJjKjMnFmzxn8ZOpPVKw5EVHrGMbkkxqGmPMr83GoQgncQxWHcLzmOBxSuaykUWtEekQ6xieOR9+ovsiMzETf6L6IMTU/NC5H5KjwLoiI2odhTKpzH2JuCN3DlYddQ8wCAslhyciMzMT01OlK8EZloldYL2g1WpVrT0R06RjG1GXMNjOOVh11BW5DALc2xNzQ282IzECwPljFmhMRdS6GMXmda4i5It81zNzaEPOExAnIjFL26/aNanmImYjI3zGM6ZKUm8s9erktDTGnhKUgMyoTV6Vd5ZpQxSFmIqJGDGNqF7PNjCNVbrOYnZOqysxlrjJRxij0jeqL6zOvV3q7HGImImoXhjF5cEgHimuKlbC9wBBzRmQGJiZN5BAzEZEXMIwDmPsQc8OkqiNVRy44xNw3qi9SwlI4xExE5EUM4wDQMMScX57vcYYq9yHmaFM0MiMzcX3m9a5Dh9Ij0jnETETUBRjGfsTusKO4Vhli/lflv/B5zucXHGJuCN3MqEyeHpKISEUM426qrK7M8zzMLQ0xO1LQN6ovrk672jWhikPMRES+h2Hs4+psdTha6TxRxoWGmKM8h5hLdpVg+rTpKtaciIjai2HsI9yHmN1PDXm8+jgkJADApDUhIzIDk5InuY7XbW2I+azmbLNlRETkmxjGKmg6xJxfkY8jlUdgtpsBKEPMvcJ7ITMyEzPSZrgOHUoOTeYQMxGRH2IYdyL3IWb3YeZyc7mrTMMQ8w19b0DfqL7oG9UX6ZHpCNIFqVhzIiLqSgxjL2gYYm56AYSWhpizk7MbZzFHZiImKEbl2hMRkdoYxh10tu5ss3MxtzbEPDNtpmu/LoeYiYioNQzjVtTZ6nCk8kjj2alaGWLuG9UX8/rNc52dikPMRETUUQEfxnaHHUU1RZ7H7DYZYg7SBSEjIgOTkye7erocYiYiIm8JqDBuOsScX5GPo5VHXUPMGqFBr7Be6BvVl0PMRETUZfwyjJsNMTvD132IOcYUg8yoTA4xExGR6vwijIuqi7Cuch0++/YzHKo4hKKaolaHmBtmMkebolWuNRERkcIvwri0rhRfV32N3rI3+kX3w6z0Wa7QTQ5LhkZo1K4iERFRq/wijAf3GIznUp7DldOuVLsqREREHeYXXUa9Rg+DxqB2NYiIiC6KX4QxERFRd8YwJiIiUhnDmIiISGUMYyIiIpUxjImIiFTGMCYiIlIZw5iIiEhlbYaxEOItIcQZIcSeVtZPEUJUCSF2OP+e8H41iYiI/Fd7zsC1GsBLAN65QJkNUspZXqkRERFRgGmzZyylzAVQ3lY5IiIiujje2mc8TgixUwjxlRBioJe2SUREFBCElLLtQkKkAlgrpRzUwrpwAA4pZa0QYgaAP0spM1vZzmIAiwEgLi5u5Jo1ay6l7h5qa2sRGhrqte11d2wPT2yPRmwLT2wPT2yPRp3RFlOnTt0mpcxquvySw7iFsgUAsqSUZy9ULisrS27durXN126vnJwcTJkyxWvb6+7YHp7YHo3YFp7YHp7YHo06oy2EEC2G8SUPUwsh4oUQwnl/tHObZZe6XSIiokDR5mxqIcT7AKYAiBVCFAN4EoAeAKSUrwG4AcDdQggbgDoAN8n2dLeJiIgIQDvCWEp5cxvrX4Jy6BMRERFdBJ6Bi4iISGUMYyIiIpUxjImIiFTGMCYiIlIZw5iIiEhlDGMiIiKVMYyJiIhU5hdhXFZrwYs/mlFSWad2VYiIiDrML8L40Jla7D1rx8wVG/DtgTNqV4eIiKhD/CKMx6bH4KnxQUiICMLtq7fgj/88AJvdoXa1iIiI2sUvwhgA4kM0+PSe8bh5dApeyTmCW978HqerzWpXi4iIqE1+E8YAYNJr8fu5Q/D8jUOxu7gKM1dswMbDF7ySIxERker8KowbzB2RjM/vnYDIYAN++pfv8edvDsHu4IWkiIjIN/llGANAZlwYPr93Aq4dloQ/fZOPhat+wNlai9rVIiIiasZvwxgAgg06PH/jUDwzdzC+P1aOmSs24Idj5WpXi4iIyINfhzEACCFw0+he+PSe8QjSa3HzG5vx2vojcHDYmoiIfITfh3GDgYkR+GLpREwfGIdnvjqAxX/disrz9WpXi4iIKHDCGADCTHq8fMsIPHXNZVifX4qZK/Kwo6hS7WoREVGAC6gwBpRh64UT0vD3u8YDAOa99h1WbzwGKTlsTURE6gi4MG4wLCUS65ZNRHZmDzz1xT4seW87qs1WtatFREQBKGDDGAAigw1442dZ+NXV/fH13tOY/WIe9p6oUrtaREQUYAI6jAFAoxH4+eQMrFk8FnVWO6575Tu8/8NxDlsTEVGXCfgwbjAqNRrrlk3CmLRo/OqT3Xjww504Z7GpXS0iIgoADGM3saFGrL59NB68oi8+21GCOS9vxKHTNWpXi4iI/BzDuAmtRmDZ5Zn42/+MQeX5esx+aSM+/bFY7WoREZEfYxi3YkKfWKxbNgmDkyPwwAc78atPdsFstatdLSIi8kMM4wuICzfhvTvH4J4pGXj/hyJc98p3OHb2nNrVIiIiP8MwboNOq8Evr+qPVQtH4WRVHa55MQ/rdp1Uu1pERORHGMbtNLV/T6xbNgmZcaFY8t52PPX5XtTbHGpXi4iI/ADDuAOSIoPwweJx+J+JaVj9XQHmrdyEovLzaleLiIi6OYZxBxl0Gjw+6zK89tOROHqmFrNezMM3+06rXS0iIurGGMYX6apB8Vi7bCKSo4Jw5ztb8fuv9sNq57A1ERF1HMP4EvSOCcHHd4/HrWN6YeX6o7jljc04VWVWu1pERNTNMIwvkUmvxW+vG4w/3zQMe09UY8aKDcjNL1W7WkRE1I0wjL1kzrAkfH7vRMSGGrBg1Q94/t/5sDt4sQkiImobw9iL+vQMxT+WTMTc4clY8Z9D+Nlb36O0xqJ2tYiIyMcxjL0syKDF/904FH+8YQi2FlRgxooN2Hy0TO1qERGRD2MYd5Ibs1Lw2ZIJCDPqcMsbm/Hyt4fh4LA1ERG1gGHciQYkhOPzpRMxY3ACnv36IP7n7S2oOFevdrWIiMjHMIw7WahRhxdvHo6n5wzExsNlmLliA7Yfr1C7WkRE5EMYxl1ACIHbxqXi47vHQ6sVuPG1TfhL3jFIyWFrIiJiGHepwckRWHvvJEzt3xNPr92Hu/62DVV1VrWrRUREKmMYd7GIYD1ev20kHps5AP/ZfwbXvJiHPSVValeLiIhUxDBWgRACd05Kxwc/Hwur3YG5r36Hv20u5LA1EVGAYhiraGTvaKxbNgnj0mPw2Gd7cN+aHai12NSuFhERdTGGscqiQwxYtXAUHpreD2t3ncDsl/Jw8FSN2tUiIqIuxDD2ARqNwJKpffC3O8egus6GOS/n4aNtxWpXi4iIugjD2IeMz4jFl/dNxLCUSPzv33filx/tRF29Xe1qERFRJ2MY+5ieYSa8e+dYLJ3WB3/fVozrXtmII6W1aleLiIg6UZthLIR4SwhxRgixp5X1QgixQghxWAixSwgxwvvVDCxajcAvruyHVQtH4XS1GbNfzMMXO0+oXS0iIuok7ekZrwZw1QXWXw0g0/m3GMCrl14tAoAp/Xpi3bJJ6J8QjqXv/4jHP9sDi43D1kRE/qbNMJZS5gIov0CROQDekYrNACKFEAneqmCgS4wMwprFY7E4Ox1/3VyIG17dhKLy82pXi4iIvMgb+4yTABS5PS52LiMv0Ws1eHTGALx+20gUlp3DjBUb8K+9p9SuFhEReYloz1mfhBCpANZKKQe1sG4tgGeklHnOx/8B8LCUcmsLZRdDGcpGXFzcyDVr1lxa7d3U1tYiNDTUa9vzVaXnHXhlhwXHqh2YnqrDvL4G6DSiWblAaY/2Yns0Ylt4Ynt4Yns06oy2mDp16jYpZVbT5TovbLsEQIrb42TnsmaklK8DeB0AsrKy5JQpU7zw8oqcnBx4c3u+bPaVdvxu3X68vakQpY5QvHTLCCRGBnmUCaT2aA+2RyO2hSe2hye2R6OubAtvDFN/DuBnzlnVYwFUSSlPemG71AqjTovfzBmEF28ejoOnajBzxQbkHDyjdrWIiOgitefQpvcBbALQTwhRLIT4HyHEXUKIu5xFvgRwFMBhAG8AuKfTakserhmaiC+WTkRcuAkLV23Bc18fhM3uULtaRETUQW0OU0spb25jvQSwxGs1og5J7xGKz5ZMwJP/2IuXvj2MrYXlWHHTcLWrRUREHcAzcPkBk16LP9wwBM/NG4odRZWYsSIP+8t4PDIRUXfBMPYjN4xMxj+WTEREkA5/3GLGS/89BIeD10gmIvJ1DGM/0y8+DJ/fOxFjErR47l/5WLh6C8rP1atdLSIiugBvHNpEPibEqMPPhxhxzdh0/OaLfZjx5w14+dbhGNk7Wu2qEZG/cjgAh+3Cf/ZOXu+wA3ar52OH9aLXj607B4zbARjDOr35GMZ+SgiBW8f0xtDkSCx5bzvmr9yMh6/qjzsnpUGI5icJISIvkhKQznByfbnb3b70rZ6P7dYW1rs9p9k2Ori+1ddovn54ZRmQH9LC+jaCUKp4JIdG5/anBTR6z8dafevrdaYW1usAjR4VZ0qRILpmAJlh7OcGJUXgi6UT8cu/78Jvv9yPHwrK8dwNQxERrFe7akSdx2YBzNWApRowVzlvqwFLjdt9Zd2AkkLgzKoWekqXGIRqaggbrd4tXHTO5W6PtbomQaaDXRsEBMe0L8g6vL6Fv2Z1uIjX6KQOxsGcHCQYQjpl200xjANAuEmPV386Aqs2FuB3X+7HzBc34JVbR2BIcqTaVSNqzmp2C8wqt+Bs6baV9XZL26+jDwFM4Qi3SsAR3kJQ6ZU/fVArQdZW0LWxvpUw7Nj6ll7j0npyu3gGLlUwjAOEEAJ3TEzDsF6RWPrej7jh1U14bNYA3Da2N4etyTukBGzm9oVlsx6r2629HRMODaGAMRwwhSu3wTFAVFrjY1M4YIxo8tjt1hiuBB2A7xk+5AMYxgFmRK8orF06EQ9+uANP/GMvvj9WjmfmDkaYicPWAU1KwHq+A0HayhCww9r2axndgzEMCOkBxGQ0Cc2IlkO04Vaj7fw2IepCDOMAFBViwF8WjMLK3KN47l8Hse9ENV65dQQGJISrXTW6GFIC9efaPYQ7qOQocOxZz/WWmnbs5xTNgzEsAejRr4XAbKVXagi75GFUIn/EMA5QGo3A3VMyMKJXJJa+/yOufXkjnp4zCPOykjls3ZUcDqC+tgPDuDUthG0NINs445rQuALRZNUAIYlAeBLQY0ALodlKr9QQyiAl6iQM4wA3Jj0G65ZNwv0f/IhffrwL3x8rx9PXDkSwgR+NNjkcQH1NxyYWtXSLNs6SJrTNe52RKYBxYCvDuC30Sg0hrhmnW7mPlMjn8BuX0CPMiHfuGIMV/zmEFf89hN0llXjl1hHo07PzD3RXjcPecjBaalqfWNRS2baCVKN3mzQUpvQ6o1Jb3xfaUpjqgzvt0A0i8g0MYwIAaDUCD1zRF1mpUbh/zQ7Mfmkjfj93MOYMS1K7ahfHYQdO7wEKNgLHN2F4yUFgr2gM0vratrehNTQPypD0NiYXNQlSnYlBSkRtYhiTh0mZPbBu2SQsfX877luzA98fK8cTsy6DSe/js1cdduDULiV8C/KA498pPVwAiEqFQxMOxPa+8OEuTcNUb1L3PRFRwGAYUzPxESa8v2gsnvtXPl5bfwQ7i5Rh694xXXMmmnax24BTO93Cd7OynxYAotOBy+YAvScCqROAiGTs5H5SIvJhDGNqkU6rwSNX98eo1Cg8+OFOzFqRh2fnDcFVgxLUqZDdBpzcCRRsAAo3AoWblMlTABDTBxh0XWP4hieqU0cioovEMKYLunxAHNYtm4gl7/2Iu/62HbdPSMWvrh4Ag66TD3GxW4ETOxrD9/jmxv28sX2BIfOA3hOA1IlAWHzn1oWIqJMxjKlNyVHB+PvPx+H3X+3Hqo0F+PF4JV6+dQSSIoO89yK2euDEj27h+z1gPaes69EfGHpTY/iG9vTe6xIR+QCGMbWLQafBk9cMxKjUaPzyo12YuWIDnr9xKKb1j7u4DdosQMl2ZX9vYZ4SvrY6ZV3Py4Dhtyrh23sCENrDe2+EiMgHMYypQ2YMTsBlCeG4593tuGP1Vtw9JQO/uKIvdNo2hq2tZqBkW2P4Fv2gXFQAAOIGASMXNIZvSEznvxEiIh/CMKYOS40NwSf3jMdvvtiHV3OOYFthBV68eTjiwt0OBbKageItzvDdqISv3QJAAPGDgKw7nOE7HgiOVu29EBH5AoYxXRSTXovfzx2M0WlRePSTPbjuhW+wcqoDg627lfAt3uK8FJ4AEoYAoxc5w3ccEBSldvWJiHwKw5guTv05oOgHXFexEVclrYf21HYY/mODAxqIxKEQoxcDqZOAXmOBoEi1a0tE5NMYxtQ+llqg6Hul11uQp0y+clgBoUVQ4jBYx96NN0qSseJQDIZpe+FPE4YhNtSodq2JiLoFhjG1zFKjzHAuzFPC98SPyvVuhRZIGgGMW+Ls+Y4BjGHQA7hTSoRtKcKTn+/FzBUb8OLNIzA6jfuDiYjawjAmhblaObGGK3x3KNfI1eiApJHA+GXKMb4pYwBjaIubEELgptG9MCQ5Ekve246b39iMh6b3w+JJ6dBoeLEEIqLWMIwDVV2lZ/ie3AlIh3LJv+QsYOIDzvAdrVwLtwMuSwzH5/dOwCMf78YzXx3AlmPl+L8bhyIy2NA574WIqJtjGAeKugrlfM6FG5WzXJ3arYSv1gAkjwIm/a8SvsmjAEPwJb9cmEmPl24ZjtGborF83T7MXJGHl28dgWEpkZf+XoiI/AzD2E/prDXA/rVu4bsHgAS0RiVws3/pDN8sQO/F01q6EUJgwfhUDEuJxD3vbse8177DozMGYOH4VAhe45eIyIVh7C/OlTmvZqTMdp5wei8AqVzcPnkUMOVXyhWNkrK6/Dq9Q1Mi8eWySfjF33fgN1/sw5aCcjxz/RCEm/RdWg8iIl/FMO6uzp1tPLtVQR5wZp+yXBcEpIxGQeotSJv6U2XylU79Q4wigvV442dZeGPDUfzhnwex74QybD0wMULtqhERqY5h3F3UnvEM39IDynJ9sDLDedD1yrBz4ghAZ0BhTg7Seo9Xt85NCCGwODsDI3pF4d73fsR1r3yH38weiJtGpXDYmogCGsPYV9WcbpzpXJAHnM1XlutDlLNaDZnvDN/hgLZ7DfdmpUZj3bKJuP+DHfjVJ7vxw7FyLL92EEKM/DgSUWDit5+vqD7ZONmqYCNQdkhZbghTwnfYrUr4JgztduHbkphQI1bfPhovf3sYL3yTj90lVXj11hHIjAtTu2pERF2OYayWqhLP8C0/oiw3hgO9xgEjfqZMuIofCmj9859JqxFYdnkmsnpHYdmaHzH7pY347XWDMHdEstpVIyLqUv75Le+LKos8w7fimLLcGKFcRjDrdqXnGz8E0GjVrWsXG98nFl8um4Sl7/+IBz/ciR+OleOp2QNh0gdWOxBR4GIYd5aKwsbJVgV5QGWhstwUqVxKcPQiJXzjBgVc+LakZ7gJ7945Bn/6Jh8vf3sEO4ur8MqtI5AW27GzfxERdUcMY2+QEqgocAvfjUDVcWVdUJQSvmPvVsK350BAo1G1ur5Kp9Xgoen9kZUajQc+2IFrXszDH64fgplDEtSuGhFRp2IYXwwplWHmhl5vwUagulhZFxyjhO/4e5Xw7TGA4dtBU/v1xLplk3Dve9ux5L3t+OFYbzw6cwCMOo4gEJF/Yhi3h5RA+dHG/b0FeUDNCWVdcKwSuqn3K7ex/Ri+XpAUGYQPFo/DH/95AG/mHcOOokq8dMsIpERf+nmziYh8DcO4JVICZYc9w7f2lLIupKczfCco1/ON7QvwhBWdwqDT4LFZlyErNRoPfbQTM1dswPM3DsNPLotTu2pERF7FMAaU8D2b7xm+584o60LjPcM3pg/Dt4tdNSgeAxLCsOS97bjzna34eXY6/nd6P+i1HIEgIv8QmGEspXI6yYZ9voUbgXOlyrqwRCB9SmP4RqczfH1A75gQfHTXeCxftw8rc49iW2EFXrxlOBIiOueKU0REXSkwwtjhAEr3e4bv+TJlXXgSkDHN2fudCESlMXx9lEmvxfJrB2NUajR+9cluzFyRhxfmD0N23x5qV42I6JL4Zxg7HMCZvZ7hW1ehrItIATKvVIK39wQgKpXh283MGZaEgYkRWPLudixY9QOWTsvEfZdnQqvhvyMRdU/+EcYOO0JrjgKb9ir7fAs3AuZKZV1kb6DfDLfw7a1qVck7+vQMxWdLJuDxf+zBiv8cwtaCcvz5puHoEab+5SKJiDrKP8L40L+Qte0B5X5UGjBglrK/t/cEIDJF3bpRpwkyaPHcvKEYnRaNJ/6xBzNWbMCLNw/H2PQYtatGRNQh/hHGvcdj34AHcNlVi4CIJLVrQ13sxqwUDEmOwD3vbsctb2zGL67sh7snZ0DDYWsi6ibadWyIEOIqIcRBIcRhIcQjLaxfKIQoFULscP7d6f2qXoApAmfipjCIA1j/+HB8fu9EzBySiGe/Pog73t6CinP1aleLiKhd2gxjIYQWwMsArgZwGYCbhRCXtVD0AynlMOffm16uJ1GbQo06rLhpGJ6+dhC+O1yGmSs2YFthhdrVIiJqU3t6xqMBHJZSHpVS1gNYA2BO51aL6OIIIXDb2N74+O7x0GoF5q/chDc3HIWUUu2qERG1qj37jJMAFLk9LgYwpoVy1wshsgHkA3hASlnUQhmiLjE4OQJrl07CQ3/fieXr9iNEDwzK34QBCeHoHx+G/gnh6BsXimCDf0ybIKLuTbTVYxBC3ADgKinlnc7HtwEYI6W8161MDIBaKaVFCPFzAPOllNNa2NZiAIsBIC4ubuSaNWu89kZqa2sRGhrqte11d2wPhZQS35+0Y88ZM06ZtSiuccBsV9YJAD2DBVLCNEgJ0yDZeRsbJKDx42PP+dnwxPbwxPZo1BltMXXq1G1Syqymy9sTxuMAPCWlnO58/CsAkFL+vpXyWgDlUsqIC203KytLbt26tZ3Vb1tOTg6mTJnite11d2wPTw3t4XBIlFTWYf/Jahw4VYMDp6px4GQNjpWdQ8N/hWCDFv3iw9A/PhwDEpTbfvFhiAjSq/smvISfDU9sD09sj0ad0RZCiBbDuD1jdFsAZAoh0gCUALgJwC1NNp4gpTzpfDgbwP5LrC9Rp9BoBFKig5ESHYwrB8a7ltfV25F/Wgnn/SeV26/2nMT7Pxx3lUmKDHIOcTcGdWpMCHS8YAURXaI2w1hKaRNC3AvgawBaAG9JKfcKIf4fgK1Sys8BLBNCzAZgA1AOYGEn1pnI64IMWgxNicTQlEjXMiklTldbsN/Zez5wqhoHT9VgfX4pbA6lG23QadA3LhT945V90QMSlF50bCjPBEZE7deu2StSyi8BfNlk2RNu938F4FferRqRuoQQiI8wIT7ChKn9erqW19scOFJa6xri3n+qBrn5pfhoW7GrTGyo0TnErfSi+yeEoU/PUBh1WjXeChH5OE4lJeogg06DAQnhGJAQDgxvXF5Wa8HBU0o4H3Duk35nUyEsNgcAQKsRyOgR4grnAc7b+HAThB9PGCOitvlFGNtKS2H64QdU19dDYzJBGE3QmIwQJhM0RuVWGI3OdUYIDffxkffFhBoxvo8R4/vEupbZ7A4UlJ139aIPnKrGtsIKfL7zhKtMRJDeNcTdPz4M/Zx/POyKKHD4xf92y6FDiHhrFUraWV7o9UpAm4zQGN1vG8NbYzJCGIye60xGCGOT55lMEAajK/wbQ7/xB4EwGNjzCVA6rQZ9eoaiT89QzBrSuLzabEV+k17037cW4Vy9ctyVEEDv6GBXL7phwlhKVHDAn3NbSgnYbJD19XDU10PW10NaLMptfT0cFgtkvdX52OK2rF5ZbrFAWhuXhRYW4vSWLRAaLaDRQGg1gNAAWo3yw12jhdAI5baFddAIz+e6lYdGQGi1gGhc13xZw7Y0ja/hscx5X6tVvke02paXNd2eVgsIwe+ebsIvwjho+HCcfepJjBo6FA6zGdJicd7WQ1rMyn2zBQ6Lcqssc95aLB7rHOfOwVFeDmluWNd4C4fjousojJ499aa3roA3GlsPfaPbDwKjodVRAI3RCPCMUz4t3KRHVmo0slKjXctaO+zq632nfOKwK2m3XzDwpMXiFo71kNZ6zyC01LueL+udZd2XuUKyybL6ejisbmUtFu98voWAMBgQBKBiQx7gcEA6HMr/c3/6/6PxDHdXyGu1LSzTINZSj8MhIY3rXD9A3H8MOH+AaDUQDevcf4A0+XHisczttYTrh03rP4Q8XkvTwo+dC72Wc3sX+iHUWKfmP4R0hYWQNhuErvOj0i/CWBMUBHt8PEwDBnTq60irtTGgXaGuhL8rtBt+CDQNfbOzTL3Fc53ZDHt1NeQZc+OPBbNZ+aIymy/6S6GnEDhwodB36/ELo6H5CIEz9D1/GBhb2QXgXNYFH1h/1tJhV1JK1J0z41BxGQ4Vl+NoSTkKTpVi9+FD2HrODL3DBr3DhoQgDXqH6dErTIfEEC0Sg7WI1gMam9UzOC0WhBUW4sRXXzWGnnuvsqWepvMWNptX3qfQ65XRIqNRuTUYoDEaIPQG13JNcLDyuTQ4lzeUNTrLGwzKyJXB4Fqucd+m3q2s0b2cEUJvgMagB/R6CCFaPJZUSqmEst2u3LfbXUEt7Xbl/6XdDumQgMPeGORuy1zB7nBA2h2AdD632bILrbMDru05b53LpMN9nQNw357bMimbrnMoz21pmUOi5sQJRPXo4fHjxPVadrtrex7LrFblx1rDa7m3R9NlTbfbyjr3Nlfrx1EMAMfcudCGhXX6a/HbswOEXg+tXg900dlppJTKh9zcJPTde/XNfhwot8cOHkSvuHgl4C31TXr6ZjjKK2Dz+JGglJEWy8VXWKdzBrax5aH/ZrsAWgl99+H+lnYBuP8Q6IT9/y0Og7r30to5DOpR1iPcLtBTdC9rsUBarQAAA4CBzr/2cAA46/6eNBrAYITGaIARwPmwMLdA1EOjN0ATGgqtW7gpoecZeB7hpnd7vnvguZ7jttz1HH23mLPh6gFqtQi0Qd78nByM8LGTfrT848jtx0lrP44cTX6ctPRjp+G5Lfw42r1rFzRBQV3yHhnGPkw4h9FgMEAbHt6h5+7JyUHcRfyHkg6HEgTu4e0xClDfpMffwi6AJkP/DeVt1dWN22wYITCbXYFzMVz7/43GlkPfGfARp06j6P01yjBoveeQZ7Nh0Pr6S9ol0Vg50aQHqG8SbsqPFm14uOcy9xDTN3m+636TXqXzuTaNDkW1NhyqtCC/zIL9ZRbsLT2PU+cae7XhBoGhvWN42BV1G2r9OKoHumzEj2FMHoRGA2EyASYTuuqrWdrtyvC+K/zdh/7d9vu3tgvA0nzo32GxwHG+Do6KSkizGTqLGTaLpXEYNLKVYVD3Ic/2DoO6L29hGLSrDXD+uXM/7Crnx3xUnre2ethVv/gw1/7ohAgedkXUFRjGpDqh1UIEB0MTHNxpr5GTk4PBPjb01pXcD7vKsBViypSJHoddHTxVg/0na7D9uOdhV+EmHfonhGOA80pX/XnYFVGn4P8oogDVkcOuPtpWzMOuiDoRw5iIPHjjsCtlf7RyPyLYP652RdSZGMZE1KZLudpVYoTJNcTdMOSdFsurXRG5YxgT0UVr7WpXZ2osjb1o521uk6tdZfYM9Th5Sf8EXu2KAhfDmIi8SgiBuHAT4sJNmNLG1a42HCrFx9s9r3blGuJ29qb79AyFSc/Drsi/MYyJqEt05GpXf93sedhVemyIK5x52BX5I4YxEanqQle7OniqcX/0j8cr8EUbh131jQtDiJFfa9T98FNLRD7H/bCrmUMSXMs7ctiV+8lLekXzsCvybQxjIuo2LuWwq75xjeHMw67I1zCMiahba89hV8qs7hp8tecU3v+hyFUmMcKEME09/la4BaFGHUJNOoQa9Qgz6RBm0inLnMvDjHrnemWdUafhPmvyGoYxEfml9hx2tf9kNQ4UnsLJKjNqLTbUmm2osdhQb2v7QiF6rfAMcNd9Z5CbdMoyow6hJr1ruSvknQFv0jPUiWFMRAGkpcOulOsZT/IoZ7HZUWu2odZiQ43z1vXYYkON2ep63BDgtWYbSmssOHb2HGrMVtSYba4Z4Rei1QhXD9wzrBsDvPl6z156qFGHYIOWod6NMYyJiJow6rQwhmoRc4knIam3OXDO4hnqNWZryyFvblxXdq4ehWXnXSFfZ7W3+VoaAWc469167I09dPcQD2uyPtxtnaNhRzt1KYYxEVEnMeg0MOgMiAoxXNJ2bHaHZ4A36ZHXWqzOMPcM+Mo6K4orzruWn69vO9QFgNCcr5sMuevdhtw9e+Rhpua99FCTDiEGHbScwd5uDGMiIh+n02oQGWxAZPClhbrdIT3CvNZiRbXZ5jHkvufgYcTEJ7sCvtZiQ3WdFScq6xrLWWzter3QVgLc1WM36Zv10sPc1jWUDYRQZxgTEQUIrUYgIkiPiKDWD+nKcRzHlCmXXXA7DofEuXr3oXfPgG+pl6704q04XW1W1pltqK23oT2j4sEGbfOwbmG/ecPjcLfeuvt6X744CcOYiIg6RKMRSq/WdGnHaTscEuetdo8Qd4V7k2H4xv3qSpmzNec89sE72hHqJr2meW+8lR55mEmHY2dsGGezw6jr/HOjM4yJiEgVGreZ5IDporcjpUSd1d5Cj9zarPfeuF4J8ePl5z2W25uk+v/MdjCMiYiI2iKEQLBBh2CDDnHhF78dKSXMVgdqLMqha+u/+x4hhq6JSYYxERERlFAPMmgRZNCiZxhwPELbZec099292URERAGCYUxERKQyhjEREZHKGMZEREQqYxgTERGpjGFMRESkMoYxERGRyhjGREREKmMYExERqYxhTEREpDIh23P9qs54YSFKARR6cZOxAM56cXvdHdvDE9ujEdvCE9vDE9ujUWe0RW8pZY+mC1ULY28TQmyVUmapXQ9fwfbwxPZoxLbwxPbwxPZo1JVtwWFqIiIilTGMiYiIVOZPYfy62hXwMWwPT2yPRmwLT2wPT2yPRl3WFn6zz5iIiKi78qeeMRERUbfU7cJYCHGVEOKgEOKwEOKRFtYbhRAfONd/L4RIVaGaXaYd7bFQCFEqhNjh/LtTjXp2BSHEW0KIM0KIPa2sF0KIFc622iWEGNHVdexK7WiPKUKIKrfPxhNdXceuIoRIEUJ8K4TYJ4TYK4S4r4UyAfH5aGdbBNJnwySE+EEIsdPZHr9poUzn54qUstv8AdACOAIgHYABwE4AlzUpcw+A15z3bwLwgdr1Vrk9FgJ4Se26dlF7ZAMYAWBPK+tnAPgKgAAwFsD3atdZ5faYAmCt2vXsorZIADDCeT8MQH4L/1cC4vPRzrYIpM+GABDqvK8H8D2AsU3KdHqudLee8WgAh6WUR6WU9QDWAJjTpMwcAG87738E4HIhhOjCOnal9rRHwJBS5gIov0CROQDekYrNACKFEAldU7uu1472CBhSypNSyu3O+zUA9gNIalIsID4f7WyLgOH89651PtQ7/5pOpur0XOluYZwEoMjtcTGaf4hcZaSUNgBVAGK6pHZdrz3tAQDXO4fdPhJCpHRN1XxSe9srkIxzDs99JYQYqHZluoJziHE4lB6Qu4D7fFygLYAA+mwIIbRCiB0AzgD4t5Sy1c9GZ+VKdwtj6rgvAKRKKYcA+Dcaf90RbYdyar6hAF4E8Jm61el8QohQAB8DuF9KWa12fdTURlsE1GdDSmmXUg4DkAxgtBBiUFfXobuFcQkA955dsnNZi2WEEDoAEQDKuqR2Xa/N9pBSlkkpLc6HbwIY2UV180Xt+fwEDClldcPwnJTySwB6IUSsytXqNEIIPZTweVdK+UkLRQLm89FWWwTaZ6OBlLISwLcArmqyqtNzpbuF8RYAmUKINCGEAcqO9M+blPkcwALn/RsA/Fc697r7oTbbo8k+r9lQ9g8Fqs8B/Mw5a3YsgCop5Um1K6UWIUR8w34vIcRoKN8HfvnD1fk+/wJgv5Ty+VaKBcTnoz1tEWCfjR5CiEjn/SAAVwA40KRYp+eKzpsb62xSSpsQ4l4AX0OZSfyWlHKvEOL/Adgqpfwcyofsr0KIw1Amr9ykXo07VzvbY5kQYjYAG5T2WKhahTuZEOJ9KLNAY4UQxQCehDIZA1LK1wB8CWXG7GEA5wHcrk5Nu0Y72uMGAHcLIWwA6gDc5Mc/XCcAuA3Abue+QQB4FEAvIOA+H+1pi0D6bCQAeFsIoYXyo+NDKeXars4VnoGLiIhIZd1tmJqIiMjvMIyJiIhUxjAmIiJSGcOYiIhIZQxjIiIilTGMiYiIVMYwJiIiUhnDmIiISGX/H4qJkbylc9PqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e34a861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  52    9   20   78  310   17   30]\n",
      " [   5    5    2   16   57    4   10]\n",
      " [   7    2    3    9   35    2    2]\n",
      " [  27    7   13  171  248   13   16]\n",
      " [  97   15   60  229 1123   47   44]\n",
      " [  28    5   14   25  139   37   15]\n",
      " [  23    7   19   33  161    9  100]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5f1ba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for each emotion:\n",
      "Emotion 0: 0.1377\n",
      "Emotion 1: 0.0671\n",
      "Emotion 2: 0.0314\n",
      "Emotion 3: 0.3239\n",
      "Emotion 4: 0.6090\n",
      "Emotion 5: 0.1888\n",
      "Emotion 6: 0.3515\n"
     ]
    }
   ],
   "source": [
    "f1_per_emotion = f1_score(y_test, y_pred, average=None)\n",
    "print(\"F1 score for each emotion:\")\n",
    "for idx, score in enumerate(f1_per_emotion):\n",
    "    print(f\"Emotion {idx}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f547e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score(macro) 0.24420160221697104\n",
      "F1 score(micro) 0.4385294117647059\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score(macro)\",f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1 score(micro)\",f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ea255d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger     0.2176    0.1008    0.1377       516\n",
      "     disgust     0.1000    0.0505    0.0671        99\n",
      "        fear     0.0229    0.0500    0.0314        60\n",
      "         joy     0.3048    0.3455    0.3239       495\n",
      "     neutral     0.5417    0.6954    0.6090      1615\n",
      "     sadness     0.2868    0.1407    0.1888       263\n",
      "    surprise     0.4608    0.2841    0.3515       352\n",
      "\n",
      "    accuracy                         0.4385      3400\n",
      "   macro avg     0.2764    0.2381    0.2442      3400\n",
      "weighted avg     0.4079    0.4385    0.4108      3400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report (includes F1 score for each class, precision, recall, and support)\n",
    "target_names = list(labelDecoder.values())  # Assuming labelDecoder is defined\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed47d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame({\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': y_pred\n",
    "})\n",
    "\n",
    "\n",
    "file_name = f\"data/dump/{dataset_path}/CNNBiLSTM_data_for_classifier/CNN_predictedTest.pkl\"\n",
    "directory = os.path.dirname(file_name)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(df_predictions, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992c643",
   "metadata": {},
   "source": [
    "#### TODO Get the predicted label of saved in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aff141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec07ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa9a5a2",
   "metadata": {},
   "source": [
    "#### Trying the BERT version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df9d0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[\"Utterance\"].to_numpy()\n",
    "x_train = np.asarray(x_train).astype(str)\n",
    "y_train_encoded = np.asarray(y_train_encoded).astype('float32')\n",
    "\n",
    "x_test = X_test[\"Utterance\"].to_numpy()\n",
    "x_test = np.asarray(x_test).astype(str)\n",
    "y_test_encoded = np.asarray(y_test_encoded).astype('float32')\n",
    "\n",
    "x_dev = X_dev[\"Utterance\"].to_numpy()\n",
    "x_dev = np.asarray(x_dev).astype(str)\n",
    "y_val_encoded = np.asarray(y_dev_encoded).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "477ae5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a463ac07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_15060\\2871278467.py:5: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  x_train_rv = X_test[\"Utterance\"].ravel()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_15060\\2871278467.py:6: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  x_dev_rv = X_dev[\"Utterance\"].ravel()\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 3\n",
    "ff_dim = 32\n",
    "\n",
    "x_train_rv = X_test[\"Utterance\"].ravel()\n",
    "x_dev_rv = X_dev[\"Utterance\"].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ee139ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_4 (KerasLayer)     {'input_type_ids':   0           ['Text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_5 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer_4[4][0]',          \n",
      "                                None, 768),                       'keras_layer_4[4][1]',          \n",
      "                                 'sequence_output':               'keras_layer_4[4][2]']          \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'default': (None,                                                \n",
      "                                768)}                                                             \n",
      "                                                                                                  \n",
      " bidirectional_7 (Bidirectional  (None, 128, 128)    426496      ['keras_layer_5[4][12]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_block_7 (Transform  (None, 128, 128)    206752      ['bidirectional_7[0][0]']        \n",
      " erBlock)                                                                                         \n",
      "                                                                                                  \n",
      " global_average_pooling1d_7 (Gl  (None, 128)         0           ['transformer_block_7[0][0]']    \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 7)            903         ['global_average_pooling1d_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,116,392\n",
      "Trainable params: 634,151\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_input = Input(shape=(), dtype=tf.string, name='Text')\n",
    "preprocess_text = bert_preprocess(text_input)\n",
    "encode_output = bert_encoder(preprocess_text)['encoder_outputs'][11]  # Output of the last encoded layer\n",
    "\n",
    "# BiLSTM layer\n",
    "bilstm = Bidirectional(LSTM(\n",
    "    units=64,\n",
    "    dropout=0.2,\n",
    "    return_sequences=True))(encode_output)\n",
    "\n",
    "\n",
    "att = TransformerBlock(embed_dim, num_heads, ff_dim)(bilstm)\n",
    "pool = GlobalAveragePooling1D()(att)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(7, activation='softmax')(pool)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=text_input, outputs=output)\n",
    "\n",
    "model=keras.Model(text_input,output)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                               mode='auto', \n",
    "                                               patience=3, \n",
    "                                               verbose=1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c84e18c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (12840,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train shape: {x_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cbebf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "402/402 [==============================] - 2959s 7s/step - loss: 1.3449 - accuracy: 0.5229 - val_loss: 1.3462 - val_accuracy: 0.5055\n",
      "Epoch 2/15\n",
      "402/402 [==============================] - 3041s 8s/step - loss: 1.2279 - accuracy: 0.5550 - val_loss: 1.3357 - val_accuracy: 0.5007\n",
      "Epoch 3/15\n",
      "402/402 [==============================] - 2940s 7s/step - loss: 1.1298 - accuracy: 0.5879 - val_loss: 1.4588 - val_accuracy: 0.4945\n",
      "Epoch 4/15\n",
      "402/402 [==============================] - 3010s 7s/step - loss: 0.9927 - accuracy: 0.6447 - val_loss: 1.5401 - val_accuracy: 0.4774\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    x_train, y_train_encoded, \n",
    "          epochs=15, \n",
    "          batch_size=32, \n",
    "          verbose=1,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_dev, y_dev_encoded)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df94f2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp8AAAGlCAYAAAC84Z1eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABgoElEQVR4nO3deXycVaH/8c+ZJcskkz1N2jTd09KVrS2L7MhWqWwuLCqIqCioXDfkqojXBfQnKIr3ekUFUepyURFtQQqCSkEoS+m+N22Tpmn2TPZZzu+PmUwyWdokTWayfN+v17ySeZ7zzHPmMKTfOc95zjHWWkRERERE4sGR6AqIiIiIyMSh8CkiIiIicaPwKSIiIiJxo/ApIiIiInGj8CkiIiIicaPwKSIiIiJxo/ApIiIiInGj8CkiIiIiceNKdAUGwhhjgCmAL9F1EREREZF+eYFD9iirGI2J8Ek4eJYluhIiIiIickxTgfL+do6V8OkDOHjwIBkZGSN+Mr/fz7PPPsvFF1+M2+0e8fNJmNo9MdTuiaF2Twy1e2Ko3RMj3u3e2NhIcXExHONK9VgJnwBkZGTELXx6PB4yMjL0P0kcqd0TQ+2eGGr3xFC7J4baPTFGa7vrhiMRERERiRuFTxERERGJG4VPEREREYkbhU8RERERiRuFTxERERGJG4VPEREREYkbhU8RERERiRuFTxERERGJG4VPEREREYkbhU8RERERiRuFTxERERGJG4VPEREREYkbhU8RERGRcSZkQ1S2VOIL+RJdlV5cia6AiIiIiAxeIBSgormCg40HOeg7yAHfAQ74DnCw8SBlTWW0B9s5O/ls3s/7E13VGAqfIiIiIqNUR7CD8qbycLhsPBANmGW+Msp95QRsoN9jXcZFh+2IY20HRuFTREREJIFaA60c9B3sswezorkCi+332CRHEsXeYoozipnmncY077To87ykPJ595tk4vpOBUfgUERERGWG+Dl80WB5sjPyMBM4jrUeOeqzH5WFaRiRUeiMhM/J8kmcSDtP3LTx+v38k3spxU/gUEREROU7WWurb62NCZfcezLr2uqMen5GUEe61zIgNl8XeYnJTcjHGxOmdjDyFTxEREZEBsNZS3VodDpWNB7p6MiNh0+c/+p3luSm5/fZgZiZnxuldJJ7Cp4iIiEhEMBSksqWyzx7MMl8ZrYHWox5f4ClgWkbX2MvuYTPNnRandzG6KXyKiIjIhOIP+aloqojpwezsxSzzleEP9T9W0mEcTEmb0mcPZlF6ESmulDi+k7FJ4VNERETGnfZgO2W+sl6Xxw80HqCiuYKgDfZ7rMvhYmr61JgezM5ezClpU3A73XF8J+OPwqeIiIiMSS3+lq6piXr0YFY2Vx51iqIUZwrFGcUUp3ddGu8MmwWeApwOZxzfycSi8CkiIiKjVkN7Q1eojEyy3vl7TVvNUY9Nd6fHhMruPZj5qfnj6g7ysUThU0RERBLGWkttW22/PZgN7Q1HPT47OTvcgxkZe9k9bGYlZylgjkIKnyIiIjKiQjZEQ6iB1ytfp6K1IqYH86DvIM3+5qMen5+a37sHMxI4M5Iy4vQuZLgofIqIiMhxC4QCHG4+3HsFn8ijPdgOz/d9rMEwOW1yTA9m54TrU9On4nF74vtmZEQpfIqIiMiA+IN+yprK+hyDWd5UTiAU6PdYBw6K0ouYljmt1xrkU9OnkuRMiuM7kURS+BQREZGo1kBreIqiSA9m92mKKporCNlQv8cmOZKiN/VEl4n0TmOyZzJv/eMtVr5rJW63pima6BQ+RUREJpimjqaYy+LRHszGgxxpPXLUY1NdqTHLQna/yWeSZxIO4+h1jN/vZ6PZOFJvR8YYhU8REZFxxlpLQ3tDdFnInj2YtW21Rz3em+Rlund6bA9mJGzmpuTqDnI5LgqfIiIiY5C1lurW6l5TFHUGTF+H76jH56TkxITK7stEZiZnxuldyESk8CkiIjJKhWyIyubKrh5M38GYO8lbA61HPb7AU9BnuCz2FpPmTovTuxCJpfApIiKSQP6Qn4qmij57MMt95XSEOvo91mEcTE6b3GcP5lTvVFJcKXF8JyIDo/ApIiIywtqD7ZT7yvtcwedQ0yGCNtjvsS6Hi6npU6M39XTvwZySNgW3U3ePy9ii8CkiIjIMWvwtMaGye8g83HwYi+332BRnClO9U3v3YGZMo9BTiNPhjOM7ERlZCp8iIiID1NjRGB1z2bMHs7q1+qjHprnTeq093vl7fmq+7iCXCUPhU0REJMJaS21bbb89mPXt9Uc9Pis5K7osZHQFn0jAzE7OVsAUQeFTREQmmJANcaTlCIdaD8VMst4ZNpv9zUc9Pj81PyZUdobNYm8xGUkZcXoXImOXwqeIiIxb/pCf3XW72VS9ic3Vm9lcvZnShlL8T/r7PcZgKEwr7LMHs9hbjMftieM7EBl/FD5FRGRcsNZS1lTGpqpN0bC5rXYb7cH2XmWdxklRelGfPZhF6UUkO5MT8A5EJgaFTxERGZNq22qjvZmdYbOvMZneJC+L8xazKG8R87Pmc+CtA1z7rmvxJKsHUyQRFD5FRGTUaw20sr12O5uqNkXDZllTWa9yboeb+TnzWZS3iEV5i1ict5hpGdNwGAcAfr+fNRvX4HZobkyRRFH4FBGRUSUYCrK3YW9Mj+bOup19TsQ+M3NmtFdzSd4S5mbP1aTrIqOcwqeIiCSMtZbKlko2VYfHaW6q2sSWmi19rlmel5rH4rzFLMlfwqK8RSzMXYg3yZuAWovI8VD4FBGRuGnsaGRL9ZZo2NxcvbnPydk9Lg8L8xayOG9xtGezwFOgeTJFxgGFTxERGREdwQ521O6IhsxN1ZsobSztVc5lXJRkl0RD5uK8xczMnKklJUXGKYVPERE5biEbYn/j/phxmttrt+MP9Z5Ps9hbHA2Zi/MWc0LOCaS4UhJQaxFJBIVPEREZtOrW6pj5NDfXbMbX4etVLjs5Oxo0O+9Az07JTkCNRWS0UPgUEZGjavG3sKVmS7RXc1P1Jg43H+5VLtmZzILcBdE7zxflLaIovUjjNEUkhsKniIhE+UN+9tTvYWPVxmjY3Nuwl5ANxZQzGGZnzQ5fOs8PXz6fnTVb82eKyDEpfIqITFCdy1F2H6e5rWYbbcG2XmUL0wpj7jxfkLuANHdaAmotImOdwqeIyARR11bXaznKuva6XuW8bm/MCkGL8haR78lPQI1FZDxS+BQRGYfaAm3h5Si7Td7e33KUJ+ScEBM0p2dMjy5HKSIy3BQ+RUTGuGAoyL6GfTHzae6q20XABnqVnZExI2Y+zXk580hyJiWg1iIyUQ06fBpjzgG+AJwKTAaustY+OcBj3wH8A9hsrT1psOcWEZnoOpej3Fy9mY3V4ZuCtlRvoSXQ0qtsbkpu9GagxXmLWZi3kIykjATUWkSky1B6PtOAt4FfAH8c6EHGmCzgMeB5oGAI5xURmXB8Hb5e4zSrWqt6lUt1pbIwd2FM2NRylCIyGg06fFprnwaeBgb7R+0nwCogCFw52POKiIx3HcEOdtbtjLl8vq9hX69yTuNkbvbcmHGaszJnaTlKERkT4jLm0xjzYWAW8AHgKwMonwwkd9vkBfD7/fj9vZdqG26d54jHuaSL2j0x1O6J0d7RTnWwmqd2P8X2+u1srtnMjrodfS5HOTV9KgtzF7IwdyGLchcxL3seqa7UmDKhYIhQMNTrWImlz3tiqN0TI97tPtDzGGvtkE9ijLEcY8ynMaYEeAk421q70xhzD3Dl0cZ8Rsp8ref2VatW4fF4hlxfEZFEaQo1URYsoyxQFv4ZLKPN9p5P02M8THVOpchZxFTXVKY6p5Lm0HyaIjL6tbS0cP311wNkWmsb+ys3oj2fxhgn4UvtX7PW7hzEofcCD3R77gXKLr74YjIyRn6wvN/vZ+3atVx00UW43VqtI17U7omhdh9+Lf4WttVtC98MVLOFzTWbOdzSezlKF67ocpSLcsPzahalaTnKkaTPe2Ko3RMj3u3e2Nhv3owx0pfdvcBS4GRjzEORbQ7AGGMCwMXW2r/3PMha2w60dz7v/EPsdrvj+qGN9/kkTO2eGGr3oQmEAuyu3x0zTnNP/Z5+l6PsHKc5P2s+u17ZxcpLVqrdE0Cf98RQuydGvNp9oOcY6fDZCCzuse2TwAXAe4DeI+lFREYpay3lTeUxd55vrdna53KUBZ4CluQviYbNnstR+v1+9pq98ay+iMioMJR5PtOBOd02zTTGnATUWmsPGGPuBYqstR+y1oaAzT2OPwK0WWtjtouIjDb1bfVsrukKmpurN1PbVturXLo7PebO80V5i5jkmZSAGouIjH5D6flcCrzQ7Xnn2MxfAjcRnnh+2vFVS0QkvjqXo+w+eftB38Fe5VwOFydkR5ajzA+HzRkZM7QcpYjIAA1lns8XgX5Hw1trbzrG8fcA9wz2vCIiwyUYClLaWBpd8/xYy1F29mYuzlvMCTknaDlKEZHjoLXdRWTcq2yuDAfNyOXzLTVbaPY39yqXk5LDkrwl0V7NhbkLyUzOTECNRUTGL4VPERlXfB2+8PRG1ZvZVBUOm0daj/Qql+pKZUHugq6wmbeYwrRCTXMkIjLCFD5FZMzyB/3R5Sg7ezX3NezDErt4htM4mZM1J7rueedylC6H/gSKiMSb/vKKyJhgreWA70DXfJpVm9hWu63P5SiL0ouiIbNznKbHrdXRRERGA4VPERmVqlur2VK9JaZXs7Gj9+oZmcmZ0ZC5OC88TjM3NTcBNRYRkYFQ+BSRhGvxt7C1ZmvM5O2Hmg/1KpfkSGJ+7vxo0Fyct5ip3qkapykiMoYofIpIXAVCAfbU74lZjnJ3/e4+l6OclTmLRXmLoisFlWSX4HZoaT4RkbFM4VNERoy1lkPNh8JBsyocNLfVbqM10Nqr7CTPpJg7zxfkLiA9KT0BtRYRkZGk8Ckiw6ahvSHam9nZs9nfcpQL8xZ2LUeZu4iCtIIE1FhEROJN4bOHxo5GfvTaz6lqPcCRrUdIdiXjcrhiHk7jDP9uum1zOGOed+53Ovov63a4cTqcWpZPxqT2YHvXcpRV4eUoD/gO9CrncriYlz0v5qagGZlajlJEZKJS+OzhYH0Vv93zCwCe3/BcXM7pMI6YoOp2uKMBNxp0uwXaaIDtue1YZc0AQ7TDhdu4Y87hNM6YenWWdRt3v/VxOVwKGONEyIYobSiNrnm+qXoTO2t39rkc5fSM6dGguShvESfknECyMzkBtRYRkdFI4bOHap8h2HA6QRvAEAITeRDE7QyRnuogLdmQmmRISQKnwxK0AQKhrkfQBqM//SF/+PdQkIAN9LqpAsL/sHfYDui9a8wzmAGHYQcOfD4ff1j7B5KcSb16lJ2OcADuFbS7v+7Rep+PFtSPUra/YO8wjnF7l3Vlc2XMneebazb3uxxlZ8hckreEhXlajlJERI5O4bOHc2fP4o2PPcSjf3yG7NknsvVwE5vKG9h6qBFfIISvR3lviotFUzI5ZWomi4oyWTI1k2k5nn5DSciGCIYiodRGQmlncI2E2M6g2j3Q9lvWBmPL9SjrD/ljysSU736+UAC/9ceco7Ns99eI7u8jcAdtsNf7tVj8IX+fE4H352DVwcH8J0uonj3VRw3GR+t9PlaI7lY2pmf8GIG7r7Kdwz06ywYDQfb49/DIlkfYWreVTdWbONLS93KU83PmR+88X5y3mMlpk8dtABcRkZGh8NkHl9PBlDRYcUoR17rD07r4gyF2H2liU1kDm8ob2FjewLaKRnxtAV7ZW8Mre2uix2ekuFjcGUaLslhclElxTirGGBzGgcPpwO0cf9PFdAbr7sG0r3DcV6gNhAK0+9t5df2rnHjKiWCIhuho73Efwbm/sN5v2Z716SfU9yzbV7AGovvHhbe7fnUYR3g5yryu5ShnZ83WcpQiInLc9C/JALmdDuZPzmD+5Azet6wYCAfSnZU+Npc3sLGsgc3lDWyr8NHYFmDd7hrW7e4KpJmpbhYXdfWOLi7KZGp26rjqNYoGa4YWrP1+P763fVw07SLc7tEVzq21vXqK+w21PYPxYMraSCjv0QPdqwe7r7L99HjHhP1uQb37cVkmi2XFyzhx0oksylvE/Jz5Wo5SRERGhMLncXA7HSycksnCKZm8f1l4W0egWyAtDwfS7RU+Glr9vLS7mpd2V0ePz/KEA2n0MTWToqzxFUjHC2MMbuMelxOcd3R08PTTT7PirBWjLvSLiMj4o/A5zJJcDhZFejivjWzrDKQbI5fsN5XXs+Owj/oWP//aVc2/dnUF0myPm8VTs1hclMHioiwWT81kSmaKAqmMGH22REQknhQ+46B7IO3UHgiy47AvHEYjoXTHYR91LX7+ubOKf+6sipbNTUuKXq5fFOklnaxAKiIiImOQwmeCJLucLJmaxZKpWXBaeFubPxxIN5Y3sDkSSHdW+qhp7uAfO6v4R7dAmpceCaTRcaRZFGQkK5CKiIjIqKbwOYqkuJ2cWJzFicVZ0W1t/iDbD/vYVFYfvsu+rIFdR5qoburgxR1VvLijeyBNjvaOLomMIS3ISEnAOxERERHpm8LnKJfidnJScRYn9QikWysaY+6y31npo7qpnb9vP8Lft3fN0TjJmxy9manz5ySvAqmIiIgkhsLnGJTidnLKtGxOmZYd3dbaEQ6k4R7SRjaV17P7SBNHfO08v/0Iz3cLpAUZyeGbmbqNI833avlDERERGXkKn+NEapKTU6dnc+r0rkDa0hFg66HGmJuadlc1UdnYTmVjJc9tq4yWnZyZEr2ZqbOXNC9dgVRERESGl8LnOOZJcrF0Rg5LZ+REtzW3B9ha0Ri9XL+xrJ691c1UNLRR0dDG2q1dgXRKJJB2v8s+V4FUREREjoPC5wSTluxi2YwclnULpE3t4R7SjWX10cnx91U3c6ihjUMNbTzbLZAWZaXGjiEtyiQ7LSkRb0VERETGIIVPIT3ZxfKZOSyf2RVIfW1+thyKvalpb3Uz5fWtlNe38syWw9GyU7N7B9IsjwKpiIiI9KbwKX3yprg5fVYup8/KjW5rbPOzJXIz06by8M1NpTUtlNW1UlbXytObuwJpcU4qS4qyusaRFmWS6dHSjSIiIhOdwqcMWEaKmzNm53LG7K5A2tDqZ0t557Kh4cf+mhYO1rZysLaV1ZsqomWn5XiivaNLijJZWJRJZqoCqYiIyESi8CnHJTPVzZlz8jhzTl50W0OLn82HGmLusj9Q2xJ9rN7YFUhn5HqiNzXNL0inNZCIdyEiIiLxovApwy7T4+Ydc/J4R7dAWt/SwebyRjaW10fHkZbVtVJa00JpTQt/jQZSFz/Z+xJLpmZFx5EunJKBN0U9pCIiIuOBwqfERZYnibNK8jirpCuQ1jV3dF2uL2tgU3k95fVt0UD61NuHomVn5adFx44ujlyyT0/Wx1dERGSs0b/ekjDZaUmcMzefc+bmA+D3+/n9n9cwecFyth5uiobSQw1t7K1qZm9VM3/eEA6kxsCsvEggjfSSLpySQZoCqYiIyKimf6llVEl3w9kleVywYHJ0W3VTO5vKG9hcFp6DdHN5AxUNbeypamZPVTNPdguks/PTWVKUGR1HumBKBp4kfcxFRERGC/2rLKNeXnoy58+bxPnzJkW3Vfna2Ry5ZN85D+nhxjZ2H2li95Em/vhWOQAOA3MmpYfDaGQM6YLJmaQmORP1dkRERCY0hU8Zk/K9yZx/wiTOP6ErkB7xtcVMir+xrIEjvnZ2Vjaxs7KJP77ZFUhLJnm7JsWfmsmCyRmkuBVIRURERprCp4wbk7wpXHBCChecUBDdVtnYFp3uqfNR5WtnR6WPHZU+nnijDACnw1AyKT1mpab5CqQiIiLDTuFTxrWCjBQKFqTwzgXhQGqtpbKxvdtd9vVsKm+guqmD7Yd9bD/s4/+6BdK5BV4WF2VEb2o6odCrQCoiInIcFD5lQjHGUJiZQmFmChd1C6SHe/aQljVQ09zBtopGtlU08vvXw4HUFQmkS6Z23dQ0r9BLskuBVEREZCAUPmXCM8YwOTOVyZmpXLywEAgH0oqGtq7xo5G77GubO9ha0cjWikZYfxAAt9Mwr9AbmYM03EM6r9BLksuRyLclIiIyKil8ivTBGMOUrFSmZKVy6aKuQFpe3xq9mamzl7S+xc/m8kY2lzfyG7oC6QmFGdHe0cVFmcwtUCAVERFR+BQZIGMMU7M9TM32cOmi8Dyk1lrK6lp7rNTUQEOrP7rtN6+Fj09yOjhhsrdrpaap4UDqdiqQiojIxKHwKXIcjDEU53gozvGwYnFXID1YGw6knWvZbyproLEtwMaycK9ppySXg/mTM1hclMGSoiwWFWVSUpCuQCoiIuOWwqfIMDPGMC3Xw7RcD+9a0hVID9S2xPSObipvwNcW4O2D9bx9sB44AEByJJB2v6lpTn46LgVSEREZBxQ+ReLAGMP03DSm56Zx+ZIpAIRC4UDaeTPTxrJ6tpQ34msPsOFgPRsO1kePT3E7WDA5I2Yt+9n5aQqkIiIy5ih8iiSIw2GYkZfGjLw03n1iVyAtrWmO6SHdcqiRpvYAbx6o580D9cB+IBxIF07JjBlDOjs/HafDJO5NiYiIHIPCp8go4nAYZuWnMys/nStOKgLCgXRfTXPMXfZbyhto7gjyxv463thfFz3ek+QM95BG7rBfMjWTmXkKpCIiMnoofIqMcg6HYXZ+OrO7BdJgyLKvuplN5fVsKmtkU3k9Ww410tIR5PX9dbzeI5AumtI1fnRRUSaz8tJwKJCKiEgCKHyKjEFOh2HOpHTmTErnqpPD24Ihy96qpvBd9pHJ8TsD6WultbxWWhs9Pi3JycLI5foFhelUtUBHIITbnaA3JCIiE4bCp8g44XQYSgq8lBR4ufqUqUA4kO6paupaqamsnq0VjTR3BHltXy2v7esMpC6+t+l5ZualMbfAG3mkU1LgZUauRzc2iYjIsFH4FBnHnJG16OcWeHnPqeFAGgiG2F3VxKZIIH27rJ5t5fW0h2DXkSZ2HWli9aaK6GskOR3Myk+LCaRzC7xMy/FoLKmIiAyawqfIBONyOjihMIMTCjN479Ji/H4/q1ev4eR3nM/e2jZ2Hvaxs7KJXUd87KpsotUfZPthH9sP+2JeJ9nlYM6kdOYWeCkpSGfupHAonZqdqvGkIiLSL4VPEcEYmJKVyvT8DM6fNym6PRQKr2e/s9LHjspwGN1Z6WP3kSbaAyG2HGpky6HGmNdKdTspKUinZFK4p7QznBZlpWKMQqmIyESn8Cki/XI4upYPvXB+QXR7MGQ5WNsSCaThntKdlT72VjXT6g/2WkYUwjc5lRR0D6Th3wszUhRKRUQmkEGHT2PMOcAXgFOBycBV1tonj1L+auATwElAMrAFuMda+7ch1FdERgFntwnyL1lYGN0eCIbYX9sSvXS/80g4nO6taqa5I9hr5SYAb4qrazzppK6bnfK9yQqlIiLj0FB6PtOAt4FfAH8cQPlzgLXAfwL1wIeBvxhjTrPWvjWE84vIKOVyOqJzkl62uGt7RyBEaU0zOyO9pOHeUh+lNS342gK9JssHyPK4mTspMp40cul+XoGX3PTkOL8rEREZToMOn9bap4GngQH1Slhr7+ix6T+NMVcAKwGFT5EJIMnliN513117IMjeqnAo7RxPuutIE/trmqlv8feanxQgNy2pWyD1Mjdy01N2WlI835KIiAxR3Md8GmMcgBeoPVZZERnfkl1O5k/OYP7kjJjtbf4gu4+E77jv6ilt4kBtCzXNHdTsreXfe2P/hOR7k3tdui8p8JKZqpnzRURGk0TccPR5IB34fX8FjDHJhMeHdvIC+P1+/H7/yNYucp7uPyU+1O6JMRrb3QnMm+Rh3iQPLOq60amlI8CequbIfKThn7uPNFFe30aVr50qXzvrdtfEvFZBRjIlk9Ijj7TwylD56XhTEnu/5Whs94lA7Z4YavfEiHe7D/Q8xlo75JMYYyzHuOGoR/nrgYeBK6y1zx2l3D3A13puX7VqFR6PZ2iVFZFxqy0Ih1vgcKuhosVEf6/v6H9oUHaSpdBjmZwKhZ7w74WpkOyMY8VFRMaRlpYWrr/+eoBMa21jf+XiFj6NMdcSvknpvdba1cco21fPZ1l1dTUZGRn9HDV8/H4/a9eu5aKLLsKtxa7jRu2eGOO53Rtb/eyO9pQ2RXpKmznia+/3mKnZqZRMSuvWW5rO7Pw0UtzDm0rHc7uPZmr3xFC7J0a8272xsZG8vDw4RviMy3UnY8x1hIPntccKngDW2nYg+q9D541Nbrc7rh/aeJ9PwtTuiTEe2z3X7SY3w8Nps/Njtte3dETnJu0cT7rriI/qpg7K6lopq2vlhR3V0fIOA9NyPDHzlM4t8DIrP41k1/GF0vHY7mOB2j0x1O6JEa92H+g5hjLPZzowp9ummcaYk4Baa+0BY8y9QJG19kOR8tcDvwQ+A7xqjOmcFLDVWhs7C7WISBxkeZJYPjOH5TNzYrbXNLVHg+jObpPn17f4Ka1pobSmhbVbK6PlnQ7D9FxPZGnRdOYWhkPpjNw0klyOeL8tEZExYSg9n0uBF7o9fyDy85fATYQnnp/Wbf/HIuf5ceRBj/IiIqNCbnoyZ6Qnc8bs3Og2ay1VTe3RqaA6777fUenD1xZgb1Uze6uaeWZL1+u4HIaZeWkx85OWFHiZkevB5VQoFZGJbSjzfL4I9DuK31p7U4/n5w32HCIio4UxhkneFCZ5U3jHnLzodmstlY3tkUAamaf0SPhnU3sgOsaUTV2vleR0MCs/jZICL7PzPPhqDQtqmpk1KROnQ6s5icjEoLXdRUSGwBhDYWYKhZkpnDO3a0yptZZDDW2x40kjP1v9QbYf9rH9sC9S2snPd6wj2RVeGSp66T4yV+nU7FQcCqUiMs4ofIqIDCNjDEVZqRRlpXL+vEnR7aGQpby+NXrpfkdFA6/vPkRVu5P2QIitFY1srYi9OTTV7WTOpPTopPmdl/GLslK17r2IjFkKnyIiceBwGIpzPBTneLhwfgF+v581aw5yyaUXc9jnjy4tuuNw+DL+3qpmWv1BNpU3sKk89t7MtCQncwq8zOu+zGhBOoUZKQqlIjLqKXyKiCSQ02GYkZfGjLw0Ll7YtT0QDLG/tiV6yX5H5DL+vupmmjuCvH2wnrcP1se8ljfFRcmkdOYVemOWGc33JiuUisioofApIjIKuZzhcaCz89O5dFHXdn8wRGl1c3QaqM5HaU0LvrYAbx6o580D9TGvlZnqjpmftCTye156MiIi8abwKSIyhridDkoil9rfxeTo9vZAkH2RULqr0seOw+HL+Ptrmmlo9bO+tI71pXUxr5WTltRtLKmXuZHxpdlpSfF+WyIygSh8ioiMA8kuJycUZnBCYewSxG3+IHuqmrrNUxq+jH+wroXa5g7+vbeWf++tjTkm35vM3IL0mEv3JQVeMlO1Mo2IHD+FTxGRcSzF7WThlEwWTsmM2d7aEWT3kdhL9zsrmyivb6XK106Vr511u2tijinMSIlesu8MpCWT0vGmKJSKyMApfIqITECpSU4WT81k8dTYUNrUHugKpYd97DwSvoxf0dDG4cbw41+7qmOOKcpKjYbSkkldU0J5kvRPjIj0pr8MIiISlZ7s4qTiLE4qzorZ3tjmZ1e3pUU7L+Mf8bVTXt9KeX0rL+6oijmmOCeVuZO6poKaW+BlzqR0UtzOOL4jERltxk34DIVCdHR0DMtr+f1+XC4XbW1tBIPBYXlNObZ4tHtSUhIOh9bWFhmsjBQ3p07P5tTp2THb61s62NW9p7SyiV1HfFQ3dXCwtpWDta08v/1ItLzDwLQcT0wgnVvgZVZ+GskuhVKRiWBchM+Ojg727dtHKBQaltez1lJYWMjBgwc1N14cxaPdHQ4HM2fOJClJd/OKDIcsTxLLZuSwbEZOzPba5o7oEqM7ui0zWtfip7SmhdKaFtZurYyWdzoM03M9kaVFw+NJ5xV6mZGbRpJLXxhFxpMxHz6ttVRUVOB0OikuLh6WXq1QKERTUxPp6enqJYujkW73UCjEoUOHqKioYNq0afpiITKCctKSOH1WLqfPyo1us9ZS3dTRK5DurPTR2BZgb1Uze6uaeWZL1+u4HIaZeWkx85POLfAyI9eDy6m/zyJj0ZgPn4FAgJaWFqZMmYLH4xmW1+y8hJ+SkqLwGUfxaPf8/HwOHTpEIBDA7dYduiLxZIwh35tMvjeZM+fkRbdbaznia2dn5/yklU3sPBL+2dQeYNeRJnYdaYJNXa+V5HQwKz8tOj9pZ0/ptBwPToe+WIqMZmM+fHaODdRlVBmIzs9JMBhU+BQZJYwxFGSkUJCRwtkl+dHt1loqGtqiS4t29pTuOtJES0eQ7Yd9bD/si3mtZFd4ZajopftIT+nU7FQcCqUio8KYD5+ddAlVBkKfE5GxwxjDlKxUpmSlcv68SdHtoZClvL41OjfprkofO4/42H2kiTZ/iK0VjWytaIx5rVS3kzmT0pkzKQ1nvWHWYR8LirLVSyqSAOMmfIqIyMTgcBiKczwU53i4cH5BdHswZCmra4kuLdoZTvdUNdHqD7KpvIFN5Q2Akyd+/AreFBdLp2ezNHLD1JKpmZoGSiQOFD4T5LzzzuOkk07iBz/4QaKrIiIyLoTvmE9jem4aFy/s2h4Ihthf28KuSh9byut59s3dlLW68bUFeGFHFS9E5idNcjpYMjUzEkazWTo9h0yPhueIDDeFTxERGddczvA40Nn56Vw4L485bTu5+JLz2VPTxmv7anl9fy2v7aujuqmd1/fX8fr+On7yj/Cx8wq8LJ2RzfKZOSydkUNRVmpi34zIOKDwKSIiE47L6WBRUSaLijK5+ayZWGvZX9PC+tJaXi+tY31pLXurm9kRmRbq8VcPADAlM4VlkSC6bEY2cyd5dSOTyCBpHqFRoK6ujg996ENkZ2fj8Xi47LLL2LVrV3T//v37WblyJdnZ2aSlpbFw4ULWrFkTPfaGG24gPz+f1NRUSkpKeOSRRxL1VkRExiRjDDPy0njv0mK+854l/P3z5/H6V97JTz5wKrecNZMTp2bidBgONbTx5w2H+OqTm7n0B//ipP96lpsfXc9/v7ib9aW1tAe0Kp7IsYy7nk9rLa3+4/ufPxQK0doRxNURGNR8k6lu55Dupr7pppvYtWsXTz31FBkZGdx5552sWLGCrVu34na7ue222+jo6OCf//wnaWlpbN26lfT0dAC++tWvsnXrVp5++mny8vLYvXs3ra2tg66DiIjEyktP5tJFhVy6qBCAlo4AGw7U81qkd/TNA3U0tgX4+/Yj/D2yhGiSy8GJkXGjy2fkcMr0bDJTNW5UpLtxFz5b/UEW3P23hJx7639dgidpcE3aGTrXrVvHmWeeCcDjjz9OcXExTz75JO9973s5cOAA11xzDYsXLwZg1qxZ0eMPHDjAySefzNKlSwGYMWPG8LwZERGJ4UlyceacvOgE+YFgeFqn9aV1vF5ay/rSWqqbOlhfWsf60jr+hz0YEx43umxGDktnZLNsRg5TNG5UJrhxFz7Hmm3btuFyuTjttNOi23Jzc5k3bx7btm0D4NOf/jSf+MQnePbZZ3nnO9/JNddcw5IlSwD4xCc+wTXXXMObb77JxRdfzJVXXhkNsSIiMnJcTgdLpmaxZGoWH4mMGy2NjBtdv6+W1/fXsa+6OToZ/q/+vR+AoqzU8N30M3JYPjOHOfnpGjcqE8q4C5+pbidb/+uS43qNUCiEr9GHN8M76MvuI+GWW27hkksuYfXq1Tz77LPce++93H///XzqU5/isssuY//+/axZs4a1a9dy4YUXctttt/G9731vROoiIiJ9Mya8Dv3MvDTet7QYgCpfe6RXtI7X99ey5VAj5fWtlG9o5ckNhwDITHWzdHo2y2aGb2JaVJRJskvzjcr4Ne7CpzFm0Je+ewqFQgSSnHiSXCO+tvv8+fMJBAK8+uqr0R7LmpoaduzYwYIFC6LliouLufXWW7n11lu56667ePjhh/nUpz4FhNcrv/HGG7nxxhs5++yz+cIXvqDwKSIyCuR7k7ls8WQuWzwZgOb2AG8dqA/fVb+/ljf319PQ6uf57Ud4PjJuNNnl4MTirGjv6KnTs8lI0bhRGT/GXfgca0pKSrjiiiv46Ec/yv/+7//i9Xr50pe+RFFREVdccQUAd9xxB5dddhlz586lrq6OF154gfnz5wNw9913c+qpp7Jw4ULa29v561//Gt0nIiKjS1qyi7NK8jirJDxu1B8MsfVQY/hSfeRGpprmDl7bV8tr+2ohMm70hMKMrkv1M3IozExJ7BsROQ4Kn6PAI488wmc+8xkuv/xyOjo6OOecc1izZg1ud/ibbjAY5LbbbqOsrIyMjAwuvfRSvv/97wOQlJTEXXfdRWlpKampqZx99tn89re/TeTbERGRAXI7w72cJxZnccvZs7DWsq+6ORJGwzcylda0sK2ikW0VjTz2Snjc6NTsVJZFlgVdNiOb2Ro3KmOIwmeCvPjii9Hfs7Ozeeyxx/ot+6Mf/ajffV/5ylf4yle+MpxVExGRBDHGMCs/nVn56bx/2TQAjvjaohPfry+tZeuhRsrqWimrK+dPb5UDkOVxs3R6TrR3dHFRJkkuTeUto5PCp4iIyCg2yZvCisWTWREZN9rUHuCtA+HpnNbvq+Wtg3XUt/h5blslz22rBMLjRk8qzopO8XTq9Gy8Gjcqo4TCp4iIyBiSnuzi7JJ8zi7JB8LjRrccamT9vsi40f111DZ38Oq+Wl7dVwuAo9u40fBd9TkUZGjcqCSGwqeIiMgY5naGezlPKs7io+eEx43uqWqOTvG0vrSWA7UtbK1oZGtFI7+MjBstzuk9bnQoq/SJDJbCp4iIyDhijGHOpHTmTErn2uXhcaOVjbHjRrdVNHKwtpWDteX88c3wuNFsj5ulkSC6bEYOC6do3KiMDIVPERGRca4gI4V3LZnMu5aEx4362vzR+UbXl9by1oF66lr8rN1aydqt4XGjKe5wj+ryGTksnZHDydOyNG5UhoXCp4iIyATjTXFzztx8zpkbHjfaEQix+VBD12pMpbXUtfj5995a/r23a9zo/MkZMZfqJ2ncqAyBwqeIiMgEl+RycMq0bE6Zls3HzoFQyLK3uil6R/36/bUcrG1ly6FGthxq5NGXSwGYnuuJTvG0bGYOs/LSNG5UjknhU0RERGI4HIY5k7zMmeTlusi40cMNbZFVmMK9o9sON7K/poX9NS384c0yAHLSklg6PZvlM8OX6hdOycDt1LhRiaXwKSIiIsdUmJnCyhOnsPLEKQA0tvl5c39d9EamDQfrqW3u4NmtlTwbGTea6nZy8rQsTinOJFhvOLc9QJZb40YnOoVPERERGbSMFDfnzZvEefMmAdAeCLK5vDHSMxqeb7S+xc/Le2p4eU8N4OSn336BBZMzWBq5o37pjGwmeTVudKJR+BQREZHjluxycur08GpKHz93NqGQZU9VE6+V1vLa3hr+tf0Qte2wqbyBTeUNPLKuFIAZuR6WzsiJ3FWfzUyNGx33FD4lyu/349blEBERGQYOh6GkwEtJgZf3nTKFNWsOcvI7LmBDuY/XS2t5bV8tOyp9lNa0UFrTwhNvhMeN5qUnsXR6TrR3dOGUDFwaNzqu6L9mAj3zzDOcddZZZGVlkZuby+WXX86ePXui+8vKyrjuuuvIyckhLS2NpUuX8uqrr0b3/+Uvf2HZsmWkpKSQl5fHVVddFd1njOHJJ5+MOV9WVhaPPvooAKWlpRhj+N3vfse5555LSkoKjz/+ODU1NVx33XUUFRXh8XhYvHgxv/nNb2JeJxQK8d3vfpc5c+aQnJzMtGnT+Na3vgXABRdcwO233x5TvqqqiqSkJJ5//vnhaDYRERmjJmem8O4Tp/BfVyzimTvOYcPdF/PIh5dx2/mzWT4jhySXg+qmDp7Zcphvrt7GFT9ex5KvP8sNP/s331+7k3W7q2luDyT6bchxGn89n9aCv+X4XiMUCr9GhxMcg8jnbg8M4lJBc3Mzn/3sZ1myZAlNTU3cfffdXHXVVWzYsIGWlhbOPfdcioqKeOqppygsLOTNN98kFAoBsHr1aq666iq+/OUv89hjj9HR0cGaNWsG+0750pe+xP3338/JJ59MSkoKbW1tnHrqqdx5551kZGSwevVqPvjBDzJ79myWL18OwF133cXDDz/M97//fc466ywqKirYvn07ALfccgu33347999/P8nJyQD8+te/pqioiAsuuGDQ9RMRkfErM9XN+fMmcX7MuNEGXtsXnmv09f11NLT6Wbe7hnW7awBwOgwLp2RE5xo9dXoO+d7kRL4NGaTxFz79LfDtKcf1Eg4gaygH/uchSEobcPFrrrkm5vkvfvEL8vPz2bp1Ky+//DJVVVWsX7+enJwcAObMmRMt+61vfYtrr72Wr3/969FtJ5544qCrfMcdd3D11VfHbPv85z8f/f1Tn/oUf/vb3/j973/P8uXL8fl8PPjggzz00EPceOONAMyePZuzzjoLgKuvvprbb7+dP//5z7zvfe8D4NFHH+Wmm27SGB4RETmq8LjRHE6dngOEx43uOtIUM8VTeX0rG8sa2FjWwM9f2gfAzLw0ls3IjiwPmsOMXI/+zRnFxl/4HEN27drF3Xffzauvvkp1dXW0V/PAgQNs2LCBk08+ORo8e9qwYQMf/ehHj7sOS5cujXkeDAb59re/ze9//3vKy8vp6Oigvb0dj8cDwLZt22hvb+fCCy/s8/VSUlL44Ac/yC9+8Qve97738eabb7J582aeeuqp466riIhMLA6HYV6hl3mFXj5w+nQAyutbu+6oL61jR6WPfdXN7Ktu5vevd44bTY6G0eUzcpg/2atxo6PI+Aufbk+4B/I4hEIhGn0+MrxeHIO97D4IK1euZPr06Tz88MNMmTKFUCjEokWL6OjoIDU19ajHHmu/MQZrbcw2v9/fq1xaWmxP7f/7f/+PBx98kB/84AcsXryYtLQ07rjjDjo6OgZ0Xghfej/ppJMoKyvjkUce4YILLmD69OnHPE5ERORYirJSKTqpiCtOKgKgocXPGwe6lgV9+2AD1U3tPL35ME9vPgyAJ8nJKdOyo5fqT5qWhSdp/EWgsWL8tbwxg7r03adQCNzB8OsMJnwOQk1NDTt27ODhhx/m7LPPBuCll16K7l+yZAk/+9nPqK2t7bP3c8mSJTz//PN8+MMf7vP18/PzqaioiD7ftWsXLS3HHgu7bt06rrjiCj7wgQ8A4SC+c+dOFixYAEBJSQmpqak8//zz3HLLLX2+xuLFi1m6dCkPP/wwq1at4qGHHjrmeUVERIYi0+PmghMKuOCEAgDa/EE2lTdEe0ZfL62lsS3AS7ureWl3NRAeN7ooMm50aSSQ5qZr3Gi8jL/wOUZkZ2eTm5vLT3/6UyZPnsyBAwf40pe+FN1/3XXX8e1vf5srr7ySe++9l8mTJ/PWW28xZcoUzjjjDL72ta9x4YUXMnv2bK699loCgQBr1qzhzjvvBMJ3nT/00EOcccYZBINB7rzzzgFNo1RSUsITTzzByy+/THZ2Ng888ACVlZXR8JmSksKdd97JF7/4RZKSknjHO95BVVUVW7Zs4SMf+Uj0dTpvPEpLS4u5C19ERGQkpbidkR7OcMdNKGTZecTXtU59aS0VDW28XdbA22UN/CwybnRWfhrLIlM8LZ+Zw7QcjRsdKQqfCeJwOPjtb3/Lpz/9aRYtWsS8efP44Q9/yHnnnQdAUlISzz77LJ/73OdYsWIFgUCABQsW8OMf/xiA8847j//7v//jG9/4Bvfddx8ZGRmcc8450de///77+fCHP8zZZ5/NlClTePDBB3njjTeOWa+vfOUr7N27l0suuQSPx8PHPvYxrrzyShoaGqJlvvrVr+Jyubj77rs5dOgQkydP5tZbb415neuuu4477riD6667jpQUrV4hIiKJ4XAYTijM4ITCDD7YbdxoZxDtHDe6t6qZvVXN/O71gwDke8PjRjuD7AmFGjc6XBQ+E+id73wnW7dujdnWfZzm9OnTeeKJJ/o9/uqrr+51p3qnKVOm8Le//S1mW319ffT3GTNm9BoTCpCTk9NrftCeHA4HX/7yl/nyl7/cb5nq6mra2tpiekNFRERGg6KsVIpOLuLKk8PjRutbOnhjf124d7S0lo1l9VT52lmz6TBrNoXHjaYnuzh5WlZ0WdCTi7NJTXIm8m2MWQqfMqz8fj81NTV85Stf4fTTT+eUU05JdJVERESOKsuTxIXzC7hwfte40Y1l4XGj60treaO0Dl97gH/tquZfu8LjRl0Ow6KizJgpnnLSkhL5NsYMhU8ZVuvWreP8889n7ty5R+21FRERGa1S3E6Wz8xh+czwuNFgyLKz0hcJo+Gxo4cb29hwsJ4NB+t5+F/hcaOz89Oil+mXzcihOCdV40b7oPApw+q8887r83K+iIjIWOV0GOZPzmD+5Aw+dEZ42FpZXSuv7+8Ko7uONLGnqpk9Vc38dn143GhBRnK4V3R6uHd0/uQMnA6FUYVPERERkUEwxlCc46E4x8NVJ08FoK65c9xo+FL9pvIGKhvbWb2xgtUbw1Mfpie7OGV6NsumZ7NsZg4nFWeR4p5440YHHT6NMecAXwBOBSYDV1lrnzzGMecBDwALgYPAN621jw723CIiIiKjUXZaEu9cUMA7F3SNG91wsD66LOib+8PjRv+5s4p/7qwCwO0MjxtdHplvdOn0bLInwLjRofR8pgFvA78A/niswsaYmcBq4CfADcCFwM+MMRXW2r8d9WARERGRMSjF7eT0WbmcPisXCI8b3X64kddLu3pHKxvbeetAPW8dqOd//7kXgDmT0qMrMS2bkcPU7PE3bnTQ4dNa+zTwNDDQxrgV2Get/Vzk+TZjzFnAfwAKnyIiIjLuOR2GhVMyWTglkxvP7Bo32hlE15fWsftIU/Txm9cOAFCYkcLSbvONziv0jvlxo/EY83kG8FyPbX8DftDfAcaYZKD7OldeCE/j03N9cr/fj7WWUChEKBQalgp33jDT+boSH/Fo91AohLUWv9+P0znxxtn0pfP/qZ7/b8nIUrsnhto9MdTufSv0ulm5uICVi8OX6mubO3jzQD2v76/jjQP1bC5v5HBjG3/dWMFfu48bnZbJ0unZnDo9iyVFmf2OG413uw/0POZ47kw2xliOMebTGLMTeMRae2+3bSsIX4r3WGtb+zjmHuBrPbevWrUKj8cTs83lclFYWEhxcTFJSeN/nIQcn46ODg4ePMjhw4cJBAKJro6IiEi/OoKwv8mw1wd7Gw37mgztwdheT6exFKfB7AzLLK9lpteSduzVtEdES0sL119/PUCmtbaxv3Kj9W73ewnfoNTJC5RdfPHFZGRkxBRsa2vj4MGDpKenD9syjtZafD4fXq93VI+zmDVrFp/5zGf4zGc+c8yyTqeTP/zhD1x55ZUjX7Ehike7t7W1kZqayjnnnKNlPyP8fj9r167loosuwu1O0F+sCUjtnhhq98RQuw+PQDDEjsom3jhQz+uldby+v46qpg5Km6C0yfB8pFzJpDROnZ7NSUVeWg9s4v2Xx6fdGxv7zZsx4hE+DwMFPbYVAI199XoCWGvbgfbO551BxO1292q8YDCIMQaHw4HDMTxrrnZe8u183dFsMHUczjYaCfFod4fDgTGmz8/SRKc2SQy1e2Ko3RND7X583G44aXoyJ03P5SNnhzttDta28lppbeSu+lr2VDWz60j48dv1cHKugw/Eqd0Heo54hM9XgBU9tl0U2S4iIiIiQ2CMYVquh2m5Ht5zani+0Zqmdl7fX8frpbW8tq+WOe7aBNeyt0F3Lxlj0o0xJxljTopsmhl5Pi2y/15jzGPdDvkJMMsY811jzAnGmE8C7wO+f7yVH8t++tOfMmXKlF431lxxxRXcfPPN7NmzhyuuuIKCggLS09NZtmwZzz3X876todu0aRMXXHABqamp5Obm8rGPfYympqbo/hdffJHly5eTlpZGVlYW73jHO9i/fz8Ab7/9Nueffz5er5eMjAxOPfVUXn/99WGrm4iIiAxNbnoylyws5MvvWsATHz+NswpH36qDQ7m2uRR4K/KA8NjMt4D/ijyfDEzrLGyt3Qe8i3Bv59vA54BbRmqOT2stLf6W4360BloHfcxgbt5673vfS01NDS+88EJ0W21tLc888ww33HADTU1NrFixgueff5633nqLSy+9lJUrV3LgwIHjbqPm5mYuueQSsrOzWb9+Pf/3f//Hc889x+233w5AIBDgyiuv5Nxzz2Xjxo288sorfOxjH4sOf7jhhhuYOnUq69ev54033uBLX/qSLqOIiIjIgAxlns8XgX7vBrHW3tTPMScP9lxD0Rpo5bRVp8XjVL28ev2reNyeYxcEsrOzueyyy1i1ahUXXnghAE888QR5eXmcf/75OBwOTjzxxGj5b3zjG/zpT3/iqaeeiobEoVq1ahVtbW089thjpKWlAfDQQw+xcuVKvvOd7+B2u2loaODyyy9n9uzZAMyfPz96/IEDB/jCF77ACSecAEBJSclx1UdEREQmjtF798kEcMMNN/CHP/yB9vbwvVWPP/441157LQ6Hg6amJj7/+c8zf/58srKySE9PZ9u2bcPS87lt2zZOPPHEaPAEeMc73kEoFGLHjh3k5ORw0003cckll7By5UoefPBBKioqomU/+9nPcsstt/DOd76T++67jz179hx3nURERGRiGK1TLQ1ZqiuVV69/9bheIxQKRaf8Gcxd16mu1EGdZ+XKlVhrWb16NcuWLeNf//oX3/9+eCjs5z//edauXcv3vvc95syZQ2pqKu95z3vo6OgY1DmG6pFHHuHTn/40zzzzDL/73e/4yle+wtq1azn99NO55557uP7661m9ejVPP/00X/va1/jtb3/LVVddFZe6iYiIyNg17sKnMWbAl777EwqFCLgCeNyeEZ2aKCUlhauvvprHH3+c3bt3M2/ePE455RQA1q1bx0033RQNdE1NTZSWlg7LeefPn8+jjz5Kc3NztPdz3bp1OBwO5s2bFy138sknc/LJJ3PXXXdxxhlnsGrVKk4//XQA5s6dy9y5c/mP//gPrrvuOh555BGFTxERETkmXXZPsBtuuIHVq1fzi1/8ghtuuCG6vaSkhD/+8Y9s2LCBt99+m+uvv37Ylpy84YYbSElJ4cYbb2Tz5s288MILfOpTn+KDH/wgBQUF7Nu3j7vuuotXXnmF/fv38+yzz7Jr1y7mz59Pa2srt99+Oy+++CL79+9n3bp1rF+/PmZMqIiIiEh/xl3P51hzwQUXkJOTw44dOzqXpALggQce4Oabb+bMM88kLy+PO++8c8ArBxyLx+Phb3/7G5/5zGdYtmwZHo+Ha665hgceeCC6f/v27fzyl7+kpqaGyZMnc9ttt/Hxj3+cQCBATU0NH/rQh6isrCQvL4+rr76ar3/968NSNxERERnfFD4TzOFwcOjQoV7bZ8yYwd///veYbbfddlvM88Fchu85DdTixYt7vX6ngoIC/vSnP/W5Lykpid/85jcDPq+IiIhId7rsLiIiIiJxo/A5Djz++OOkp6f3+Vi4cGGiqyciIiISpcvu48C73/1uTjut74n1tfKQiIiIjCYKn+OA1+vF6/UmuhoiIiIix6TL7iIiIiISNwqfIiIiIhI3Cp8iIiIiEjcKnyIiIiISNwqfIiIiIhI3Cp9j2IwZM/jBD36Q6GqIiIiIDJjCp4iIiIjEjcKnJEQwGCQUCiW6GiIiIhJnCp8J8tOf/pQpU6b0CmBXXHEFN998M3v27OGKK66goKCA9PR0li1bxnPPPTfk8z3wwAMsXryYtLQ0iouL+eQnP0lTU1NMmXXr1nHeeefh8XjIzs7mkksuoa6uDoBQKMR3v/td5syZQ3JyMtOmTeNb3/oWAC+++CLGGOrr66OvtWHDBowxlJaWAvDoo4+SlZXFU089xYIFC0hOTubAgQOsX7+eiy66iLy8PLKzs3nXu97Fm2++GVOv+vp6Pv7xj1NQUEBKSgqLFi3ir3/9K83NzWRkZPDEE0/ElH/yySdJS0vD5/MNub1ERERkZIy78GmtJdTScvyP1tZBH2OtHXA93/ve91JTU8MLL7wQ3VZbW8szzzzDDTfcQFNTEytWrOD555/nrbfe4tJLL2XlypUcOHBgSO3icDj44Q9/yJYtW/jlL3/J3//+d774xS9G92/YsIELL7yQBQsW8Morr/DSSy+xcuVKgsEgAHfddRf33XcfX/3qV9m6dSurVq2ioKBgUHVoaWnhO9/5Dj/72c/YsmULkyZNwufzceONN/LSSy/x8ssvM3v2bC6//PJocAyFQlx22WWsW7eOX//612zdupX77rsPp9NJWloa1157LY888kjMeR555BHe8573aNUnERGRUWjcLa9pW1vZccqpw/JalYMsP+/NNzAez4DKZmdnc9lll7Fq1SouvPBCAJ544gny8vI4//zzcTgcnHjiidHy3/jGN/jTn/7EU089xe233z7ImsEdd9wR/X3GjBl885vf5NZbb+W///u/Afjud7/L0qVLo88BFi5cCIDP5+PBBx/koYce4sYbbwRg9uzZnHXWWYOqg9/v57//+79j3tcFF1wQ/T0UCvGDH/yAGTNm8I9//IPLL7+c5557jtdee41t27Yxd+5cAGbNmhU95pZbbuHMM8+koqKCyZMnc+TIEdasWXNcvcQiIiIycsZdz+dYcsMNN/CHP/yB9vZ2AB5//HGuvfZaHA4HTU1NfP7zn2f+/PlkZWWRnp7Otm3bhtzz+dxzz3HhhRdSVFSE1+vlgx/8IDU1NbS0tABdPZ992bZtG+3t7f3uH6ikpCSWLFkSs62yspKPfvSjlJSUkJ2dzbRp02hqaoq+zw0bNjB16tRo8Oxp+fLlLFy4kF/+8pcA/PrXv2b69Omcc845x1VXERERGRnjrufTpKYy7803jus1QqEQjT4fGV4vDsfA87lJTR3UeVauXIm1ltWrV7Ns2TL+9a9/8f3vfx+Az3/+86xdu5bvfe97zJkzh9TUVN7znvfQ0dExqHMAlJaWcvnll/OJT3yCb33rW+Tk5PDSSy/xkY98hI6ODjweD6lHqfvR9gHRNuo+7MDv9/f5OsaYmG033ngjNTU1PPjggxQXF+P3+7nkkkui7/NY54Zw7+ePf/xjvvSlL/HII4/w4Q9/uNd5REREZHQYdz2fxhgcHs/xP1JTB33MYANPSkoKV199NY8//ji/+c1vmDdvHqeccgoQvvnnpptu4qqrrmLx4sUUFhZGb94ZrDfeeINQKMT999/P6aefzty5czl06FBMmSVLlvD888/3eXxJSQmpqan97s/PzwegoqIium3Dhg0Dqtu6dev49Kc/zYoVK1i4cCFJSUlUV1fH1KusrIydO3f2+xof+MAH2L9/Pz/84Q/ZunVrdGiAiIiIjD7jLnyONTfccAOrV6/mF7/4BTfccEN0e0lJCX/84x/ZsGEDb7/9Ntdff/2QpyaaM2cOfr+fH/3oR+zdu5df/epX/OQnP4kpc9ddd7F+/Xo++clPsnHjRrZv387//M//UF1dTUpKCnfeeSdf/OIXeeyxx9izZw///ve/+fnPfx59/eLiYu655x527drF6tWruf/++wdUt5KSEn71q1+xbds2Xn31VT72sY/F9Haee+65nHPOOVxzzTWsXbuWffv28fTTT/PMM89Ey2RnZ3P11VfzhS98gYsvvpipU6cOqZ1ERERk5Cl8JtgFF1xATk4OO3bs4Prrr49uf+CBB8jOzubMM89k5cqVXHLJJdFe0cE68cQTeeCBB/jOd77DokWLePzxx7n33ntjysydO5dnn32Wt99+m+XLl3PGGWfw5z//GZcrPDLjq1/9Kp/73Oe4++67mT9/Pu9///s5cuQIAG63m9/85jds376dJUuW8J3vfIdvfvObA6rbz3/+c+rq6jjllFO48cYb+fjHP86kSZNiyvzhD39g2bJlXHfddSxYsIAvfvGL0bvwO3UOIbj55puH1EYiIiISH2Yw0wMlijEmA2hoaGggIyMjZl9bWxv79u1j5syZpKSkDMv5QqEQjY2NZGRkDGrMpxyf42n3X/3qV/zHf/wHhw4dIikpqd9yI/F5Gev8fj9r1qxhxYoVuN3uRFdnwlC7J4baPTHU7okR73ZvbGwkMzMTINNa29hfuXF3w5FMLC0tLVRUVHDffffx8Y9//KjBU0RERBJP3XrjwOOPP056enqfj865Oser7373u5xwwgkUFhZy1113Jbo6IiIicgzq+RwH3v3ud3Paaaf1uW+8X9645557uOeeexJdDRERERkghc9xwOv1ailJERERGRN02V1ERERE4mbchM+xcNe+JJ4+JyIiIok15sOn0+kEGNKykzLxdH5OOj83IiIiEl9jfsyny+XC4/FQVVWF2+0elnk5Q6EQHR0dtLW1aZ7POBrpdg+FQlRVVeHxeKKT54uIiEh8jfl/gY0xTJ48mX379rF///5heU1rLa2traSmpg56vXYZuni0u8PhYNq0afrvKiIikiBjPnwCJCUlUVJSMmyX3v1+P//85z8555xzxv1URaNJPNo9KSlJvdkiIiIJNC7CJ4R7tIZruUSn00kgECAlJUXhM47U7iIiIuOfuoBEREREJG4UPkVEREQkbhQ+RURERCRuFD5FREREJG4UPkVEREQkbhQ+RURERCRuFD5FREREJG4UPkVEREQkbhQ+RURERCRuFD5FREREJG4UPkVEREQkbhQ+RURERCRuFD5FREREJG4UPkVEREQkbhQ+RURERCRuFD5FREREJG6GFD6NMbcZY0qNMW3GmFeNMcuPUf4OY8wOY0yrMeagMeb7xpiUoVVZRERERMaqQYdPY8z7gQeArwOnAG8DfzPGTOqn/PXAfZHy84GPAO8Hvj3EOouIiIjIGDWUns/PAg9bax+x1m4FbgVagJv7KX8msM5au8paW2qtfRb4DXDU3lIRERERGX9cgylsjEkCTgXu7dxmrQ0ZY54DzujnsJeBDxhjlltrXzPGzAJWAL86ynmSgeRum7wAfr8fv98/mCoPSec54nEu6aJ2Twy1e2Ko3RND7Z4YavfEiHe7D/Q8xlo74Bc1xkwByoEzrbWvdNv+XeBca+1p/Rz3aeB7gCEceH9irf3EUc5zD/C1nttXrVqFx+MZcH1FREREJD5aWlq4/vrrATKttY39lRtUz+dQGGPOA/4T+CTwKjAHeNAY81Vr7Tf6OexewuNKO3mBsosvvpiMjIwRrG2Y3+9n7dq1XHTRRbjd7hE/n4Sp3RND7Z4YavfEULsnhto9MeLd7o2N/ebNGIMNn9VAECjosb0AONzPMd8AfmWt/Vnk+SZjTBrwU2PMt6y1oZ4HWGvbgfbO58YYANxud1w/tPE+n4Sp3RND7Z4YavfEULsnhto9MeLV7gM9x6BuOLLWdgBvABd2bjPGOCLPX+nnMA/QM2AGOw8fzPlFREREZGwbymX3B4BfGmNeB14D7gDSgEcAjDGPAeXW2rsi5f8CfNYY8xZdl92/AfzFWhtERERERCaMQYdPa+3vjDH5wH8BhcAG4FJrbWWkyDRiezq/CdjIzyKginAg/fLQqy0iIiIiY9GQbjiy1j4EPNTPvvN6PA8QnmD+60M5l4iIiIiMH1rbXURERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERE4kbhU0RERETiRuFTREREROJG4VNERERkvLKhRNegF1eiKyAiIiIig2AttNZBUyX4Dvf709V0mPnZ5wOXJ7rGMRQ+RUREREaDUBCaq6HpMPgqe/zsDJWV4Z/B9mO+nAFS/HUjX+9BUvgUERERGUmBjnBgjPZM9hUuK6G5Cmxw4K+bkgXeQkgv6POnPyWXjS9vZPKIvbGhUfgUERERGYqO5qNe9o7+bK0dxIsaSMsHbwGkF3b72SNcpheAO+XoL+X3E3TuPK63OBIUPkVEREQ69RpPeaT/nsoO38Bf1+GOhMeeobLHz7R8cI7veDa+352IiIgIDPt4yii3p9/L3l0/CyE1GxyaZAgUPkVERGQsG7HxlJn99FD2CJfJXjBm5N7fOKTwKSIiIqNPQsdTTgJ36oi9tYlO4VNERETio3M8ZV89kxpPOWHov4CIiIgcn1AIWqp79Ex2hUln42HeWbUP18aPajylKHyKiIhIP442nrLpSFfYbDpy1PGUDiCt+waNp5zQhhQ+jTG3AV8ACoG3gU9Za187Svks4FvA1UAOsB+4w1q7ZijnFxERkeMwYuMp8/oMlQFPHi9v3MsZF1+JO6tI4yknuEGHT2PM+4EHgFuBV4E7gL8ZY+ZZa4/0UT4JWAscAd4DlAPTgfoh11pERERiWQtt9XEeT1nQ1VOZNqnf8ZTW76du7xrImg5u9/C8XxmzhtLz+VngYWvtIwDGmFuBdwE3A/f1Uf5mwr2dZ1pr/ZFtpUM4r4iIyMRzjPGUMZfBA20Df12Np5QEGVT4jPRingrc27nNWhsyxjwHnNHPYe8GXgF+bIy5AqgCVgHfsXYwE26JiIiMI4EOaD7Sx0TnPX4eYzxlLxpPKaPcYHs+8wAnUNljeyVwQj/HzAIuAB4HVgBzgP8G3MDX+zrAGJMMJHfb5AXw+/34/f6+DhlWneeIx7mki9o9MdTuiaF2T4y4tHtHMzRVYiI36pjmI7HPO3+21Az4JW3neMq0AmzkUnf0p7fb87RBzE8ZCAzxDQ6ePu+JEe92H+h5jLV2wC9qjJlCeMzmmdbaV7pt/y5wrrX2tD6O2QmkADM7ezqNMZ8FvmCtndzPee4BvtZz+6pVq/B4PAOur4iIyLCwFnewhRR/PcmBelL84Ueyv54UfwMpga7n7tDAL32HcNLuzqTNnUWbO5M2Vxbt7qzI86zwPlcW7e4MrNEENTK6tbS0cP311wNkWmsb+ys32E9yNRAECnpsLwAO93NMBeDvcYl9G1BojEmy1nb0ccy9hG9q6uQFyi6++GIyMjIGWeXB8/v9rF27losuugi3BkbHjdo9MdTuiaF2T4xe7W5DkfW+++iZ7P6z+QhmEOMpbWQ8ZbR3Mm1S7PPOG3U8ObiMg3QgfeTedsLp854Y8W73xsZ+82aMQYVPa22HMeYN4ELgSQBjjCPy/KF+DlsHXG+McVhrQ5Ftc4GKfoIn1tp2IDoLrYmMSXG73XH90Mb7fBKmdk8MtXtiqN2HgbXgb4V2H7Q3hh9tjd2e+yLPG3G21nPa3s2kVNyPo/nIiI2nNJHxlBpRGUuf98SIV7sP9BxD6cN/APilMeZ14DXCUy2lAZ13vz8GlFtr74qU/x/gduBBY8yPgBLgP4EfDuHcIiIyngT9kXDY0GdYjH1+lP2hgY1fdBCeoJqYDpr+56fsNa2Q5qcUOW6DDp/W2t8ZY/KB/yL8//AG4FJrbedNSNOAULfyB40xlwDfBzYSHjP6IPCd46u6iIgkTCgEHU19hMOGfsJiX+HSB4HWYayUgeSM8F3cKZGfPZ4H3els2nuYRadfiCurKDI/ZT441RsnEi9DGr1srX2Ifi6zW2vP62PbK8DpQzmXiIgMI2vDc0EONSx2Pm/3AQO/YfWY3J4+wyLJmUcJkxmxz91px5yPMuT3s79xDQvnXqrJzkUSRLfOiYiMFZ2XqPsc03i0sNij/AAvUQ+IwxUOgEMJi933q+dRZMJQ+BQRGWl9XKI2LXVMqfs35q1q8DcfPUh2hslhv0TdV0/jQMJit7DpStFE5SIyKAqfIiL9GcFL1C5gGQxtseHoJeohhMXO/UnpWjJRRBJC4VNExqdRfonaJnmpafaTM3k6jtQsXaIWkQlD4VNERpfoJeqBTrvTz5yOo/wSdcDvZ92aNaxYsQKHbnwRkQlE4VNEhkfnJeqhhsXuz4fzLmpX6tDDoi5Ri4gMO4VPEcHYILTWQVPLIMNijzGQIf/wVarzEvVALkOnZPYTLnWJWkRktFH4FBnLAh3hS9TRMYzdH40D+N2Hq93Hu/0t4eUihkXnJeqjXabuKyz2mKZHd1GLiIxLCp8iiRBo76cnsXc4jN3WY1+g7birEhPvXKlDD4ud+3WJWkREjkLhU2Sguo9pHHRw7LE92DG8dYuZeqd7r2NGP9u90dDod6Sw9l+vctG7rsKdkja89RIREelB4VPGP2vB39J/QOxrTsaYR7dL2sM57Q6Eewl7hcNuPYrHCI7hm2G84DyO/5X9fvyureBMGr73JSIi0g+FTxm9rO025c5Rehb7HO/Yo7wNDmPFTD/B8Gg9jj2ep3RennYOY71ERERGP4VPGX59zdPYX69it+DobGvk/JpDuPbcBe2RpQiHc8od4+g7CA7mMrWm3RERETkuCp/SJRQc2HjFfsc7Rh4dviGd3gFkAPS8h8Y4e0+fM5TexqQ03T0tIiKSYAqf40EwMPAbXXr2QnYPj/7m4a1Xn/M09h8cAy4Pr27YymlnX4grLadrvztVoVFERGScUPhMpEDH4O6Q7i84DusygoAzucf4xGPd/NJPj6MreVCh0fr9VO8BO+Vk0HKDIiIi45LC51AE2rstHzjQib1HZo7GGJ1zNA45OGZAcno4NIqIiIiMAIXPnhorcLz4HU4p3Y7zd78OX4oe8Tka0/oIjQO5+aXHfi0jKCIiIqOcwmdP/hacbz5CMUDdMcomHWWqnZQBBsek9OObo1FERERkDFHq6Sl9EsGzv8jWvWXMP2k5Lk9W38FR0+2IiIiIDJrCZ0/JXkLnfJG9TWs44aQVuvFFREREZBip605ERERE4kbhU0RERETiRpfde+g4eJA9l15GCbDnq3eHx3U6HBhjwj8jz8MPgzH9/O5wRsqb8LKORysTee0+y/fxOn3+7ozUzXQe5xhEmR7lnccqYzBO5/CUcTjA4cQ4DIFQiOSyMtq2biWYlBRuAxNbpud/j5gyTmdsW/b3u8bqioiIJIzCZ0+hEASDGMAGg9HNw7jCuBzFdKDsRw+N/IkGElAHXKb7l4o+yjudR/+iYhwDK+OIfIE4RpmuLw99feEJ7+teJmRDZG3fTn1DIy63Kxz0neHAH/0SEX2/A9tnHJ3vu+c+R9cXhWj7HWVft9/1pUFEZHxQ+OzBXVTEjL8/z9+fe47zzz8ft8OBtRZCoXAY7fw9FIr9PWTB9lGmc3uv30PHLhMM9XNsX2XC5+y3fCjYo0zv8jYUjJTtq0y334ORuttQ3+VDXe+xe5le5Xu8jg0FaW1uITU5GaztKh8MRv8bhMv38Xu3LwoD0nl8t00T+QvGJKD6qb8kuhrHdrRgGrPP0e2qQec+Ew7Nfe2LfEmICdYOZ8xxvff1CNam23kHsC9oIWvHdurr6nEmufs9LnpeZ9cXns6rCn1+Eei+z+kMXxnovq/b+z7avq4vTpEvUSIiw0ThswfjcuHKzyeQmYm7sBC37naPG7/fz5o1a1ixYsWQ2r3fgBoJ4r1CbOf2nr9HAv1xl+njC0af5UPBfr4wdA/1/ZeJCfjdvkD0F/x7lgn5AxwqL2dyQQEOiHypCYaPCYbC5w52ey/d9kVfKxgc4L5uzzu3DebLQzA4rq5ITAKq//LXRFfj2DrDec+w3vOLQJ/7HF1Bvt99jugVgKN/gei5r/uXhG49/z2/JHQL5EEga+dOGlpacKWm4khOxiQnY9xJmOQkHElJ4eedP91JOJKTup47nYn+ryEy5il8yrgR7alxOlE/zcD5/X7eXLOGU4YY+odLv8G0e+jtDP3d93UPyJ1XH4ZrX/R5sOsqQ3Rf5EtBZF9MWB9AIA8GAhwqK2NKYSHG2t5hvfOLQl9BvnubHHXfUdoy0t6EQgP5jwOBQDTsj4fQXzXUnn6nE5OcjMPtjg2pSX0E1yQ3jqTYMtGAm9T13CQlhUNwt+2OZIVgGb8UPkVkVDDGgMs1Yb44jKrQ3yu0dgvWx7uvr7BubVegj9nXT+gexn2hQIBDBw5QmJMTDtTt7diODkIdHdiOjuhz294e3tbeHhvQg0FsSwuDHOgz/I4VgjufD0cI7v5aCsEyDBQ+RUQmsIl2xcDv9/PGmjWcPIjQbwOBcEBtb8d2+LEd7TFBNdQeCa4d7d3KdWC7bY8e27mvozPcxobePkOwP3xczPCUMRaCcbmYXFtL5Usv4UxJPb4QHN3u7irnUpwZS/RfS0RE5CiMy4VxuXB4PAmtx7CE4F6BN34h2Av4Nm4cmcYZaE9wn4G3e5BVCI4HtZKIiMgYMNpCcFfgPXYIDrS2suWtt5hfUoIjEOgn8LaPj57gPsf/DjYEx/YGj7cQPLprJyIiIqNKZwjG42GgIz79fj/1Hg/ZwzjGuVcI9kdC6nH1BEdCcOcQiaGE4NZWgq2tw/IehywSgk1SEnknnggrViS2Pj0ofIqIiMiYM5QQPBL6DcExN64dZwju/lqDCMG2tRUTCCSucfqh8CkiIiIyRKM5BPubm9nz2msJrFXfFD5FRERExri+QrDx+wlu357QevVFiyWLiIiISNwofIqIiIhI3Ch8ioiIiEjcKHyKiIiISNwofIqIiIhI3Ch8ioiIiEjcKHyKiIiISNwofIqIiIhI3Ch8ioiIiEjcKHyKiIiISNwofIqIiIhI3Ch8ioiIiEjcKHyKiIiISNy4El2BwWhsbIzLefx+Py0tLTQ2NuJ2u+NyTlG7J4raPTHU7omhdk8MtXtixLvdB5rTjLV2hKty/IwxRUBZoushIiIiIsc01Vpb3t/OsRI+DTAF8MXplF7CYXdqHM8pavdEUbsnhto9MdTuiaF2T4xEtLsXOGSPEjDHxGX3yBvoN0EPt3DWBcBnrY3PtX5RuyeI2j0x1O6JoXZPDLV7YiSo3Y95Ht1wJCIiIiJxo/ApIiIiInGj8Nm3duDrkZ8SP2r3xFC7J4baPTHU7omhdk+MUdnuY+KGIxEREREZH9TzKSIiIiJxo/ApIiIiInGj8CkiIiIicaPwKSIiIiJxM2HDpzHmNmNMqTGmzRjzqjFm+THKv9cYsz1SfpMxZkW86jqeDKbdjTE3GWNsj0dbPOs71hljzjHG/MUYcyjSflcO4JjzjDFvGmPajTG7jTE3jXxNx5fBtnukzXt+1q0xpjBOVR4XjDF3GWPWG2N8xpgjxpgnjTHzBnCc/r4fh6G0u/6+Hz9jzCeMMRuNMY2RxyvGmMuOccyo+KxPyPBpjHk/8ADh6QdOAd4G/maMmdRP+TOB3wA/B04GngSeNMYsikuFx4nBtntEIzC522P6SNdznEkj3M63DaSwMWYmsBp4ATgJ+AHwM2PMJSNUv/FqUO3ezTxiP+9Hhrle4925wI+B04GLADfwrDEmrb8D9Pd9WAy63SP09/34lAFfAk4FlgJ/B/5sjFnYV+HR9FmfkFMtGWNeBdZba2+PPHcAB4EfWWvv66P874A0a+3l3bb9G9hgrb01TtUe84bQ7jcBP7DWZsWznuOVMcYCV1lrnzxKme8A77LWLuq27bdAlrX20pGv5fgzwHY/j3Dgz7bW1selYhOAMSafcIA/11r7z37K6O/7MBtgu9+E/r4PO2NMLfAFa+3P+9g3aj7rE67n0xiTRPhbwnOd26y1ocjzM/o57Izu5SP+dpTy0sMQ2x0g3Riz3xhz0BjT7zc6GTb6rCfWBmNMhTFmrTHmHYmuzDiQGflZe5Qy+swPv4G0O+jv+7AxxjiNMdcSvurySj/FRs1nfcKFTyAPcAKVPbZXAv2NryocZHnpbSjtvgO4GbgC+ADhz+vLxpipI1VJ6feznmGMSU1AfSaKCuBW4JrI4yDwojHmlITWagyLXFn5AbDOWrv5KEX1930YDaLd9fd9GBhjFhtjmgivYPQTwldZtvZTfNR81l3xPqHIQFlrX6HbNzhjzMvANuDjwFcTVS+R4Wat3UH4H+NOLxtjZgP/AXwwMbUa834MLALOSnRFJpgBtbv+vg+bHYTH52cC7wF+aYw59ygBdFSYiD2f1UAQKOixvQA43M8xhwdZXnobSrvHsNb6gbeAOcNbNemmv896o7W2NQH1mcheQ5/1ITHGPARcDpxvrS07RnH9fR8mg2z3GPr7PjTW2g5r7W5r7RvW2rsI3+j4mX6Kj5rP+oQLn9baDuAN4MLObZHLBBfS/ziJV7qXj7joKOWlhyG2ewxjjBNYTPgSpYwMfdZHj5PQZ31QTNhDwFXABdbafQM4TJ/54zTEdu/5Gvr7PjwcQHI/+0bNZ32iXnZ/gHDX9OuEexfuIDxI9xEAY8xjQHnkWwTAg8A/jDGfIzwNzbWEpzX4WJzrPdYNqt2NMXcD/wZ2A1nAFwhPxfGzeFd8rDLGpBPbkzDTGHMSUGutPWCMuRcostZ+KLL/J8DtxpjvAr8ALgDeB7wrjtUe8wbb7saYO4B9wBYgBbiFcNtfHM96jwM/Bq4nPI7QZ7rmSW3o7LnX3/cRMeh219/34xf5O/I0cADwEv5vcB5wSWT/6P2sW2sn5AO4HdhPeJDuq8Bp3fa9CDzao/x7CY+taAc2AysS/R7G4mMw7Q58v1vZw4T/Zzk50e9hLD0I/yGyfTwejex/FHixj2PeirT7HuCmRL+PsfYYbLsDXyT8j3ArUEN42qXzE/0+xtqjnza33T/D+vs+Otpdf9+Hpd1/DpRG2vAI4TvZL+qvzSPbRsVnfULO8ykiIiIiiTHhxnyKiIiISOIofIqIiIhI3Ch8ioiIiEjcKHyKiIiISNwofIqIiIhI3Ch8ioiIiEjcKHyKiIiISNwofIqIiIhI3Ch8ioiIiEjcKHyKiIiISNwofIqIiIhI3Ch8ioiIiEjc/H91qcYEHiRF9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "def plot(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a90581b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 684s 6s/step - loss: 1.4920 - accuracy: 0.5253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4919867515563965, 0.5252941250801086]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8aadb2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[  72   22    3   32  298   14   75]\n",
      " [   9   18    2    2   49    9   10]\n",
      " [   5    0    5    3   34    5    8]\n",
      " [  11    3    0   94  350    8   29]\n",
      " [  37   12    1   54 1376   23  112]\n",
      " [   8   10    1   11  176   42   15]\n",
      " [   6    3    2   21  133    8  179]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05be7fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 176    0    0   26  222   48   44]\n",
      " [  42    0    0    2   31    8   16]\n",
      " [  22    0    0    2   20   13    3]\n",
      " [  45    0    0   86  288   44   32]\n",
      " [ 161    0    0  127 1009  177  141]\n",
      " [  54    0    0   18   82   90   19]\n",
      " [  56    0    0   18  116   21  141]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a751989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score(macro) 0.3141580324153164\n",
      "F1 score(micro) 0.5252941176470588\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score(macro)\",f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1 score(micro)\",f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e832efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5abcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6458f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"embed/glove/\" + dataset_path + \"/tokenizer.pkl\")\n",
    "\n",
    "## tokenize all sentences ##\n",
    "if not checkFile:\n",
    "    all_text = list(X_train[\"Utterance\"])\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "    pickle.dump(tokenizer, open('embed/glove/' + dataset_path + '/tokenizer.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('embed/glove/' + dataset_path + '/tokenizer.pkl', 'rb')\n",
    "    tokenizer = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1caa67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the sentences into sequences ##\n",
    "train_sequence = tokenizer.texts_to_sequences(list(X_train['Utterance']))\n",
    "dev_sequence = tokenizer.texts_to_sequences(list(X_dev['Utterance']))\n",
    "test_sequence = tokenizer.texts_to_sequences(list(X_test['Utterance']))\n",
    "\n",
    "X_train['sentence_length'] = [len(item) for item in train_sequence]\n",
    "X_dev['sentence_length'] = [len(item) for item in dev_sequence]\n",
    "X_test['sentence_length'] = [len(item) for item in test_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae3a80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_tokens = 250\n",
    "\n",
    "train_sequence = pad_sequences(train_sequence, maxlen=max_num_tokens, padding='post')\n",
    "dev_sequence = pad_sequences(dev_sequence, maxlen=max_num_tokens, padding='post')\n",
    "test_sequence = pad_sequences(test_sequence, maxlen=max_num_tokens, padding='post')\n",
    "\n",
    "X_train['sequence'] = list(train_sequence)\n",
    "X_dev['sequence'] = list(dev_sequence)\n",
    "X_test['sequence'] = list(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bd8817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also I was the point person on my companys tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>[455, 1, 32, 3, 940, 443, 28, 18, 3621, 3622, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You mustve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[2, 1120, 104, 45, 706, 871, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[9, 1, 76, 9, 1, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So lets talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>[17, 228, 193, 5, 96, 487, 51, 45, 1971, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[18, 1971, 29, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance          Speaker  \\\n",
       "0  also I was the point person on my companys tr...         Chandler   \n",
       "1                   You mustve had your hands full.  The Interviewer   \n",
       "2                            That I did. That I did.         Chandler   \n",
       "3      So lets talk a little bit about your duties.  The Interviewer   \n",
       "4                             My duties?  All right.         Chandler   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  sentence_length  \\\n",
       "0   neutral   neutral            0             0               18   \n",
       "1   neutral   neutral            0             1                6   \n",
       "2   neutral   neutral            0             2                6   \n",
       "3   neutral   neutral            0             3                9   \n",
       "4  surprise  positive            0             4                4   \n",
       "\n",
       "                                            sequence  \n",
       "0  [455, 1, 32, 3, 940, 443, 28, 18, 3621, 3622, ...  \n",
       "1  [2, 1120, 104, 45, 706, 871, 0, 0, 0, 0, 0, 0,...  \n",
       "2  [9, 1, 76, 9, 1, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "3  [17, 228, 193, 5, 96, 487, 51, 45, 1971, 0, 0,...  \n",
       "4  [18, 1971, 29, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d04fb6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also I was the point person on my companys transition from the KL-5 to GR-6 system.\n"
     ]
    }
   ],
   "source": [
    "print(X_train['Utterance'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ea2580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe...\n",
      "Completed loading pretrained GloVe model.\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/glove/\" + dataset_path + \"/glv_embedding_matrix.pkl\")\n",
    "\n",
    "if True or not checkFile:\n",
    "    glv_vector = load_pretrained_glove()\n",
    "    word_vector_length = len(glv_vector['the'])\n",
    "    word_index = tokenizer.word_index\n",
    "    inv_word_index = {v: k for k, v in word_index.items()}\n",
    "    num_unique_words = len(word_index)\n",
    "    glv_embedding_matrix = np.zeros((num_unique_words + 1, word_vector_length))\n",
    "    pickle.dump(glv_embedding_matrix, open('embed/glove/' + dataset_path + '/glv_embedding_matrix.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('embed/glove/' + dataset_path + '/glv_embedding_matrix.pkl', 'rb')\n",
    "    glv_embedding_matrix = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac41b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Completed preprocessing.\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix')\n",
    "\n",
    "if True or not checkFile:\n",
    "    for j in range(1, num_unique_words + 1):\n",
    "        try:\n",
    "            glv_embedding_matrix[j] = glv_vector[inv_word_index[j]]\n",
    "        except KeyError:\n",
    "            glv_embedding_matrix[j] = np.random.randn(word_vector_length) / 200\n",
    "\n",
    "    np.ndarray.dump(glv_embedding_matrix, open('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix', 'wb'))\n",
    "    vocab_size = word_vector_length\n",
    "    print('Done. Completed preprocessing.')\n",
    "    \n",
    "else:\n",
    "#     file1 = open('embed/glove/pretrained_glv_embedding_matrix', 'rb')\n",
    "#     glv_embedding_matrix = pickle.load(file1)\n",
    "    glv_embedding_matrix = np.load(open('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix', 'rb'), allow_pickle=True)\n",
    "    vocab_size, embedding_dim = glv_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4bde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change D_m into\n",
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 100\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31b6cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_feat_extractor = CNNFeatureExtractor(vocab_size=vocab_size, embedding_dim=300, output_size=100, filters=50, kernel_sizes=(3, 4, 5), dropout=0.5)\n",
    "cnn_feat_extractor.init_pretrained_embeddings_from_numpy(glv_embedding_matrix)\n",
    "lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c6c03",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07e5e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4459dbb5",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9644\\2247667134.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "\n",
    "if True or not checkFile:\n",
    "    for range_pair in rangesTrain:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_train['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTrain.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTrain, rangesTrain], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\")\n",
    "    encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf00b0",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2df3e0b0",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9644\\970745468.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "\n",
    "if True or not checkFile:\n",
    "    for range_pair in rangesTest:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_test['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTest.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTest, rangesTest], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_test.pkl', \"rb\")\n",
    "    encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e2384",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d4eae62",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\")\n",
    "encodedSpeakersDev = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesDev:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_dev['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersDev.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersDev, rangesDev], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_dev.pkl', \"rb\")\n",
    "    encodedSpeakersDev, rangesDev = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1827fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 200\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "n_speakers=2\n",
    "max_seq_len=110\n",
    "window_past=0\n",
    "window_future=5\n",
    "# vocab_size=vocab_size\n",
    "n_classes=7\n",
    "listener_state=False\n",
    "context_attention='general'\n",
    "dropout=0.5\n",
    "nodal_attention=False\n",
    "no_cuda=True\n",
    "n_relations = 2 * n_speakers ** 2\n",
    "att_model = MaskedEdgeAttention(D_e, max_seq_len, no_cuda)\n",
    "nodal_attention=True\n",
    "edge_type_mapping = {}\n",
    "for j in range(n_speakers):\n",
    "    for k in range(n_speakers):\n",
    "        edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "        edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f487a",
   "metadata": {},
   "source": [
    "<h4> Getting data required for graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8920b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OverrideFileChecks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcbad5c0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, X_set, rangesSet, encodedSpeakersSet):\n",
    "        self.X_set = X_set\n",
    "        self.rangesSet = rangesSet\n",
    "        self.encodedSpeakersSet = encodedSpeakersSet\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rangesSet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        startIdx, endIdx = self.rangesSet[idx]\n",
    "        sequence = self.X_set[\"sequence\"][startIdx:endIdx+1].tolist()\n",
    "        qmask = self.encodedSpeakersSet[startIdx: endIdx+1]\n",
    "        return torch.FloatTensor(sequence), qmask\n",
    "\n",
    "# Define the ContextEncoding function\n",
    "def ContextEncoding(file_path, dataset):\n",
    "    all_emotions, all_umask, all_seq_lengths = [], [], []\n",
    "    all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths = [], [], [], [], []\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for textf, qmask in tqdm(dataloader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        textf = textf.squeeze(0)  # Remove batch dimension (1, utterance_size, embedding_size) -> (utterance_size, embedding_size)\n",
    "        umask = torch.FloatTensor([[1] * textf.size(0)])  # Adjust to (1, utterance_size)\n",
    "        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "        # print(umask.shape, lengths)\n",
    "        U = cnn_feat_extractor(textf, umask)\n",
    "        emotions, hidden = lstm(U)\n",
    "        all_emotions.append(emotions)\n",
    "        \n",
    "        features, edge_index, \\\n",
    "        edge_norm, edge_type, \\\n",
    "        edge_index_lengths = batch_graphify(emotions, \n",
    "                                            qmask,\n",
    "                                            lengths,\n",
    "                                            window_past,\n",
    "                                            window_future,\n",
    "                                            edge_type_mapping,\n",
    "                                            att_model, \n",
    "                                            no_cuda)\n",
    "        all_umask.append(umask)\n",
    "        all_seq_lengths.append(lengths)\n",
    "        all_features.append(features)\n",
    "        all_edge_index.append(edge_index)\n",
    "        all_edge_norm.append(edge_norm)\n",
    "        all_edge_type.append(edge_type)\n",
    "        all_edge_index_lengths.append(edge_index_lengths)\n",
    "\n",
    "#     all_emotions = torch.cat(all_emotions, dim=0)  # (total_num_utterances, lstm_hidden_size)\n",
    "    \n",
    "    with open(file_path[0], 'wb') as file:\n",
    "        pickle.dump(all_emotions, file)\n",
    "        \n",
    "    with open(file_path[1], 'wb') as file:\n",
    "        pickle.dump([   all_umask, \\\n",
    "                        all_seq_lengths,\n",
    "                        all_features, \\\n",
    "                        all_edge_index, \\\n",
    "                        all_edge_norm, \\\n",
    "                        all_edge_type, \\\n",
    "                        all_edge_index_lengths], file)\n",
    "    \n",
    "    return all_emotions, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths\n",
    "\n",
    "# File paths\n",
    "file_path1 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_train.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl']\n",
    "file_path2 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_test.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl']\n",
    "file_path3 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_dev.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl']\n",
    "\n",
    "# Check if files exist\n",
    "checkFile1 = os.path.isfile(file_path1[0])\n",
    "checkFile2 = os.path.isfile(file_path2[0])\n",
    "checkFile3 = os.path.isfile(file_path3[0])\n",
    "\n",
    "if not checkFile1:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "\n",
    "    trainDataset = ContextDataset(X_train, rangesTrain, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trainContext, \\\n",
    "     all_edge_index, \\\n",
    "     all_edge_norm, \\\n",
    "     all_edge_type, \\\n",
    "     all_edge_index_lengths = ContextEncoding(file_path1, trainDataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train text\")\n",
    "\n",
    "if not checkFile2:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTest for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    testDataset = ContextDataset(X_test, rangesTest, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    testContext, _, _, _, _ = ContextEncoding(file_path2, testDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "    \n",
    "if not checkFile3:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersDev for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    devDataset = ContextDataset(X_dev, rangesDev, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    devContext, _, _, _, _ = ContextEncoding(file_path3, devDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3:\n",
    "    with open(file_path1[0], 'rb') as file1:\n",
    "        trainContext = pickle.load(file1)\n",
    "    with open(file_path2[0], 'rb') as file2:\n",
    "        testContext = pickle.load(file2)\n",
    "    with open(file_path3[0], 'rb') as file3:\n",
    "        devContext = pickle.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871aaea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 1, 200])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainContext[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c07b8f",
   "metadata": {},
   "source": [
    "<h4> Visualize utterance embeddnig (u') with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe10df08",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     # Assuming trainContext, X_train, and labelDecoder are already defined\n",
    "#     num_instance = len(X_train[\"Emotion\"])\n",
    "\n",
    "#     # Calculate the counts for each unique label\n",
    "#     unique_labels, label_counts = np.unique(list(X_train[\"Emotion\"][:num_instance]), return_counts=True)\n",
    "\n",
    "#     # Print the counts for each unique label\n",
    "#     for label, count in zip(unique_labels, label_counts):\n",
    "#         print(f\"{labelDecoder[label]}: {count} occurrences\")\n",
    "\n",
    "#     encodedFeaturesFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "#     trainContext = trainContext.squeeze(1)\n",
    "\n",
    "#     # Convert the tensor to a numpy array for use with sklearn\n",
    "#     trainContext_np = trainContext.detach().numpy()\n",
    "\n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     trainContext_pca = pca.fit_transform(trainContext_np)\n",
    "\n",
    "#     # Perform t-SNE\n",
    "#     tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "#     trainContext_tsne = tsne.fit_transform(trainContext_np)\n",
    "\n",
    "#     # Plot PCA results with color-coded labels  \n",
    "#     plt.figure(figsize=(14, 7))\n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(trainContext_pca[indices, 0], trainContext_pca[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('PCA of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     # Plot t-SNE results with color-coded labels\n",
    "#     plt.subplot(1, 2, 2, position=[0.65, 0.1, 0.35, 0.8])\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(trainContext_tsne[indices, 0], trainContext_tsne[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('t-SNE of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('t-SNE Component 1')\n",
    "#     plt.ylabel('t-SNE Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78804eeb",
   "metadata": {},
   "source": [
    "<h4>Visualize node features (pre-h') with T-SNE and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c3f3a2d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     # Assuming trainContext, X_train, and labelDecoder are already defined\n",
    "#     num_instance = len(X_train[\"Emotion\"])\n",
    "\n",
    "#     # Calculate the counts for each unique label\n",
    "#     unique_labels, label_counts = np.unique(list(X_train[\"Emotion\"][:num_instance]), return_counts=True)\n",
    "\n",
    "#     # Print the counts for each unique label\n",
    "#     for label, count in zip(unique_labels, label_counts):\n",
    "#         print(f\"{labelDecoder[label]}: {count} occurrences\")\n",
    "\n",
    "#     flattened_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "#     # Convert the tensor to a numpy array for use with sklearn\n",
    "#     flattened_features_np = flattened_features.detach().numpy()\n",
    "\n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     flattened_features_np_pca = pca.fit_transform(flattened_features_np)\n",
    "\n",
    "#     # Perform t-SNE\n",
    "#     tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "#     flattened_features_tsne = tsne.fit_transform(flattened_features_np)\n",
    "\n",
    "#     # Plot PCA results with color-coded labels\n",
    "#     plt.figure(figsize=(14, 7))\n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(flattened_features_np_pca[indices, 0], flattened_features_np_pca[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('PCA of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     # Plot t-SNE results with color-coded labels\n",
    "#     plt.subplot(1, 2, 2, position=[0.65, 0.1, 0.35, 0.8])\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(flattened_features_tsne[indices, 0], flattened_features_tsne[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('t-SNE of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('t-SNE Component 1')\n",
    "#     plt.ylabel('t-SNE Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61099f3e",
   "metadata": {},
   "source": [
    "#### Objective measure on observing howuseful the data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43c18404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be546014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     neutral\n",
       "1     neutral\n",
       "2     neutral\n",
       "3     neutral\n",
       "4    surprise\n",
       "5     neutral\n",
       "6     neutral\n",
       "7     neutral\n",
       "8     neutral\n",
       "9     neutral\n",
       "Name: Emotion, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4a564d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n",
    "\n",
    "flat_trainContext = flatten_extend(trainContext)\n",
    "stacked_trainContext = torch.stack(flat_trainContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38c4adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNp = stacked_trainContext.squeeze(1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acade541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.25304386019706726\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=len(set(y_train[\"Emotion\"])))\n",
    "clusters = kmeans.fit_predict(trainNp)\n",
    "\n",
    "score = silhouette_score(trainNp, clusters)\n",
    "print(f\"Silhouette Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ef4b2",
   "metadata": {},
   "source": [
    "- Silhouette Score > 0.5: Indicates a good clustering with well-separated clusters.\n",
    "- Silhouette Score between 0 and 0.5: Indicates overlapping clusters to some degree.\n",
    "- Silhouette Score < 0: Indicates that samples might have been assigned to the wrong clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b66d8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbcf9d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabasz Index: 3835.5476772002985\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=len(set(y_train[\"Emotion\"])))\n",
    "clusters = kmeans.fit_predict(trainNp)\n",
    "\n",
    "score = calinski_harabasz_score(trainNp, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b9966",
   "metadata": {},
   "source": [
    "Higher values: Indicate better-defined clusters. There is no strict threshold for \"good\" scores, as it depends on the specific dataset and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56c959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9885870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473: ConvergenceWarning: Number of distinct clusters (12832) found smaller than n_clusters (12840). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin Index: 2.2036955879278816e-07\n"
     ]
    }
   ],
   "source": [
    "# Assuming `embeddings` is your BERT/CNN output and `labels` are your true labels\n",
    "kmeans = KMeans(n_clusters=len(y_train[\"Emotion\"]))\n",
    "clusters = kmeans.fit_predict(trainNp)\n",
    "\n",
    "score = davies_bouldin_score(trainNp, clusters)\n",
    "print(f\"Davies-Bouldin Index: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb6c34",
   "metadata": {},
   "source": [
    "- Lower values: Indicate better clustering with well-separated clusters.\n",
    "- Higher values: Indicate poor clustering with overlapping clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "221.991px",
    "width": "339.977px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
