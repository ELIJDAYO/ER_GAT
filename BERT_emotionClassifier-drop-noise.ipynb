{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from graph_context_dataset import FeatureEngineeredDataset\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import random\n",
    "from model import FCClassifier, MyNetwork, DATASET_PATH, MatchingAttention\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b22dd",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "\n",
    " - dataset_original\n",
    " - dataset_drop_noise\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1470c3",
   "metadata": {},
   "source": [
    "<h3> Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68406d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a4fb045",
   "metadata": {
    "code_folding": [
     0,
     9,
     18,
     21,
     24,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# class FCLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(FCLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# class ActivationLayer(nn.Module):\n",
    "#     def __init__(self, activation_fn):\n",
    "#         super(ActivationLayer, self).__init__()\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.activation_fn(x)\n",
    "#         return x\n",
    "\n",
    "# def tanh(x):\n",
    "#     return torch.tanh(x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return torch.sigmoid(x)\n",
    "# # loss function and its derivative\n",
    "# def mse(y_true, y_pred):\n",
    "#     return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "# def mse_prime(y_true, y_pred):\n",
    "#     return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "246bf76e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oversample_data(X_train, Y_train, num_classes):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            # Calculate the number of instances to oversample for this class\n",
    "            num_to_oversample = max_class_count - len(indices)\n",
    "            # Randomly select instances with replacement to oversample\n",
    "            oversampled_indices = np.random.choice(indices, size=num_to_oversample, replace=True)\n",
    "            # Append the oversampled instances to the original data\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return torch.tensor(X_train), torch.tensor(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8349606b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    if not tensor_list:\n",
    "        raise ValueError(\"The tensor list is empty\")\n",
    "\n",
    "    feature_dim = tensor_list[0].shape[1]\n",
    "    for tensor in tensor_list:\n",
    "        if tensor.shape[1] != feature_dim:\n",
    "            raise ValueError(\"All tensors must have the same feature dimension\")\n",
    "    \n",
    "    concatenated_tensor = torch.cat(tensor_list, dim=0)\n",
    "    \n",
    "    return concatenated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503aae",
   "metadata": {},
   "source": [
    "<h4> Import labels and label decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/labels_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_dev.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_val = pickle.load(file)\n",
    "y_val = torch.tensor(y_val)\n",
    "    \n",
    "file_path = 'data/dump/' + dataset_path + '/label_decoder.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b721a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f3e716",
   "metadata": {},
   "source": [
    "<h4> Import the BERT base-node outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c53ba",
   "metadata": {},
   "source": [
    "first we disregard the u' and directly train the h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "396cc340",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_EGAT_train.pkl\",\n",
    "]\n",
    "\n",
    "test_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_EGAT_test.pkl\",\n",
    "]\n",
    "\n",
    "val_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_EGAT_dev.pkl\",\n",
    "]\n",
    "\n",
    "dictKey = {\n",
    "    0 : 'bert',\n",
    "    1 : 'bert-select-few',\n",
    "    2 : 'bert-select-mod',\n",
    "    3 : 'bert-select-more',\n",
    "    4 : 'dgcn',\n",
    "    5 : 'dgcn-select',\n",
    "    6 : 'gatv1',\n",
    "    7 : 'gatv1-select',\n",
    "    8 : 'gatv1-edge',\n",
    "    9 : 'gatv1-edge-select',\n",
    "    10 : 'gatv2-edge',\n",
    "    11 : 'gatv2-edge-select',\n",
    "    12 : 'rgat',\n",
    "    13 : 'rgat-select',\n",
    "    14 : 'egat',\n",
    "    15 : 'egat-select',\n",
    "    16 : 'bert-select-mod-dgcn',\n",
    "    17 : 'bert-select-mod-gatv1',\n",
    "    18 : 'bert-select-mod-gatv1-edge',\n",
    "    19 : 'bert-select-mod-gatv2-edge',\n",
    "    20 : 'bert-select-mod-rgat',\n",
    "    21 : 'bert-select-mod-egat',\n",
    "}\n",
    "selected_combination = [0, 14]\n",
    "# selected_combination = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff1721",
   "metadata": {},
   "source": [
    "<h4> Getting BERT and GAT outputs for all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2c366b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFeaturesList[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e3426c4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>  instance of list:  1588\n",
      "<class 'list'>  instance of list:  1588\n",
      "<class 'list'>  instance of list:  1588\n",
      "<class 'list'>  instance of list:  1588\n",
      "<class 'list'>  instance of list:  1588\n",
      "<class 'list'>  instance of list:  1588\n",
      "<class 'list'>  instance of list:  1588\n",
      "<class 'list'>  instance of list:  435\n",
      "<class 'list'>  instance of list:  435\n",
      "<class 'list'>  instance of list:  435\n",
      "<class 'list'>  instance of list:  435\n",
      "<class 'list'>  instance of list:  435\n",
      "<class 'list'>  instance of list:  435\n",
      "<class 'list'>  instance of list:  435\n",
      "<class 'list'>  instance of list:  203\n",
      "<class 'list'>  instance of list:  203\n",
      "<class 'list'>  instance of list:  203\n",
      "<class 'list'>  instance of list:  203\n",
      "<class 'list'>  instance of list:  203\n",
      "<class 'list'>  instance of list:  203\n",
      "<class 'list'>  instance of list:  203\n"
     ]
    }
   ],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    return torch.cat(tensor_list, dim=0)\n",
    "\n",
    "def import_h_prime(file_paths):\n",
    "    featuresList = []\n",
    "    attList = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "#             print(\"Check... \", len(data))\n",
    "            if isinstance(data, list):\n",
    "                print(type(data), \" instance of list: \", len(data))\n",
    "                featuresList.append(concatenate_tensors(data))\n",
    "            else:\n",
    "                print(type(data), \" instance of tensor, \", data.shape)\n",
    "                featuresList.append(data)\n",
    "                \n",
    "    return featuresList\n",
    "    \n",
    "trainFeaturesList = import_h_prime(train_file_paths)\n",
    "\n",
    "testFeaturesList = import_h_prime(test_file_paths)\n",
    "\n",
    "valFeaturesList = import_h_prime(val_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c84fe092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6955, 128])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeaturesList[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33882e42",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getNodalAttn():\n",
    "    nodalAttList = []\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    return nodalAttList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d84c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodalAttn = getNodalAttn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e174164",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "479a3b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a070ce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a43fa315",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "<h3> Feature Selection and creating data combination for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Define select feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6501b577",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_norm_features(encoded_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "    return torch.tensor(features_scaled)\n",
    "\n",
    "def get_selected_features(encoded_features, labels, top_n):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "\n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        # Create a binary mask indicating instances belonging to the current class\n",
    "        mask = (labels == label)\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=top_n) \n",
    "        selector.fit(features_scaled, mask)  \n",
    "\n",
    "        top_features_indices = np.argsort(selector.scores_)[-top_n:]\n",
    "        scores = selector.scores_[top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "\n",
    "    selected_features = encoded_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3346eac",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Function to train the autoencoder\n",
    "def train_autoencoder(encoded_features, hidden_dim=100, num_epochs=20, lr=0.001):\n",
    "    input_dim = encoded_features.shape[1]\n",
    "    autoencoder = Autoencoder(input_dim, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    X_tensor = torch.tensor(encoded_features, dtype=torch.float32)\n",
    "    train_loader = torch.utils.data.DataLoader(X_tensor, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Autoencoder\", unit=\"epoch\"):\n",
    "        total_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs = data\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = autoencoder(inputs)\n",
    "            loss = criterion(decoded, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_features, _ = autoencoder(torch.tensor(encoded_features, dtype=torch.float32))\n",
    "    \n",
    "    return encoded_features, autoencoder\n",
    "\n",
    "def get_selected_features_autoencoder(autoencoder, encoded_features, labels, top_n=100):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "\n",
    "    reduced_features = autoencoder.encoder(torch.tensor(encoded_features, dtype=torch.float32)).detach().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(reduced_features)\n",
    "    \n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        mask = (labels == label)\n",
    "        selector = chi2(features_scaled, mask)\n",
    "        top_features_indices = np.argsort(selector[0])[-top_n:]\n",
    "        scores = selector[0][top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "    selected_features = reduced_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7165221a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6955, 768])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainList[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "46d79a94",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "efccc17d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ca73f7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Now prepare the data that will be ued to train the classifier, there are 20 combinations. And pick top 7 combinations yielding top F1 weighted-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3b7d2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in trainFeaturesList[6][:10]:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "61905fb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_16852\\3968918384.py:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  trainList_tensors = [torch.tensor(item) for item in trainList]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_16852\\3968918384.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  testList_tensors = [torch.tensor(item) for item in testList]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_16852\\3968918384.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valList_tensors = [torch.tensor(item) for item in valList]\n"
     ]
    }
   ],
   "source": [
    "trainList = []\n",
    "testList = []\n",
    "valList = []\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/trainList.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/testList.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/valList.pkl\"\n",
    "\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3: \n",
    "    with open(file_path1, \"rb\") as file:\n",
    "        trainList = pickle.load(file)\n",
    "    with open(file_path2, \"rb\") as file:\n",
    "        testList = pickle.load(file)\n",
    "    with open(file_path3, \"rb\") as file:\n",
    "        valList = pickle.load(file)\n",
    "else:\n",
    "#     trainFeaturesList.append(data)\n",
    "    #1\n",
    "    trainList.append(trainFeaturesList[0])\n",
    "    testList.append(testFeaturesList[0])\n",
    "    valList.append(valFeaturesList[0])\n",
    "    #2\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 16)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #3\n",
    "    selectedTrainFeatures1b, indicesFeatures1b = get_selected_features(trainFeaturesList[0], y_train, 32)\n",
    "    selectedTestFeatures1b = testFeaturesList[0][:, indicesFeatures1b]\n",
    "    selectedValFeatures1b = valFeaturesList[0][:, indicesFeatures1b]\n",
    "    trainList.append(selectedTrainFeatures1b)\n",
    "    testList.append(selectedTestFeatures1b)\n",
    "    valList.append(selectedValFeatures1b)\n",
    "    #4\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 64)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #5\n",
    "    trainList.append(trainFeaturesList[1])\n",
    "    testList.append(testFeaturesList[1])\n",
    "    valList.append(valFeaturesList[1])\n",
    "    #6\n",
    "    selectedTrainFeatures2, indicesFeatures2 = get_selected_features(trainFeaturesList[1], y_train, 12)\n",
    "    selectedTestFeatures2 = testFeaturesList[1][:, indicesFeatures2]\n",
    "    selectedValFeatures2 = valFeaturesList[1][:, indicesFeatures2]\n",
    "    trainList.append(selectedTrainFeatures2)\n",
    "    testList.append(selectedTestFeatures2)\n",
    "    valList.append(selectedValFeatures2)\n",
    "    #7\n",
    "    trainList.append(trainFeaturesList[2])\n",
    "    testList.append(testFeaturesList[2])\n",
    "    valList.append(valFeaturesList[2])\n",
    "    #8\n",
    "    selectedTrainFeatures3, indicesFeatures3 = get_selected_features(trainFeaturesList[2], y_train, 12)\n",
    "    selectedTestFeatures3 = testFeaturesList[2][:, indicesFeatures3]\n",
    "    selectedValFeatures3 = valFeaturesList[2][:, indicesFeatures3]\n",
    "    trainList.append(selectedTrainFeatures3)\n",
    "    testList.append(selectedTestFeatures3)\n",
    "    valList.append(selectedValFeatures3)\n",
    "    #9\n",
    "    trainList.append(trainFeaturesList[3])\n",
    "    testList.append(testFeaturesList[3])\n",
    "    valList.append(valFeaturesList[3])\n",
    "    #10\n",
    "    selectedTrainFeatures4, indicesFeatures4 = get_selected_features(trainFeaturesList[3], y_train, 12)\n",
    "    selectedTestFeatures4 = testFeaturesList[3][:, indicesFeatures4]\n",
    "    selectedValFeatures4 = valFeaturesList[3][:, indicesFeatures4]\n",
    "    trainList.append(selectedTrainFeatures4)\n",
    "    testList.append(selectedTestFeatures4)\n",
    "    valList.append(selectedValFeatures4)\n",
    "    #11\n",
    "    trainList.append(trainFeaturesList[4])\n",
    "    testList.append(testFeaturesList[4])\n",
    "    valList.append(valFeaturesList[4])\n",
    "    #12\n",
    "    selectedTrainFeatures5, indicesFeatures5 = get_selected_features(trainFeaturesList[4], y_train, 12)\n",
    "    selectedTestFeatures5 = testFeaturesList[4][:, indicesFeatures5]\n",
    "    selectedValFeatures5 = valFeaturesList[4][:, indicesFeatures5]\n",
    "    trainList.append(selectedTrainFeatures5)\n",
    "    testList.append(selectedTestFeatures5)\n",
    "    valList.append(selectedValFeatures5)\n",
    "    #13\n",
    "    trainList.append(trainFeaturesList[5])\n",
    "    testList.append(testFeaturesList[5])\n",
    "    valList.append(valFeaturesList[5])\n",
    "    #14\n",
    "    selectedTrainFeatures6, indicesFeatures6 = get_selected_features(trainFeaturesList[5], y_train, 12)\n",
    "    selectedTestFeatures6 = testFeaturesList[5][:, indicesFeatures6]\n",
    "    selectedValFeatures6 = valFeaturesList[5][:, indicesFeatures6]\n",
    "    trainList.append(selectedTrainFeatures6)\n",
    "    testList.append(selectedTestFeatures6)\n",
    "    valList.append(selectedValFeatures6)\n",
    "    #15\n",
    "    trainList.append(trainFeaturesList[6])\n",
    "    testList.append(testFeaturesList[6])\n",
    "    valList.append(valFeaturesList[6])\n",
    "    #16\n",
    "    selectedTrainFeatures7, indicesFeatures7 = get_selected_features(trainFeaturesList[6], y_train, 12)\n",
    "    selectedTestFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    selectedValFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    trainList.append(selectedTrainFeatures7)\n",
    "    testList.append(selectedTestFeatures7)\n",
    "    valList.append(selectedValFeatures7)\n",
    "    selectedNormTrainFeatures1 = get_norm_features(selectedTrainFeatures1b)\n",
    "    selectedNormTestFeatures1 = get_norm_features(selectedTestFeatures1b)\n",
    "    selectedNormValFeatures1 = get_norm_features(selectedValFeatures1b)\n",
    "\n",
    "    #17\n",
    "    trainNormFeatures2 = get_norm_features(trainFeaturesList[1].detach().numpy())\n",
    "    testNormFeatures2 = get_norm_features(testFeaturesList[1].detach().numpy())\n",
    "    valNormFeatures2 = get_norm_features(valFeaturesList[1].detach().numpy())\n",
    "    concatenatedTrainFeatures2 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures2), dim=1)\n",
    "    concatenatedTestFeatures2 = torch.cat((selectedNormTestFeatures1, testNormFeatures2), dim=1)\n",
    "    concatenatedValFeatures2 = torch.cat((selectedNormValFeatures1, valNormFeatures2), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures2)\n",
    "    testList.append(concatenatedTestFeatures2)\n",
    "    valList.append(concatenatedValFeatures2)\n",
    "    #18\n",
    "    trainNormFeatures3 = get_norm_features(trainFeaturesList[2].detach().numpy())\n",
    "    testNormFeatures3 = get_norm_features(testFeaturesList[2].detach().numpy())\n",
    "    valNormFeatures3 = get_norm_features(valFeaturesList[2].detach().numpy())\n",
    "    concatenatedTrainFeatures3 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures3), dim=1)\n",
    "    concatenatedTestFeatures3 = torch.cat((selectedNormTestFeatures1, testNormFeatures3), dim=1)\n",
    "    concatenatedValFeatures3 = torch.cat((selectedNormValFeatures1, valNormFeatures3), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures3)\n",
    "    testList.append(concatenatedTestFeatures3)\n",
    "    valList.append(concatenatedValFeatures3)\n",
    "    #19\n",
    "    trainNormFeatures4 = get_norm_features(trainFeaturesList[3].detach().numpy())\n",
    "    testNormFeatures4 = get_norm_features(testFeaturesList[3].detach().numpy())\n",
    "    valNormFeatures4 = get_norm_features(valFeaturesList[3].detach().numpy())\n",
    "    concatenatedTrainFeatures4 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures4), dim=1)\n",
    "    concatenatedTestFeatures4 = torch.cat((selectedNormTestFeatures1, testNormFeatures4), dim=1)\n",
    "    concatenatedValFeatures4 = torch.cat((selectedNormValFeatures1, valNormFeatures4), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures4)\n",
    "    testList.append(concatenatedTestFeatures4)\n",
    "    valList.append(concatenatedValFeatures4)\n",
    "    #20\n",
    "    trainNormFeatures5 = get_norm_features(trainFeaturesList[4].detach().numpy())\n",
    "    testNormFeatures5 = get_norm_features(testFeaturesList[4].detach().numpy())\n",
    "    valNormFeatures5 = get_norm_features(valFeaturesList[4].detach().numpy())\n",
    "    concatenatedTrainFeatures5 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures5), dim=1)\n",
    "    concatenatedTestFeatures5 = torch.cat((selectedNormTestFeatures1, testNormFeatures5), dim=1)\n",
    "    concatenatedValFeatures5 = torch.cat((selectedNormValFeatures1, valNormFeatures5), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures5)\n",
    "    testList.append(concatenatedTestFeatures5)\n",
    "    valList.append(concatenatedValFeatures5)\n",
    "\n",
    "    #21\n",
    "    trainNormFeatures6 = get_norm_features(trainFeaturesList[5].detach().numpy())\n",
    "    testNormFeatures6 = get_norm_features(testFeaturesList[5].detach().numpy())\n",
    "    valNormFeatures6 = get_norm_features(valFeaturesList[5].detach().numpy())\n",
    "    concatenatedTrainFeatures6 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures6), dim=1)\n",
    "    concatenatedTestFeatures6 = torch.cat((selectedNormTestFeatures1, testNormFeatures6), dim=1)\n",
    "    concatenatedValFeatures6 = torch.cat((selectedNormValFeatures1, valNormFeatures6), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures6)\n",
    "    testList.append(concatenatedTestFeatures6)\n",
    "    valList.append(concatenatedValFeatures6)\n",
    "\n",
    "    #22\n",
    "    trainNormFeatures7 = get_norm_features(trainFeaturesList[6].detach().numpy())\n",
    "    testNormFeatures7 = get_norm_features(testFeaturesList[6].detach().numpy())\n",
    "    valNormFeatures7 = get_norm_features(valFeaturesList[6].detach().numpy())\n",
    "    concatenatedTrainFeatures7 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures7), dim=1)\n",
    "    concatenatedTestFeatures7 = torch.cat((selectedNormTestFeatures1, testNormFeatures7), dim=1)\n",
    "    concatenatedValFeatures7 = torch.cat((selectedNormValFeatures1, valNormFeatures7), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures7)\n",
    "    testList.append(concatenatedTestFeatures7)\n",
    "    valList.append(concatenatedValFeatures7)\n",
    "\n",
    "    trainList_tensors = [torch.tensor(item) for item in trainList]\n",
    "    testList_tensors = [torch.tensor(item) for item in testList]\n",
    "    valList_tensors = [torch.tensor(item) for item in valList]\n",
    "\n",
    "    with open(file_path1, 'wb') as file:\n",
    "        pickle.dump(trainList_tensors, file)\n",
    "    with open(file_path2, 'wb') as file:\n",
    "        pickle.dump(testList_tensors, file)\n",
    "    with open(file_path3, 'wb') as file:\n",
    "        pickle.dump(valList_tensors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6688b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "28b8647e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# for item in trainFeaturesList:\n",
    "#     print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ebb893e7",
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def to_tensor(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return torch.tensor(data)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")\n",
    "        \n",
    "def prep_data(features, labels, isOversample):\n",
    "    num_classes = 7\n",
    "\n",
    "    if isOversample:\n",
    "        X_set, Y_set = oversample_data(features, labels, num_classes)\n",
    "    else:\n",
    "        X_set, Y_set = features, labels\n",
    "\n",
    "    if isinstance(X_set, torch.Tensor):\n",
    "        X_tensor = X_set.float()\n",
    "    else:\n",
    "        X_tensor = torch.tensor(X_set, dtype=torch.float32)\n",
    "    \n",
    "    if isinstance(Y_set, torch.Tensor):\n",
    "        Y_tensor = Y_set.long()\n",
    "    else:\n",
    "        Y_tensor = torch.tensor(Y_set, dtype=torch.long)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(Y_set, return_counts=True)\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4bff2",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9a71c886",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def model_train1(X_set, Y_set, num_epochs=20, batch_size=32, loss_difference_threshold=0.01, \n",
    "#                  hidden_dims=[256, 128], dropout_rate=0.5, lr=0.0001, \n",
    "#                  optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, matchAtt\n",
    "#                  ):\n",
    "#     output_dim = 7  # Number of classes\n",
    "#     model = MyNetwork(len(X_set[0]), hidden_dims, output_dim, dropout_rate)\n",
    "#     criterion = criterion_class()\n",
    "#     optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "#     loss_history = []\n",
    "#     accuracy_history = []\n",
    "# #     print_interval = 1  # Print tqdm every epoch\n",
    "#     previous_loss = float('inf')\n",
    "\n",
    "#     # Create dataset and dataloader\n",
    "#     dataset = TensorDataset(X_set, Y_set)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     epoch_num = num_epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         correct_predictions = 0\n",
    "#         total_instances = 0\n",
    "#         with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "#             for inputs, labels in dataloader:\n",
    "#                 inputs = inputs.float()  # Ensure inputs are float32\n",
    "#                 labels = labels.long()   # Ensure labels are long\n",
    "#                 optimizer.zero_grad()\n",
    "# #                 TODO\n",
    "#                 outputs = model(inputs)\n",
    "#                 outputs = outputs.squeeze()\n",
    "#                 labels = labels.squeeze()\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # Check for NaN loss values\n",
    "#                 if torch.isnan(loss):\n",
    "#                     print(\"NaN loss encountered. Skipping this batch.\")\n",
    "#                     break\n",
    "                \n",
    "#                 # Apply gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, dim=1)\n",
    "#                 correct_predictions += (predicted == labels).sum().item()\n",
    "#                 total_instances += labels.size(0)\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#         epoch_loss = total_loss / total_instances\n",
    "#         epoch_accuracy = correct_predictions / total_instances\n",
    "#         loss_history.append(epoch_loss)\n",
    "#         accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "#         if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "#             epoch_num = epoch\n",
    "#             break\n",
    "\n",
    "#         previous_loss = epoch_loss\n",
    "\n",
    "#     return model, epoch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a0827f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X_set, Y_set):\n",
    "    indices = np.arange(len(X_set))\n",
    "    np.random.shuffle(indices)\n",
    "    return X_set[indices], Y_set[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5237e4fd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train1(X_set=None, Y_set=None, num_epochs=50, loss_difference_threshold=0.0001, \n",
    "                 hidden_dims=[64, 32], dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = MyNetwork(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=7, dropout_rate=0.5)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "                if nodalAtt:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                else:\n",
    "                    loss.backward(retain_graph=False)\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "        if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "            epoch_num = epoch\n",
    "            break\n",
    "\n",
    "        previous_loss = epoch_loss\n",
    "\n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae7cd0",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9a03e2cd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train2(X_set=None, Y_set=None, num_epochs=20, loss_difference_threshold=0.001, \n",
    "                 hidden_dims=128, dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    output_dim = 7  # Number of classes\n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = FCClassifier(input_dim, hidden_dims, output_dim, dropout_rate)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "#                 print(outputs.shape, labels.shape)\n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                if nodalAtt:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                else:\n",
    "                    loss.backward(retain_graph=False)\n",
    "                    \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(log_prob, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "                \n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "        \n",
    "        if epoch > 0:\n",
    "            loss_diff = abs(epoch_loss - previous_loss)\n",
    "            if loss_diff < loss_difference_threshold:\n",
    "                print(f\"Training stopped early at epoch {epoch+1}.\")\n",
    "                print(f\"Loss difference ({loss_diff}) is below the threshold ({loss_difference_threshold}).\")\n",
    "                epoch_num = epoch + 1\n",
    "                break\n",
    "        \n",
    "        previous_loss = epoch_loss\n",
    "    \n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b435c98b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_emotions(model=None, X_set=None, Y_set=None, typeSet=None, \n",
    "                      isSimpleFC=False, i_dict=None,\n",
    "                      nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize empty lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    i = 0\n",
    "    # Iterate over the given ranges\n",
    "    for start, end in ranges:\n",
    "        X_batch = X_set[start:end+1].float()\n",
    "        Y_batch = Y_set[start:end+1].long()\n",
    "\n",
    "#         if X_batch.dtype != torch.float32:\n",
    "#             X_batch = X_batch.float()\n",
    "\n",
    "        # Use no_grad to save memory and computations\n",
    "        with torch.no_grad():\n",
    "            inputs = X_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            labels = Y_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            last_idx = seq_len[i][0]\n",
    "            \n",
    "            umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "            outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "\n",
    "            outputs = outputs.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "            log_prob = F.log_softmax(outputs, 1)\n",
    "\n",
    "            _, predicted = torch.max(log_prob, 1)\n",
    "\n",
    "            # Append the predictions and labels to the lists\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            i = i+1\n",
    "\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'true_label': all_labels,\n",
    "        'predicted_label': all_predictions\n",
    "    })\n",
    "    \n",
    "    \n",
    "    file_name = f\"data/dump/{dataset_path}/BERT_data_for_classifier/{dictKey[i_dict]}_predictedTest.pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(df_predictions, file)\n",
    "        \n",
    "    # Generate the classification report\n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_decoder.values(), \n",
    "                                   output_dict=True, zero_division=0, digits=4)\n",
    "    print(report)\n",
    "    # Calculate the required metrics\n",
    "    accuracy = report['accuracy']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    f1_micro = report.get('micro avg', {}).get('f1-score', accuracy)\n",
    "    f1_macro = report.get('macro avg', {}).get('f1-score', 0.0) \n",
    "\n",
    "    if typeSet == \"validation\":\n",
    "        print(\"Classified: \", dictKey[i_dict])\n",
    "    \n",
    "    return dictKey[i_dict], typeSet, isSimpleFC, accuracy, recall, weighted_f1, f1_micro, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b35df7e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for trainSet, testSet, valSet, _, _, _ in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "#     print(i, type(trainSet))\n",
    "#     if isinstance(trainSet, list):\n",
    "#         print(type(trainSet[0]))\n",
    "#         sample = trainSet[0]\n",
    "#         print(sample.shape)\n",
    "#     else:\n",
    "#         print(trainSet.squeeze(0).shape)\n",
    "#     i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9231db5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getGraphComponents(file_path):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    if checkFile:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths = pickle.load(file)  \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a83d29f",
   "metadata": {
    "code_folding": [
     4,
     7,
     10
    ]
   },
   "outputs": [],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/pre_h_prime_BERT_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/pre_h_prime_BERT_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/pre_h_prime_BERT_dev.pkl'\n",
    "\n",
    "train_umask, train_seq_lengths, train_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path1)\n",
    "\n",
    "test_umask, test_seq_lengths, test_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path2)\n",
    "\n",
    "val_umask, val_seq_lengths, val_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1984b",
   "metadata": {},
   "source": [
    "UpdateJune 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "741daeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588 1588\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_umask), len(train_seq_lengths))\n",
    "print(train_seq_lengths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "163d2e11",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getSpeakersAndRanges(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        encodedSpeakers, ranges = pickle.load(file)\n",
    "    file.close()\n",
    "    return encodedSpeakers, ranges\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\"\n",
    "\n",
    "encodedSpeakersTrain, rangesTrain = getSpeakersAndRanges(file_path1)\n",
    "encodedSpeakersTest, rangesTest = getSpeakersAndRanges(file_path2)\n",
    "encodedSpeakersDev, rangesDev = getSpeakersAndRanges(file_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32d7a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in rangesTrain:\n",
    "#     if item[0] == item[1]:\n",
    "        \n",
    "#         print(\"Dup detected\", item[0], \" vs \", item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60304bfd",
   "metadata": {},
   "source": [
    "Sample run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f1571",
   "metadata": {},
   "source": [
    "Verifying the attention in batch before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f9385da0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class MatchingAttention(nn.Module):\n",
    "#     def __init__(self, mem_dim, cand_dim, alpha_dim, att_type='general2'):\n",
    "#         super(MatchingAttention, self).__init__()\n",
    "#         self.mem_dim = mem_dim\n",
    "#         self.cand_dim = cand_dim\n",
    "#         self.alpha_dim = alpha_dim\n",
    "#         self.att_type = att_type\n",
    "\n",
    "#         if self.att_type == 'general2':\n",
    "#             self.transform = nn.Linear(self.mem_dim, self.cand_dim * self.alpha_dim)\n",
    "\n",
    "#     def forward(self, M, x, mask):\n",
    "#         M_ = M.permute(1, 2, 0)  # (batch, mem_dim, seq_len)\n",
    "#         x_ = self.transform(x).unsqueeze(1)  # (batch, 1, cand_dim * alpha_dim)\n",
    "#         mask_ = mask.unsqueeze(2).repeat(1, 1, self.mem_dim).transpose(1, 2)  # (batch, mem_dim, seq_len)\n",
    "        \n",
    "#         M_ = M_ * mask_\n",
    "#         alpha = torch.bmm(x_, M_)  # (batch, 1, seq_len)\n",
    "        \n",
    "#         alpha = F.softmax(alpha, dim=-1)  # Apply softmax to get attention weights\n",
    "#         attended = torch.bmm(alpha, M.permute(1, 0, 2))  # (batch, 1, mem_dim)\n",
    "        \n",
    "#         return attended.squeeze(1), alpha\n",
    "    \n",
    "# def attentive_node_features(emotions, seq_lengths, umask, matchatt_layer):\n",
    "#     max_len = max(seq_lengths)\n",
    "#     batch_size = len(seq_lengths)\n",
    "#     mem_dim = emotions.size(1)\n",
    "\n",
    "#     padded_emotions = []\n",
    "#     for i in range(batch_size):\n",
    "#         length = seq_lengths[i]\n",
    "#         # Assuming emotions is already a 2D tensor of shape (seq_len, mem_dim)\n",
    "#         padded_emotion = F.pad(emotions[:length], (0, 0, 0, max_len - length), \"constant\", 0)\n",
    "#         padded_emotions.append(padded_emotion)\n",
    "\n",
    "#     emotions_padded = torch.stack(padded_emotions, dim=1)  # (max_len, batch_size, mem_dim)\n",
    "    \n",
    "#     att_emotions = []\n",
    "#     alpha_list = []\n",
    "#     for t in range(max_len):\n",
    "#         att_em, alpha = matchatt_layer(emotions_padded, emotions_padded[t], umask)\n",
    "#         att_emotions.append(att_em)\n",
    "#         alpha_list.append(alpha)\n",
    "\n",
    "#     att_emotions = torch.stack(att_emotions, dim=0)  # (max_len, batch_size, mem_dim)\n",
    "\n",
    "#     # Remove the singleton dimension for batch size 1\n",
    "#     att_emotions = att_emotions.squeeze(1)  # (seq_len, mem_dim)\n",
    "\n",
    "#     return att_emotions, alpha_list\n",
    "    \n",
    "# class MyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "#         super(MyNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dims[0])  # Adjust input_dim to match flattened shape\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=64, cand_dim=64, alpha_dim=1, att_type='general2')\n",
    "\n",
    "#     def forward(self, x, nodalAtt, seq_lengths, umask):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "        \n",
    "#         x = self.fc1(att_emotions)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# class FCClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "#         super(FCClassifier, self).__init__()\n",
    "#         self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=input_dim, cand_dim=input_dim, alpha_dim=1, att_type='general2')\n",
    "#     def forward(self, x=None, nodalAtt=None, seq_lengths=None, umask=None, no_cuda=True):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "            \n",
    "#         x = F.relu(self.linear1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.linear2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef46abb",
   "metadata": {},
   "source": [
    "part a test case (uncomment the top part to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a77dc9f6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# seq_lengths = [14]  # Sequence length matches the input tensor\n",
    "# umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "# inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "# # Initialize MatchingAttention\n",
    "# matchatt_layer = MatchingAttention(mem_dim=inputs.shape[1], cand_dim=inputs.shape[1], alpha_dim=1, att_type='general2')\n",
    "\n",
    "# # Call attentive_node_features\n",
    "# att_emotions, alpha_list = attentive_node_features(inputs, seq_lengths, umask, matchatt_layer)\n",
    "\n",
    "# # Print shapes for verification\n",
    "# print(\"att_emotions shape:\", att_emotions.shape)  # Should be (seq_len, mem_dim)\n",
    "# print(\"alpha_list shape:\", len(alpha_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69e91b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [14]\n",
    "umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "model = MyNetwork(input_dim=inputs.shape[1], hidden_dims=[128, 30], output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  #.Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea58147",
   "metadata": {},
   "source": [
    "part b test case (uncomment the part and the function to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0fcc9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7])\n"
     ]
    }
   ],
   "source": [
    "model = FCClassifier(input_dim=inputs.shape[1], hidden_dim=64, output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  # Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e24dbc",
   "metadata": {},
   "source": [
    "end of sample experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b7940",
   "metadata": {},
   "source": [
    "Actual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4db90480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 14]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "459175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in rangesTrain:\n",
    "#     if item[0] == item[1]:\n",
    "#         print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c6a93e4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curerntly at bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 20.\n",
      "Loss difference (0.0008872029990878993) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.391304347826087, 'recall': 0.06, 'f1-score': 0.10404624277456648, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.38961038961038963, 'recall': 0.22900763358778625, 'f1-score': 0.28846153846153844, 'support': 262.0}, 'neutral': {'precision': 0.501219512195122, 'recall': 0.9330306469920545, 'f1-score': 0.6521221737405791, 'support': 881.0}, 'sadness': {'precision': 0.6, 'recall': 0.0797872340425532, 'f1-score': 0.14084507042253522, 'support': 188.0}, 'surprise': {'precision': 0.47368421052631576, 'recall': 0.06818181818181818, 'f1-score': 0.11920529801324503, 'support': 132.0}, 'accuracy': 0.49044585987261147, 'macro avg': {'precision': 0.3365454943082735, 'recall': 0.19571533325774457, 'f1-score': 0.18638290334463775, 'support': 1884.0}, 'weighted avg': {'precision': 0.44393308516938657, 'recall': 0.49044585987261147, 'f1-score': 0.3840363076177963, 'support': 1884.0}}\n",
      "Curerntly at bert-select-few\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod\n",
      "This is skipped\n",
      "Curerntly at bert-select-more\n",
      "This is skipped\n",
      "Curerntly at dgcn\n",
      "This is skipped\n",
      "Curerntly at dgcn-select\n",
      "This is skipped\n",
      "Curerntly at gatv1\n",
      "This is skipped\n",
      "Curerntly at gatv1-select\n",
      "This is skipped\n",
      "Curerntly at gatv1-edge\n",
      "This is skipped\n",
      "Curerntly at gatv1-edge-select\n",
      "This is skipped\n",
      "Curerntly at gatv2-edge\n",
      "This is skipped\n",
      "Curerntly at gatv2-edge-select\n",
      "This is skipped\n",
      "Curerntly at rgat\n",
      "This is skipped\n",
      "Curerntly at rgat-select\n",
      "This is skipped\n",
      "Curerntly at egat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (0.0008307281798548005) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 262.0}, 'neutral': {'precision': 0.4676220806794055, 'recall': 1.0, 'f1-score': 0.637251356238698, 'support': 881.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.4676220806794055, 'macro avg': {'precision': 0.06680315438277222, 'recall': 0.14285714285714285, 'f1-score': 0.09103590803409971, 'support': 1884.0}, 'weighted avg': {'precision': 0.21867041033893644, 'recall': 0.4676220806794055, 'f1-score': 0.297992805120113, 'support': 1884.0}}\n",
      "Curerntly at egat-select\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-dgcn\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv1\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv1-edge\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv2-edge\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-rgat\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-egat\n",
      "This is skipped\n"
     ]
    }
   ],
   "source": [
    "dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        df_results_sorted = pickle.load(file)\n",
    "else:\n",
    "    results = []\n",
    "    num_epochs = 30\n",
    "    i = 0\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor = to_tensor(trainSet)\n",
    "        Y_tensor = to_tensor(y_train)\n",
    "        \n",
    "        print(\"Curerntly at\", dictKey[i])\n",
    "#         print(X_tensor.shape, Y_tensor.shape)\n",
    "#         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#                                    isSimpleFC=False, i_dict=i, \n",
    "#                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#                                    ranges=rangesTrain )\n",
    "#         results.append(result1)\n",
    "        if i in selected_combination:\n",
    "            model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "                                     umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "                                   seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "            X_tensor = to_tensor(testSet)\n",
    "            Y_tensor = to_tensor(y_test)\n",
    "            result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "                                       isSimpleFC=True, i_dict=i, \n",
    "                                       nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "                                       ranges=rangesTest)\n",
    "    #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "    #                                    isSimpleFC=True, i_dict=i, \n",
    "    #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "    #                                    ranges=rangesTrain)\n",
    "            results.append(result2)\n",
    "        else:\n",
    "            print(\"This is skipped\")\n",
    "        i = i+1\n",
    "\n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "    df_results = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2013cdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>0.384036</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>0.186383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.091036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_combination typeSet  isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "0             bert    test        True  0.490446  0.490446     0.384036   \n",
       "1             egat    test        True  0.467622  0.467622     0.297993   \n",
       "\n",
       "   F1-micro  F1-macro  \n",
       "0  0.490446  0.186383  \n",
       "1  0.467622  0.091036  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4ed51",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e36b6",
   "metadata": {},
   "source": [
    "<h4> Select top 10 unique data combinations then tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "36973bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "58e8a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1 (isSimpleFC=False): []\n",
      "Combination 2 (isSimpleFC=True): ['bert', 'egat']\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 9\n",
    "counter = 0\n",
    "combination1 = []\n",
    "combination2 = []\n",
    "seen_combinations = set()\n",
    "\n",
    "for idx, row in df_results_sorted.iterrows():\n",
    "    if counter >= max_iterations:\n",
    "        break\n",
    "\n",
    "    if row['data_combination'] in seen_combinations:\n",
    "        continue\n",
    "\n",
    "    if row['isSimpleFC']:\n",
    "        combination2.append(row['data_combination'])\n",
    "    else:\n",
    "        combination1.append(row['data_combination'])\n",
    "\n",
    "    seen_combinations.add(row['data_combination'])\n",
    "    counter += 1\n",
    "\n",
    "# Display the results\n",
    "print(\"Combination 1 (isSimpleFC=False):\", combination1)\n",
    "print(\"Combination 2 (isSimpleFC=True):\", combination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "54513411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for isSimpleFC=False: []\n",
      "Indices for isSimpleFC=True: [0, 14]\n"
     ]
    }
   ],
   "source": [
    "indices1 = [key for key, value in dictKey.items() if value in combination1]\n",
    "indices2 = [key for key, value in dictKey.items() if value in combination2]\n",
    "\n",
    "print(\"Indices for isSimpleFC=False:\", indices1)\n",
    "print(\"Indices for isSimpleFC=True:\", indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "62a5e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainDeepList = [trainList[i] for i in indices1]\n",
    "selectedTestDeepList = [testList[i] for i in indices1]\n",
    "selectedValDeepList = [valList[i] for i in indices1]\n",
    "\n",
    "len(selectedTrainDeepList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2ccae3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b3a1b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainSet in selectedTrainList:\n",
    "#     print(type(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3fee3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainList = [trainList[i] for i in indices2]\n",
    "selectedTestList = [testList[i] for i in indices2]\n",
    "selectedValList = [valList[i] for i in indices2]\n",
    "\n",
    "len(selectedTrainList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c266050",
   "metadata": {},
   "source": [
    "<h4> Tuning using random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "da2dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should call both model_train1 and 2\n",
    "\n",
    "def objective_func(X_train, X_test, X_val, y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,\n",
    "                  train_umask, test_umask, val_umask,\n",
    "                  train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                  rangesTrain, rangesTest, rangesDev, nodalAtt):\n",
    "    results = []\n",
    "    hyperparams_string = (\n",
    "        f'num_epochs={hyperparams[\"num_epochs\"]} '\n",
    "        f'loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]} '\n",
    "        f'hidden_dims={hyperparams[\"hidden_dims\"]} '\n",
    "        f'dropout_rate={hyperparams[\"dropout_rate\"]} '\n",
    "        f'learning_rate={hyperparams[\"learning_rate\"]} '\n",
    "        f'optimizers={hyperparams[\"optimizers\"]} '\n",
    "        f'criteria={hyperparams[\"criteria\"]}'\n",
    "    )    \n",
    "    print(hyperparams_string)\n",
    "            \n",
    "    X_train_tensor = to_tensor(X_train)\n",
    "    y_train_tensor = to_tensor(y_train).long()\n",
    "    X_val_tensor = to_tensor(X_val)\n",
    "    y_val_tensor = to_tensor(y_val).long()\n",
    "    X_test_tensor = to_tensor(X_test)\n",
    "    y_test_tensor = to_tensor(y_test).long()\n",
    "# train\n",
    "    start_time = time.time()\n",
    "    if isSimpleFC:\n",
    "        model, num_epoch = model_train2(X_set=X_train_tensor, Y_set=y_train_tensor, \n",
    "                            num_epochs=hyperparams[\"num_epochs\"], loss_difference_threshold=hyperparams[\"loss_difference_threshold\"], \n",
    "                            hidden_dims=hyperparams[\"hidden_dims\"], dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "                            lr=hyperparams[\"learning_rate\"], optimizer_class=hyperparams[\"optimizers\"], \n",
    "                            criterion_class=hyperparams[\"criteria\"],\n",
    "                            umask=train_umask, seq_len=train_seq_lengths, ranges=rangesTrain, nodalAtt=nodalAtt)        \n",
    "#     else:\n",
    "#         model, num_epoch = model_train1(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "#                             hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "#                             hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "#                             hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "# val\n",
    "    result = classify_emotions(model=model, X_set=X_val_tensor, Y_set=y_val_tensor, typeSet='validation', \n",
    "                               isSimpleFC=isSimpleFC, i_dict=i_dict,\n",
    "                               nodalAtt=nodalAtt, umask=val_umask, seq_len=val_seq_lengths, ranges=rangesDev)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    result = list(result)\n",
    "# test\n",
    "\n",
    "    hyperparams_string = f'num_epochs={hyperparams[\"num_epochs\"]}-loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]}-hidden_dims={hyperparams[\"hidden_dims\"]}-dropout_rate={hyperparams[\"dropout_rate\"]}-learning_rate={hyperparams[\"learning_rate\"]}-optimizers={hyperparams[\"optimizers\"]}-criteria={hyperparams[\"criteria\"]}'\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "# test\n",
    "    result = classify_emotions(model=model, X_set=X_test_tensor, Y_set=y_test_tensor, typeSet='test', \n",
    "                               isSimpleFC=isSimpleFC, i_dict=i_dict,\n",
    "                               nodalAtt=nodalAtt, umask=test_umask, seq_len=test_seq_lengths, ranges=rangesTest)\n",
    "    result = list(result)\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "#     result = classify_emotions(model, X_test_tensor, y_test_tensor, \\\n",
    "#                                'test', isSimpleFC, i_dict)\n",
    "    \n",
    "#     result = list(result)\n",
    "#     result.append(elapsed_time)\n",
    "#     result.append(hyperparams_string)\n",
    "#     result.append(num_epoch)\n",
    "#     results.append(result)\n",
    "    \n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='data_combination', ascending=False)\n",
    "    \n",
    "    return df_results_sorted\n",
    "\n",
    "\n",
    "# def objective_func(X_train, X_test, X_val, \n",
    "#                y_train, y_test, y_val, hyperparams, i_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f998054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X_train=None, X_test=None, X_val=None, \n",
    "                  y_train=None, y_test=None, y_val=None, \n",
    "                  param_grid=None, isSimpleFC=True, i_dict=None,\n",
    "                  train_umask=None, test_umask=None, val_umask=None,\n",
    "                  train_seq_lengths=None, test_seq_lengths=None, val_seq_lengths=None,\n",
    "                  rangesTrain=None, rangesTest=None, rangesDev=None, nodalAtt=False\n",
    "                 ):\n",
    "    sub_total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    MAX_EVALS = 5\n",
    "    for i in range(MAX_EVALS):\n",
    "        hyperparams = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        try:\n",
    "            new_results = objective_func(X_train, X_test, X_val,  y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,  \n",
    "                                        train_umask, test_umask, val_umask,\n",
    "                                        train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                        rangesTrain, rangesTest, rangesDev, nodalAtt)\n",
    "            sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparams {hyperparams}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    return sub_total_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7295744e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [[256, 128], [128, 64], [64, 32]],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}\n",
    "param_grid2 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [128, 256, 512],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcf2b3",
   "metadata": {},
   "source": [
    "<h5> First find the best hyperparameter combination for the DeepClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "163a1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamTuning(X_trainSet, X_testSet, X_valSet, y_train, y_test, y_val, isSimpleFC, param_grid, indices,\n",
    "                    train_umask, test_umask, val_umask,\n",
    "                    train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                    rangesTrain, rangesTest, rangesDev, nodalAttn):\n",
    "    \n",
    "    total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    \n",
    "    for i in indices:\n",
    "        print(\"============\", dictKey[i], \"============\")\n",
    "        X_train = X_trainSet[i]\n",
    "        X_test = X_testSet[i]\n",
    "        X_val = X_valSet[i]\n",
    "\n",
    "        sub_total_results = random_search(X_train, X_test, X_val, y_train, y_test, y_val,\n",
    "                     param_grid, isSimpleFC, i,\n",
    "                     train_umask, test_umask, val_umask,\n",
    "                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                     rangesTrain, rangesTest, rangesDev, nodalAttn[i])\n",
    "\n",
    "        total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be82a8",
   "metadata": {},
   "source": [
    "Uncomment below next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3e1fd4d3",
   "metadata": {
    "code_folding": [
     3,
     6
    ]
   },
   "outputs": [],
   "source": [
    "# file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/deep_classifier_tuned_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         total_results1_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     total_results1 = hyperparamTuning(selectedTrainDeepList, selectedTestDeepList, selectedValDeepList, \\\n",
    "#                                  y_train, y_test, y_val, False, param_grid1, indices1)\n",
    "    \n",
    "#     total_results1_sorted = total_results1.sort_values(by='Weighted-F1', ascending=False)\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(total_results1_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "90c5d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "# pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "# pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "\n",
    "# total_results1_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e1ea75ad",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "# dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         df_results_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     results = []\n",
    "#     num_epochs = 30\n",
    "#     i = 0\n",
    "#     for trainSet, testSet, valSet in dataLoader:\n",
    "#         if isinstance(trainSet, list):\n",
    "#             trainSet = trainSet[0].squeeze(0)\n",
    "#             testSet = testSet[0].squeeze(0)\n",
    "#             valSet = valSet[0].squeeze(0)\n",
    "#         else:\n",
    "#             trainSet = trainSet.squeeze(0)\n",
    "#             testSet = testSet.squeeze(0)\n",
    "#             valSet = valSet.squeeze(0)\n",
    "\n",
    "#         X_tensor = to_tensor(trainSet)\n",
    "#         Y_tensor = to_tensor(y_train)\n",
    "\n",
    "#         print(\"Curerntly at\", dictKey[i])\n",
    "# #         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "# #                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "# #                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "# #         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "# #                                    isSimpleFC=False, i_dict=i, \n",
    "# #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "# #                                    ranges=rangesTrain )\n",
    "# #         results.append(result1)\n",
    "#         if i in selected_combination:\n",
    "#             model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                      umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                    seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#             X_tensor = to_tensor(testSet)\n",
    "#             Y_tensor = to_tensor(y_test)\n",
    "#             result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "#                                        isSimpleFC=True, i_dict=i, \n",
    "#                                        nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "#                                        ranges=rangesTest)\n",
    "#     #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#     #                                    isSimpleFC=True, i_dict=i, \n",
    "#     #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#     #                                    ranges=rangesTrain)\n",
    "#             results.append(result2)\n",
    "#         else:\n",
    "#             print(\"This is skipped\")\n",
    "#         i = i+1\n",
    "\n",
    "#     columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "#     df_results = pd.DataFrame(results, columns=columns)\n",
    "#     df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d6d089c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ bert ============\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_16852\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.29347826086956524, 'recall': 0.19285714285714287, 'f1-score': 0.23275862068965517, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.3380281690140845, 'recall': 0.18045112781954886, 'f1-score': 0.23529411764705882, 'support': 133.0}, 'neutral': {'precision': 0.43144424131627057, 'recall': 0.7539936102236422, 'f1-score': 0.5488372093023256, 'support': 313.0}, 'sadness': {'precision': 0.4090909090909091, 'recall': 0.08571428571428572, 'f1-score': 0.14173228346456693, 'support': 105.0}, 'surprise': {'precision': 0.3225806451612903, 'recall': 0.28169014084507044, 'f1-score': 0.3007518796992481, 'support': 71.0}, 'accuracy': 0.39060568603213847, 'macro avg': {'precision': 0.2563746036360171, 'recall': 0.21352947249424142, 'f1-score': 0.20848201582897924, 'support': 809.0}, 'weighted avg': {'precision': 0.3546903854061831, 'recall': 0.39060568603213847, 'f1-score': 0.3360959756213407, 'support': 809.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.28688524590163933, 'recall': 0.23333333333333334, 'f1-score': 0.25735294117647056, 'support': 300.0}, 'disgust': {'precision': 0.12121212121212122, 'recall': 0.05714285714285714, 'f1-score': 0.07766990291262135, 'support': 70.0}, 'fear': {'precision': 0.16666666666666666, 'recall': 0.0392156862745098, 'f1-score': 0.06349206349206349, 'support': 51.0}, 'joy': {'precision': 0.37748344370860926, 'recall': 0.21755725190839695, 'f1-score': 0.27602905569007263, 'support': 262.0}, 'neutral': {'precision': 0.5426179604261796, 'recall': 0.8093076049943246, 'f1-score': 0.6496583143507972, 'support': 881.0}, 'sadness': {'precision': 0.4473684210526316, 'recall': 0.09042553191489362, 'f1-score': 0.1504424778761062, 'support': 188.0}, 'surprise': {'precision': 0.30434782608695654, 'recall': 0.21212121212121213, 'f1-score': 0.25, 'support': 132.0}, 'accuracy': 0.4729299363057325, 'macro avg': {'precision': 0.32094024072211486, 'recall': 0.2370147825270754, 'f1-score': 0.24637782221401877, 'support': 1884.0}, 'weighted avg': {'precision': 0.4268984521464082, 'recall': 0.4729299363057325, 'f1-score': 0.42029333554643267, 'support': 1884.0}}\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (9.170721795903614e-05) is below the threshold (0.0001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133.0}, 'neutral': {'precision': 0.3868974042027194, 'recall': 1.0, 'f1-score': 0.5579322638146168, 'support': 313.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 105.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 71.0}, 'accuracy': 0.3868974042027194, 'macro avg': {'precision': 0.05527105774324563, 'recall': 0.14285714285714285, 'f1-score': 0.07970460911637382, 'support': 809.0}, 'weighted avg': {'precision': 0.14968960137880244, 'recall': 0.3868974042027194, 'f1-score': 0.21586254459082208, 'support': 809.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 262.0}, 'neutral': {'precision': 0.4676220806794055, 'recall': 1.0, 'f1-score': 0.637251356238698, 'support': 881.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.4676220806794055, 'macro avg': {'precision': 0.06680315438277222, 'recall': 0.14285714285714285, 'f1-score': 0.09103590803409971, 'support': 1884.0}, 'weighted avg': {'precision': 0.21867041033893644, 'recall': 0.4676220806794055, 'f1-score': 0.297992805120113, 'support': 1884.0}}\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 28.\n",
      "Loss difference (5.0145251600075547e-05) is below the threshold (0.0001).\n",
      "{'anger': {'precision': 0.2894736842105263, 'recall': 0.07857142857142857, 'f1-score': 0.12359550561797752, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 1.0, 'recall': 0.034482758620689655, 'f1-score': 0.06666666666666667, 'support': 29.0}, 'joy': {'precision': 0.35294117647058826, 'recall': 0.22556390977443608, 'f1-score': 0.27522935779816515, 'support': 133.0}, 'neutral': {'precision': 0.4239316239316239, 'recall': 0.792332268370607, 'f1-score': 0.5523385300668151, 'support': 313.0}, 'sadness': {'precision': 0.30612244897959184, 'recall': 0.14285714285714285, 'f1-score': 0.19480519480519481, 'support': 105.0}, 'surprise': {'precision': 0.35714285714285715, 'recall': 0.2112676056338028, 'f1-score': 0.26548672566371684, 'support': 71.0}, 'accuracy': 0.39555006180469715, 'macro avg': {'precision': 0.38994454153359825, 'recall': 0.21215358768972956, 'f1-score': 0.21116028294550512, 'support': 809.0}, 'weighted avg': {'precision': 0.37905820834445025, 'recall': 0.39555006180469715, 'f1-score': 0.3313081231082678, 'support': 809.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.3125, 'recall': 0.1, 'f1-score': 0.15151515151515152, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.36363636363636365, 'recall': 0.2595419847328244, 'f1-score': 0.3028953229398664, 'support': 262.0}, 'neutral': {'precision': 0.5306859205776173, 'recall': 0.8342792281498297, 'f1-score': 0.6487202118270079, 'support': 881.0}, 'sadness': {'precision': 0.32456140350877194, 'recall': 0.19680851063829788, 'f1-score': 0.24503311258278146, 'support': 188.0}, 'surprise': {'precision': 0.2289156626506024, 'recall': 0.14393939393939395, 'f1-score': 0.17674418604651163, 'support': 132.0}, 'accuracy': 0.4718683651804671, 'macro avg': {'precision': 0.2514713357676222, 'recall': 0.21922415963719227, 'f1-score': 0.2178439978444741, 'support': 1884.0}, 'weighted avg': {'precision': 0.39691689736259916, 'recall': 0.4718683651804671, 'f1-score': 0.40643953524845366, 'support': 1884.0}}\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 22.\n",
      "Loss difference (3.8991413212452475e-05) is below the threshold (0.0001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.5, 'recall': 0.007518796992481203, 'f1-score': 0.014814814814814815, 'support': 133.0}, 'neutral': {'precision': 0.3878562577447336, 'recall': 1.0, 'f1-score': 0.5589285714285714, 'support': 313.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 105.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 71.0}, 'accuracy': 0.38813349814585907, 'macro avg': {'precision': 0.12683660824924764, 'recall': 0.1439312567132116, 'f1-score': 0.08196334089191233, 'support': 809.0}, 'weighted avg': {'precision': 0.23226082654400693, 'recall': 0.38813349814585907, 'f1-score': 0.21868357630100524, 'support': 809.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.25, 'recall': 0.003816793893129771, 'f1-score': 0.007518796992481203, 'support': 262.0}, 'neutral': {'precision': 0.46726982437466735, 'recall': 0.996594778660613, 'f1-score': 0.636231884057971, 'support': 881.0}, 'sadness': {'precision': 1.0, 'recall': 0.005319148936170213, 'f1-score': 0.010582010582010581, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.46709129511677283, 'macro avg': {'precision': 0.24532426062495247, 'recall': 0.14367581735570184, 'f1-score': 0.09347609880463756, 'support': 1884.0}, 'weighted avg': {'precision': 0.35305982764017085, 'recall': 0.46709129511677283, 'f1-score': 0.2996176394142891, 'support': 1884.0}}\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_16852\\1322761683.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.4838709677419355, 'recall': 0.10714285714285714, 'f1-score': 0.17543859649122806, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.42592592592592593, 'recall': 0.17293233082706766, 'f1-score': 0.24598930481283424, 'support': 133.0}, 'neutral': {'precision': 0.4183381088825215, 'recall': 0.9329073482428115, 'f1-score': 0.5776458951533135, 'support': 313.0}, 'sadness': {'precision': 0.9, 'recall': 0.08571428571428572, 'f1-score': 0.1565217391304348, 'support': 105.0}, 'surprise': {'precision': 0.1875, 'recall': 0.04225352112676056, 'f1-score': 0.06896551724137931, 'support': 71.0}, 'accuracy': 0.4227441285537701, 'macro avg': {'precision': 0.3450907146500547, 'recall': 0.1915643347219689, 'f1-score': 0.17493729326131285, 'support': 809.0}, 'weighted avg': {'precision': 0.4488781356146457, 'recall': 0.4227441285537701, 'f1-score': 0.32065819600086476, 'support': 809.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.3559322033898305, 'recall': 0.07, 'f1-score': 0.116991643454039, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.44360902255639095, 'recall': 0.22519083969465647, 'f1-score': 0.29873417721518986, 'support': 262.0}, 'neutral': {'precision': 0.5003043213633597, 'recall': 0.9330306469920545, 'f1-score': 0.6513470681458003, 'support': 881.0}, 'sadness': {'precision': 0.65, 'recall': 0.06914893617021277, 'f1-score': 0.125, 'support': 188.0}, 'surprise': {'precision': 0.3103448275862069, 'recall': 0.06818181818181818, 'f1-score': 0.11180124223602485, 'support': 132.0}, 'accuracy': 0.49044585987261147, 'macro avg': {'precision': 0.3228843392708268, 'recall': 0.1950788915769631, 'f1-score': 0.18626773300729343, 'support': 1884.0}, 'weighted avg': {'precision': 0.4389272023828147, 'recall': 0.49044585987261147, 'f1-score': 0.3850638951582786, 'support': 1884.0}}\n",
      "============ egat ============\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133.0}, 'neutral': {'precision': 0.3868974042027194, 'recall': 1.0, 'f1-score': 0.5579322638146168, 'support': 313.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 105.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 71.0}, 'accuracy': 0.3868974042027194, 'macro avg': {'precision': 0.05527105774324563, 'recall': 0.14285714285714285, 'f1-score': 0.07970460911637382, 'support': 809.0}, 'weighted avg': {'precision': 0.14968960137880244, 'recall': 0.3868974042027194, 'f1-score': 0.21586254459082208, 'support': 809.0}}\n",
      "Classified:  egat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_16852\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 262.0}, 'neutral': {'precision': 0.4676220806794055, 'recall': 1.0, 'f1-score': 0.637251356238698, 'support': 881.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.4676220806794055, 'macro avg': {'precision': 0.06680315438277222, 'recall': 0.14285714285714285, 'f1-score': 0.09103590803409971, 'support': 1884.0}, 'weighted avg': {'precision': 0.21867041033893644, 'recall': 0.4676220806794055, 'f1-score': 0.297992805120113, 'support': 1884.0}}\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0002765315886798514) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133.0}, 'neutral': {'precision': 0.3868974042027194, 'recall': 1.0, 'f1-score': 0.5579322638146168, 'support': 313.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 105.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 71.0}, 'accuracy': 0.3868974042027194, 'macro avg': {'precision': 0.05527105774324563, 'recall': 0.14285714285714285, 'f1-score': 0.07970460911637382, 'support': 809.0}, 'weighted avg': {'precision': 0.14968960137880244, 'recall': 0.3868974042027194, 'f1-score': 0.21586254459082208, 'support': 809.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 262.0}, 'neutral': {'precision': 0.4676220806794055, 'recall': 1.0, 'f1-score': 0.637251356238698, 'support': 881.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.4676220806794055, 'macro avg': {'precision': 0.06680315438277222, 'recall': 0.14285714285714285, 'f1-score': 0.09103590803409971, 'support': 1884.0}, 'weighted avg': {'precision': 0.21867041033893644, 'recall': 0.4676220806794055, 'f1-score': 0.297992805120113, 'support': 1884.0}}\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133.0}, 'neutral': {'precision': 0.3868974042027194, 'recall': 1.0, 'f1-score': 0.5579322638146168, 'support': 313.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 105.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 71.0}, 'accuracy': 0.3868974042027194, 'macro avg': {'precision': 0.05527105774324563, 'recall': 0.14285714285714285, 'f1-score': 0.07970460911637382, 'support': 809.0}, 'weighted avg': {'precision': 0.14968960137880244, 'recall': 0.3868974042027194, 'f1-score': 0.21586254459082208, 'support': 809.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 262.0}, 'neutral': {'precision': 0.4676220806794055, 'recall': 1.0, 'f1-score': 0.637251356238698, 'support': 881.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.4676220806794055, 'macro avg': {'precision': 0.06680315438277222, 'recall': 0.14285714285714285, 'f1-score': 0.09103590803409971, 'support': 1884.0}, 'weighted avg': {'precision': 0.21867041033893644, 'recall': 0.4676220806794055, 'f1-score': 0.297992805120113, 'support': 1884.0}}\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133.0}, 'neutral': {'precision': 0.3868974042027194, 'recall': 1.0, 'f1-score': 0.5579322638146168, 'support': 313.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 105.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 71.0}, 'accuracy': 0.3868974042027194, 'macro avg': {'precision': 0.05527105774324563, 'recall': 0.14285714285714285, 'f1-score': 0.07970460911637382, 'support': 809.0}, 'weighted avg': {'precision': 0.14968960137880244, 'recall': 0.3868974042027194, 'f1-score': 0.21586254459082208, 'support': 809.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 262.0}, 'neutral': {'precision': 0.4676220806794055, 'recall': 1.0, 'f1-score': 0.637251356238698, 'support': 881.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.4676220806794055, 'macro avg': {'precision': 0.06680315438277222, 'recall': 0.14285714285714285, 'f1-score': 0.09103590803409971, 'support': 1884.0}, 'weighted avg': {'precision': 0.21867041033893644, 'recall': 0.4676220806794055, 'f1-score': 0.297992805120113, 'support': 1884.0}}\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 4.\n",
      "Loss difference (0.0006122730517027497) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 140.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133.0}, 'neutral': {'precision': 0.3868974042027194, 'recall': 1.0, 'f1-score': 0.5579322638146168, 'support': 313.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 105.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 71.0}, 'accuracy': 0.3868974042027194, 'macro avg': {'precision': 0.05527105774324563, 'recall': 0.14285714285714285, 'f1-score': 0.07970460911637382, 'support': 809.0}, 'weighted avg': {'precision': 0.14968960137880244, 'recall': 0.3868974042027194, 'f1-score': 0.21586254459082208, 'support': 809.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 300.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 70.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 51.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 262.0}, 'neutral': {'precision': 0.4676220806794055, 'recall': 1.0, 'f1-score': 0.637251356238698, 'support': 881.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 188.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 132.0}, 'accuracy': 0.4676220806794055, 'macro avg': {'precision': 0.06680315438277222, 'recall': 0.14285714285714285, 'f1-score': 0.09103590803409971, 'support': 1884.0}, 'weighted avg': {'precision': 0.21867041033893644, 'recall': 0.4676220806794055, 'f1-score': 0.297992805120113, 'support': 1884.0}}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/simple_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results2_sorted = pickle.load(file)\n",
    "else: \n",
    "\n",
    "    total_results2 = hyperparamTuning(trainList, testList, valList, \n",
    "                                     y_train, y_test, y_val, True, param_grid2, indices2,\n",
    "                                     train_umask, test_umask, val_umask,\n",
    "                                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                     rangesTrain, rangesTest, rangesDev, nodalAttn)\n",
    "    \n",
    "    total_results2_sorted = total_results2.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results2_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7797910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.472930</td>\n",
       "      <td>0.472930</td>\n",
       "      <td>0.420293</td>\n",
       "      <td>0.472930</td>\n",
       "      <td>0.246378</td>\n",
       "      <td>157.463449</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.471868</td>\n",
       "      <td>0.471868</td>\n",
       "      <td>0.406440</td>\n",
       "      <td>0.471868</td>\n",
       "      <td>0.217844</td>\n",
       "      <td>104.404705</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>0.385064</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>0.186268</td>\n",
       "      <td>104.471232</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.390606</td>\n",
       "      <td>0.390606</td>\n",
       "      <td>0.336096</td>\n",
       "      <td>0.390606</td>\n",
       "      <td>0.208482</td>\n",
       "      <td>157.463449</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.395550</td>\n",
       "      <td>0.395550</td>\n",
       "      <td>0.331308</td>\n",
       "      <td>0.395550</td>\n",
       "      <td>0.211160</td>\n",
       "      <td>104.404705</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.422744</td>\n",
       "      <td>0.422744</td>\n",
       "      <td>0.320658</td>\n",
       "      <td>0.422744</td>\n",
       "      <td>0.174937</td>\n",
       "      <td>104.471232</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467091</td>\n",
       "      <td>0.467091</td>\n",
       "      <td>0.299618</td>\n",
       "      <td>0.467091</td>\n",
       "      <td>0.093476</td>\n",
       "      <td>34.293842</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.091036</td>\n",
       "      <td>1949.611947</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.091036</td>\n",
       "      <td>2965.685751</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.091036</td>\n",
       "      <td>159.028963</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.091036</td>\n",
       "      <td>1179.856067</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.091036</td>\n",
       "      <td>130.226914</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.467622</td>\n",
       "      <td>0.091036</td>\n",
       "      <td>9.030041</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.388133</td>\n",
       "      <td>0.388133</td>\n",
       "      <td>0.218684</td>\n",
       "      <td>0.388133</td>\n",
       "      <td>0.081963</td>\n",
       "      <td>34.293842</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.215863</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>1949.611947</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.215863</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>2965.685751</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.215863</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>159.028963</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.215863</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>9.030041</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.215863</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>130.226914</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.215863</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.079705</td>\n",
       "      <td>1179.856067</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_combination     typeSet isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "11             bert        test       True  0.472930  0.472930     0.420293   \n",
       "15             bert        test       True  0.471868  0.471868     0.406440   \n",
       "19             bert        test       True  0.490446  0.490446     0.385064   \n",
       "10             bert  validation       True  0.390606  0.390606     0.336096   \n",
       "14             bert  validation       True  0.395550  0.395550     0.331308   \n",
       "18             bert  validation       True  0.422744  0.422744     0.320658   \n",
       "17             bert        test       True  0.467091  0.467091     0.299618   \n",
       "5              egat        test       True  0.467622  0.467622     0.297993   \n",
       "7              egat        test       True  0.467622  0.467622     0.297993   \n",
       "9              egat        test       True  0.467622  0.467622     0.297993   \n",
       "1              egat        test       True  0.467622  0.467622     0.297993   \n",
       "3              egat        test       True  0.467622  0.467622     0.297993   \n",
       "13             bert        test       True  0.467622  0.467622     0.297993   \n",
       "16             bert  validation       True  0.388133  0.388133     0.218684   \n",
       "4              egat  validation       True  0.386897  0.386897     0.215863   \n",
       "6              egat  validation       True  0.386897  0.386897     0.215863   \n",
       "8              egat  validation       True  0.386897  0.386897     0.215863   \n",
       "12             bert  validation       True  0.386897  0.386897     0.215863   \n",
       "2              egat  validation       True  0.386897  0.386897     0.215863   \n",
       "0              egat  validation       True  0.386897  0.386897     0.215863   \n",
       "\n",
       "    F1-micro  F1-macro   train_time  \\\n",
       "11  0.472930  0.246378   157.463449   \n",
       "15  0.471868  0.217844   104.404705   \n",
       "19  0.490446  0.186268   104.471232   \n",
       "10  0.390606  0.208482   157.463449   \n",
       "14  0.395550  0.211160   104.404705   \n",
       "18  0.422744  0.174937   104.471232   \n",
       "17  0.467091  0.093476    34.293842   \n",
       "5   0.467622  0.091036  1949.611947   \n",
       "7   0.467622  0.091036  2965.685751   \n",
       "9   0.467622  0.091036   159.028963   \n",
       "1   0.467622  0.091036  1179.856067   \n",
       "3   0.467622  0.091036   130.226914   \n",
       "13  0.467622  0.091036     9.030041   \n",
       "16  0.388133  0.081963    34.293842   \n",
       "4   0.386897  0.079705  1949.611947   \n",
       "6   0.386897  0.079705  2965.685751   \n",
       "8   0.386897  0.079705   159.028963   \n",
       "12  0.386897  0.079705     9.030041   \n",
       "2   0.386897  0.079705   130.226914   \n",
       "0   0.386897  0.079705  1179.856067   \n",
       "\n",
       "                                                                                                                                                                                                 hyperparams  \\\n",
       "11           num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "15          num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "19          num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "10           num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "14          num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "18          num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "17    num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "5             num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "7              num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "9              num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "1   num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "3            num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "13             num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "16    num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "4             num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "6              num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "8              num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "12             num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "2            num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "0   num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "\n",
       "   num_epoch  \n",
       "11        20  \n",
       "15        28  \n",
       "19        20  \n",
       "10        20  \n",
       "14        28  \n",
       "18        20  \n",
       "17        22  \n",
       "5         30  \n",
       "7         40  \n",
       "9          4  \n",
       "1         20  \n",
       "3          3  \n",
       "13         5  \n",
       "16        22  \n",
       "4         30  \n",
       "6         40  \n",
       "8          4  \n",
       "12         5  \n",
       "2          3  \n",
       "0         20  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "    \n",
    "total_results2_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7f728",
   "metadata": {},
   "source": [
    "### Use the tuned result to predict the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c3c6ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [141]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_difference_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriteria\u001b[39m\u001b[38;5;124m'\u001b[39m: nn\u001b[38;5;241m.\u001b[39mNLLLoss\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m tuned_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_combination\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtypeSet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misSimpleFC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \\\n\u001b[0;32m     12\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeighted-F1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-micro\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-macro\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparams\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m],)\n\u001b[1;32m---> 14\u001b[0m new_results \u001b[38;5;241m=\u001b[39m \u001b[43mobjective_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_umask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_umask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_umask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_seq_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_seq_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seq_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrangesTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrangesTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrangesDev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodalAttn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m tuned_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([tuned_results, new_results], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m tuned_results\n",
      "Input \u001b[1;32mIn [132]\u001b[0m, in \u001b[0;36mobjective_func\u001b[1;34m(X_train, X_test, X_val, y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC, train_umask, test_umask, val_umask, train_seq_lengths, test_seq_lengths, val_seq_lengths, rangesTrain, rangesTest, rangesDev, nodalAtt)\u001b[0m\n\u001b[0;32m     26\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isSimpleFC:\n\u001b[1;32m---> 28\u001b[0m         model, num_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_difference_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_difference_threshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcriterion_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcriteria\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mumask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_umask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_seq_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrangesTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodalAtt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodalAtt\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#         model, num_epoch = model_train1(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#                             hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#                             hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#                             hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[1;32mIn [109]\u001b[0m, in \u001b[0;36mmodel_train2\u001b[1;34m(X_set, Y_set, num_epochs, loss_difference_threshold, hidden_dims, dropout_rate, lr, optimizer_class, criterion_class, nodalAtt, umask, seq_len, no_cuda, ranges)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     61\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(log_prob, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:443\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 443\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 30,\n",
    "    'loss_difference_threshold': 0.001,\n",
    "    'hidden_dims': 512,\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[0], testList[0], valList[0],  y_train, y_test, y_val, hyperparams, 0, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[0])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be810eb",
   "metadata": {},
   "source": [
    "rerun again to correct the model name in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 20,\n",
    "    'loss_difference_threshold': 0.0001,\n",
    "    'hidden_dims': 128,\n",
    "    'dropout_rate': 0.5,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[3], testList[3], valList[3],  y_train, y_test, y_val, hyperparams, 3, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[3])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 40,\n",
    "    'loss_difference_threshold': 0.0001,\n",
    "    'hidden_dims': 128,\n",
    "    'dropout_rate': 0.5,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[1], testList[1], valList[1],  y_train, y_test, y_val, hyperparams, 1, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[1])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b74053",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 40,\n",
    "    'loss_difference_threshold': 0.001,\n",
    "    'hidden_dims': 512,\n",
    "    'dropout_rate': 0.7,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[2], testList[2], valList[2],  y_train, y_test, y_val, hyperparams, 2, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[2])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
