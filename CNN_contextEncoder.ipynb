{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7856ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, pickle, collections, importlib, datetime, torch, nltk, pandas as pd, numpy as np, random\n",
    "from chardet import detect\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict, Counter\n",
    "from wordebd import WORDEBD\n",
    "from vocab import Vocab, Vectors\n",
    "from munch import Munch\n",
    "from cnnlstmseq import CNNLSTMseq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from model import batch_graphify, LSTMModel,MaskedEdgeAttention\n",
    "# Autoreload extensions (if you're using Jupyter Notebook or IPython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f23de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d7c41",
   "metadata": {},
   "source": [
    "<b>Make sure to specify which dataset to use\n",
    "<br>\n",
    " - dataset_original\n",
    "<br>\n",
    " - dataset_drop_noise\n",
    "<br>\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5dd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c5414b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_data(data):\n",
    "    data = data.apply(lambda x: x.lower())\n",
    "    data = data.apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbba9e3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, filters, kernel_sizes, dropout):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=K) for K in kernel_sizes])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * filters, output_size)\n",
    "        self.feature_dim = output_size\n",
    "\n",
    "    def init_pretrained_embeddings_from_numpy(self, pretrained_word_vectors):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x, umask):\n",
    "        if len(x.size()) == 2:  # Check if x has only 2 dimensions\n",
    "            num_utt, num_words = x.size()\n",
    "            batch = 1\n",
    "        else:\n",
    "            num_utt, batch, num_words = x.size()\n",
    "\n",
    "        x = x.type(torch.LongTensor)\n",
    "        x = x.view(-1, num_words)  # Flatten to (num_utt * batch, num_words)\n",
    "        torch.manual_seed(SEED)\n",
    "        emb = self.embedding(x)  # Embed (num_utt * batch, num_words) -> (num_utt * batch, num_words, embedding_dim)\n",
    "        emb = emb.transpose(-2, -1).contiguous()  # (num_utt * batch, num_words, embedding_dim) -> (num_utt * batch, embedding_dim, num_words)\n",
    "\n",
    "        convoluted = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in convoluted]\n",
    "        concated = torch.cat(pooled, 1)\n",
    "        features = F.relu(self.fc(self.dropout(concated)))  # Apply dropout and fully connected layer\n",
    "        features = features.view(num_utt, batch, -1)  # Reshape back to (num_utt, batch, feature_dim)\n",
    "\n",
    "        mask = umask.unsqueeze(-1).type(torch.FloatTensor)  # (batch, num_utt) -> (batch, num_utt, 1)\n",
    "        mask = mask.transpose(0, 1)  # (batch, num_utt, 1) -> (num_utt, batch, 1)\n",
    "        mask = mask.repeat(1, 1, self.feature_dim)  # (num_utt, batch, 1) -> (num_utt, batch, feature_dim)\n",
    "        features = features * mask  # Apply mask\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a273b63b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e24b0d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_glove():\n",
    "    print(\"Loading GloVe...\")\n",
    "    glv_vector = {}\n",
    "    f = open('embed/glove/glove.840B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float')\n",
    "            glv_vector[word] = coefs\n",
    "        except ValueError:\n",
    "            continue\n",
    "    f.close()\n",
    "    print(\"Completed loading pretrained GloVe model.\")\n",
    "    return glv_vector\n",
    "\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "def _read_words(data, convmode=None):\n",
    "    '''    \n",
    "    Count the occurrences of all words\n",
    "    @param convmode: str, None for non conversational scope, 'naive' for classic or naive approach, 'conv' for conversation depth into account (one additional dim and nested values)\n",
    "    @param data: list of examples\n",
    "    @return words: list of words (with duplicates)\n",
    "    '''    \n",
    "    words = []\n",
    "    if convmode is None:\n",
    "        for example in data:\n",
    "            words += example.split()\n",
    "    return words\n",
    "\n",
    "def find_value_ranges(lst):\n",
    "    value_ranges = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            value_ranges.append((start_index, i - 1))\n",
    "            start_index = i\n",
    "\n",
    "    # Add the last range\n",
    "    value_ranges.append((start_index, len(lst) - 1))\n",
    "\n",
    "    return value_ranges\n",
    "\n",
    "def seed_everything(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd611917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/DatasetPreparation/X_train.csv', encoding='shift_jis')\n",
    "\n",
    "# # Print the column names\n",
    "# print(df.columns)\n",
    "\n",
    "# columns_to_use = df.columns[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2cf5f5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Read the CSV file\n",
    "# X_train = pd.read_csv('data\\DatasetPreparation\\X_train.csv', encoding='shift_jis', usecols=columns_to_use)\n",
    "# X_test = pd.read_csv('data\\DatasetPreparation\\X_test.csv', encoding='shift_jis', usecols=columns_to_use)\n",
    "# X_dev = pd.read_csv('data\\DatasetPreparation\\X_dev.csv', encoding='shift_jis', usecols=columns_to_use)\n",
    "\n",
    "# y_train = pd.read_csv('data\\DatasetPreparation\\y_train.csv', encoding='shift_jis')\n",
    "# y_test = pd.read_csv('data\\DatasetPreparation\\y_test.csv', encoding='shift_jis')\n",
    "# y_dev = pd.read_csv('data\\DatasetPreparation\\y_dev.csv', encoding='shift_jis')\n",
    "\n",
    "# # Display the first three rows\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# print(X_dev.shape)\n",
    "# print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bf1a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12840, 12)\n",
      "(3400, 12)\n",
      "(1462, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('data/' + dataset_path + '/train_sent_emo_dya.csv', encoding='shift_jis')\n",
    "X_test = pd.read_csv('data/' + dataset_path+ '/test_sent_emo_dya.csv', encoding='utf-8')\n",
    "X_dev = pd.read_csv('data/' + dataset_path + '/dev_sent_emo_dya.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "# Display the first three rows\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_dev.shape)\n",
    "\n",
    "# Define features to drop\n",
    "drop_features = list(X_train.columns[6:]) \n",
    "\n",
    "# Create DataFrame for target labels\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "y_dev = pd.DataFrame()\n",
    "\n",
    "y_train[\"Emotion\"] = X_train[\"Emotion\"].copy()\n",
    "y_test[\"Emotion\"] = X_test[\"Emotion\"].copy()\n",
    "y_dev[\"Emotion\"] = X_dev[\"Emotion\"].copy()\n",
    "\n",
    "y_train[\"Dialogue_ID\"] = X_train[\"Dialogue_ID\"].copy()\n",
    "y_test[\"Dialogue_ID\"] = X_test[\"Dialogue_ID\"].copy()\n",
    "y_dev[\"Dialogue_ID\"] = X_dev[\"Dialogue_ID\"].copy()\n",
    "\n",
    "\n",
    "\n",
    "# Drop features from X_train DataFrame\n",
    "X_train = X_train.drop(drop_features, axis=1)\n",
    "X_train[\"Utterance\"] = processed_data(X_train[\"Utterance\"]) \n",
    "X_test = X_test.drop(drop_features, axis=1)\n",
    "X_test[\"Utterance\"] = processed_data(X_test[\"Utterance\"]) \n",
    "X_dev = X_dev.drop(drop_features, axis=1)\n",
    "X_dev[\"Utterance\"] = processed_data(X_dev[\"Utterance\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3abd8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now you’ll be heading a whole division, so you...</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I see.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>But there’ll be perhaps 30 people under you so...</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Good to know.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We can go into detail</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance          Speaker  \\\n",
       "0  also I was the point person on my company’s tr...         Chandler   \n",
       "1                   You must’ve had your hands full.  The Interviewer   \n",
       "2                            That I did. That I did.         Chandler   \n",
       "3      So let’s talk a little bit about your duties.  The Interviewer   \n",
       "4                             My duties?  All right.         Chandler   \n",
       "5  Now you’ll be heading a whole division, so you...  The Interviewer   \n",
       "6                                             I see.         Chandler   \n",
       "7  But there’ll be perhaps 30 people under you so...  The Interviewer   \n",
       "8                                      Good to know.         Chandler   \n",
       "9                              We can go into detail  The Interviewer   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  \n",
       "0   neutral   neutral            0             0  \n",
       "1   neutral   neutral            0             1  \n",
       "2   neutral   neutral            0             2  \n",
       "3   neutral   neutral            0             3  \n",
       "4  surprise  positive            0             4  \n",
       "5   neutral   neutral            0             5  \n",
       "6   neutral   neutral            0             6  \n",
       "7   neutral   neutral            0             7  \n",
       "8   neutral   neutral            0             8  \n",
       "9   neutral   neutral            0             9  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86036b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_encoder.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_decoder.pkl\")\n",
    "\n",
    "if not(checkFile1 and checkFile2):\n",
    "    labels = sorted(set(y_train.Emotion))\n",
    "    labelEncoder = {label: i for i, label in enumerate(labels)}\n",
    "    labelDecoder = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    pickle.dump(labelEncoder, open('data/dump/' + dataset_path + '/label_encoder.pkl', 'wb'))\n",
    "    pickle.dump(labelDecoder, open('data/dump/' + dataset_path + '/label_decoder.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('data/dump/' + dataset_path +'/label_encoder.pkl', 'rb')\n",
    "    file2 = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "    labelEncoder = pickle.load(file1)\n",
    "    labelDecoder = pickle.load(file2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a15940aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_train.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_test.pkl\")\n",
    "checkFile3 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_dev.pkl\")\n",
    "\n",
    "if not (checkFile1 or checkFile2 or checkFile3):\n",
    "    pickle.dump(X_train[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_train.pkl', 'wb'))\n",
    "    pickle.dump(X_test[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(X_dev[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_dev.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f5abcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6458f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"embed/glove/\" + dataset_path + \"/tokenizer.pkl\")\n",
    "\n",
    "## tokenize all sentences ##\n",
    "if not checkFile:\n",
    "    all_text = list(X_train[\"Utterance\"])\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "    pickle.dump(tokenizer, open('embed/glove/' + dataset_path + '/tokenizer.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('embed/glove/' + dataset_path + '/tokenizer.pkl', 'rb')\n",
    "    tokenizer = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1caa67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the sentences into sequences ##\n",
    "train_sequence = tokenizer.texts_to_sequences(list(X_train['Utterance']))\n",
    "dev_sequence = tokenizer.texts_to_sequences(list(X_dev['Utterance']))\n",
    "test_sequence = tokenizer.texts_to_sequences(list(X_test['Utterance']))\n",
    "\n",
    "X_train['sentence_length'] = [len(item) for item in train_sequence]\n",
    "X_dev['sentence_length'] = [len(item) for item in dev_sequence]\n",
    "X_test['sentence_length'] = [len(item) for item in test_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae3a80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_tokens = 250\n",
    "\n",
    "train_sequence = pad_sequences(train_sequence, maxlen=max_num_tokens, padding='post')\n",
    "dev_sequence = pad_sequences(dev_sequence, maxlen=max_num_tokens, padding='post')\n",
    "test_sequence = pad_sequences(test_sequence, maxlen=max_num_tokens, padding='post')\n",
    "\n",
    "X_train['sequence'] = list(train_sequence)\n",
    "X_dev['sequence'] = list(dev_sequence)\n",
    "X_test['sequence'] = list(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bd8817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>[455, 1, 32, 3, 940, 443, 28, 18, 3621, 3622, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[2, 1120, 104, 45, 706, 871, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[9, 1, 76, 9, 1, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>[17, 228, 193, 5, 96, 487, 51, 45, 1971, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[18, 1971, 29, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance          Speaker  \\\n",
       "0  also I was the point person on my company’s tr...         Chandler   \n",
       "1                   You must’ve had your hands full.  The Interviewer   \n",
       "2                            That I did. That I did.         Chandler   \n",
       "3      So let’s talk a little bit about your duties.  The Interviewer   \n",
       "4                             My duties?  All right.         Chandler   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  sentence_length  \\\n",
       "0   neutral   neutral            0             0               18   \n",
       "1   neutral   neutral            0             1                6   \n",
       "2   neutral   neutral            0             2                6   \n",
       "3   neutral   neutral            0             3                9   \n",
       "4  surprise  positive            0             4                4   \n",
       "\n",
       "                                            sequence  \n",
       "0  [455, 1, 32, 3, 940, 443, 28, 18, 3621, 3622, ...  \n",
       "1  [2, 1120, 104, 45, 706, 871, 0, 0, 0, 0, 0, 0,...  \n",
       "2  [9, 1, 76, 9, 1, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "3  [17, 228, 193, 5, 96, 487, 51, 45, 1971, 0, 0,...  \n",
       "4  [18, 1971, 29, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d04fb6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also I was the point person on my company’s transition from the KL-5 to GR-6 system.\n"
     ]
    }
   ],
   "source": [
    "print(X_train['Utterance'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ea2580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe...\n",
      "Completed loading pretrained GloVe model.\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/glove/\" + dataset_path + \"/glv_embedding_matrix.pkl\")\n",
    "\n",
    "if True or not checkFile:\n",
    "    glv_vector = load_pretrained_glove()\n",
    "    word_vector_length = len(glv_vector['the'])\n",
    "    word_index = tokenizer.word_index\n",
    "    inv_word_index = {v: k for k, v in word_index.items()}\n",
    "    num_unique_words = len(word_index)\n",
    "    glv_embedding_matrix = np.zeros((num_unique_words + 1, word_vector_length))\n",
    "    pickle.dump(glv_embedding_matrix, open('embed/glove/' + dataset_path + '/glv_embedding_matrix.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('embed/glove/' + dataset_path + '/glv_embedding_matrix.pkl', 'rb')\n",
    "    glv_embedding_matrix = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac41b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Completed preprocessing.\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix')\n",
    "\n",
    "if True or not checkFile:\n",
    "    for j in range(1, num_unique_words + 1):\n",
    "        try:\n",
    "            glv_embedding_matrix[j] = glv_vector[inv_word_index[j]]\n",
    "        except KeyError:\n",
    "            glv_embedding_matrix[j] = np.random.randn(word_vector_length) / 200\n",
    "\n",
    "    np.ndarray.dump(glv_embedding_matrix, open('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix', 'wb'))\n",
    "    vocab_size = word_vector_length\n",
    "    print('Done. Completed preprocessing.')\n",
    "    \n",
    "else:\n",
    "#     file1 = open('embed/glove/pretrained_glv_embedding_matrix', 'rb')\n",
    "#     glv_embedding_matrix = pickle.load(file1)\n",
    "    glv_embedding_matrix = np.load(open('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix', 'rb'), allow_pickle=True)\n",
    "    vocab_size, embedding_dim = glv_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4bde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change D_m into\n",
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 100\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31b6cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_feat_extractor = CNNFeatureExtractor(vocab_size=vocab_size, embedding_dim=300, output_size=100, filters=50, kernel_sizes=(3, 4, 5), dropout=0.5)\n",
    "cnn_feat_extractor.init_pretrained_embeddings_from_numpy(glv_embedding_matrix)\n",
    "lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c6c03",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for train set¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07e5e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4459dbb5",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9644\\2247667134.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "\n",
    "if True or not checkFile:\n",
    "    for range_pair in rangesTrain:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_train['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTrain.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTrain, rangesTrain], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\")\n",
    "    encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf00b0",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2df3e0b0",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9644\\970745468.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "\n",
    "if True or not checkFile:\n",
    "    for range_pair in rangesTest:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_test['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTest.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTest, rangesTest], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_test.pkl', \"rb\")\n",
    "    encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e2384",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d4eae62",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\")\n",
    "encodedSpeakersDev = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesDev:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_dev['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersDev.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersDev, rangesDev], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_dev.pkl', \"rb\")\n",
    "    encodedSpeakersDev, rangesDev = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1827fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 200\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "n_speakers=2\n",
    "max_seq_len=110\n",
    "window_past=0\n",
    "window_future=5\n",
    "# vocab_size=vocab_size\n",
    "n_classes=7\n",
    "listener_state=False\n",
    "context_attention='general'\n",
    "dropout=0.5\n",
    "nodal_attention=False\n",
    "no_cuda=True\n",
    "n_relations = 2 * n_speakers ** 2\n",
    "att_model = MaskedEdgeAttention(D_e, max_seq_len, no_cuda)\n",
    "nodal_attention=True\n",
    "edge_type_mapping = {}\n",
    "for j in range(n_speakers):\n",
    "    for k in range(n_speakers):\n",
    "        edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "        edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f487a",
   "metadata": {},
   "source": [
    "<h4> Getting data required for graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8920b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OverrideFileChecks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcbad5c0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, X_set, rangesSet, encodedSpeakersSet):\n",
    "        self.X_set = X_set\n",
    "        self.rangesSet = rangesSet\n",
    "        self.encodedSpeakersSet = encodedSpeakersSet\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rangesSet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        startIdx, endIdx = self.rangesSet[idx]\n",
    "        sequence = self.X_set[\"sequence\"][startIdx:endIdx+1].tolist()\n",
    "        qmask = self.encodedSpeakersSet[startIdx: endIdx+1]\n",
    "        return torch.FloatTensor(sequence), qmask\n",
    "\n",
    "# Define the ContextEncoding function\n",
    "def ContextEncoding(file_path, dataset):\n",
    "    all_emotions, all_umask, all_seq_lengths = [], [], []\n",
    "    all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths = [], [], [], [], []\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for textf, qmask in tqdm(dataloader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        textf = textf.squeeze(0)  # Remove batch dimension (1, utterance_size, embedding_size) -> (utterance_size, embedding_size)\n",
    "        umask = torch.FloatTensor([[1] * textf.size(0)])  # Adjust to (1, utterance_size)\n",
    "        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "        # print(umask.shape, lengths)\n",
    "        U = cnn_feat_extractor(textf, umask)\n",
    "        emotions, hidden = lstm(U)\n",
    "        all_emotions.append(emotions)\n",
    "        \n",
    "        features, edge_index, \\\n",
    "        edge_norm, edge_type, \\\n",
    "        edge_index_lengths = batch_graphify(emotions, \n",
    "                                            qmask,\n",
    "                                            lengths,\n",
    "                                            window_past,\n",
    "                                            window_future,\n",
    "                                            edge_type_mapping,\n",
    "                                            att_model, \n",
    "                                            no_cuda)\n",
    "        all_umask.append(umask)\n",
    "        all_seq_lengths.append(lengths)\n",
    "        all_features.append(features)\n",
    "        all_edge_index.append(edge_index)\n",
    "        all_edge_norm.append(edge_norm)\n",
    "        all_edge_type.append(edge_type)\n",
    "        all_edge_index_lengths.append(edge_index_lengths)\n",
    "\n",
    "#     all_emotions = torch.cat(all_emotions, dim=0)  # (total_num_utterances, lstm_hidden_size)\n",
    "    \n",
    "    with open(file_path[0], 'wb') as file:\n",
    "        pickle.dump(all_emotions, file)\n",
    "        \n",
    "    with open(file_path[1], 'wb') as file:\n",
    "        pickle.dump([   all_umask, \\\n",
    "                        all_seq_lengths,\n",
    "                        all_features, \\\n",
    "                        all_edge_index, \\\n",
    "                        all_edge_norm, \\\n",
    "                        all_edge_type, \\\n",
    "                        all_edge_index_lengths], file)\n",
    "    \n",
    "    return all_emotions, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths\n",
    "\n",
    "# File paths\n",
    "file_path1 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_train.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl']\n",
    "file_path2 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_test.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl']\n",
    "file_path3 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_dev.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl']\n",
    "\n",
    "# Check if files exist\n",
    "checkFile1 = os.path.isfile(file_path1[0])\n",
    "checkFile2 = os.path.isfile(file_path2[0])\n",
    "checkFile3 = os.path.isfile(file_path3[0])\n",
    "\n",
    "if not checkFile1:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "\n",
    "    trainDataset = ContextDataset(X_train, rangesTrain, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trainContext, \\\n",
    "     all_edge_index, \\\n",
    "     all_edge_norm, \\\n",
    "     all_edge_type, \\\n",
    "     all_edge_index_lengths = ContextEncoding(file_path1, trainDataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train text\")\n",
    "\n",
    "if not checkFile2:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTest for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    testDataset = ContextDataset(X_test, rangesTest, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    testContext, _, _, _, _ = ContextEncoding(file_path2, testDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "    \n",
    "if not checkFile3:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersDev for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    devDataset = ContextDataset(X_dev, rangesDev, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    devContext, _, _, _, _ = ContextEncoding(file_path3, devDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3:\n",
    "    with open(file_path1[0], 'rb') as file1:\n",
    "        trainContext = pickle.load(file1)\n",
    "    with open(file_path2[0], 'rb') as file2:\n",
    "        testContext = pickle.load(file2)\n",
    "    with open(file_path3[0], 'rb') as file3:\n",
    "        devContext = pickle.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871aaea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 1, 200])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainContext[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c07b8f",
   "metadata": {},
   "source": [
    "<h4> Visualize utterance embeddnig (u') with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe10df08",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     # Assuming trainContext, X_train, and labelDecoder are already defined\n",
    "#     num_instance = len(X_train[\"Emotion\"])\n",
    "\n",
    "#     # Calculate the counts for each unique label\n",
    "#     unique_labels, label_counts = np.unique(list(X_train[\"Emotion\"][:num_instance]), return_counts=True)\n",
    "\n",
    "#     # Print the counts for each unique label\n",
    "#     for label, count in zip(unique_labels, label_counts):\n",
    "#         print(f\"{labelDecoder[label]}: {count} occurrences\")\n",
    "\n",
    "#     encodedFeaturesFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "#     trainContext = trainContext.squeeze(1)\n",
    "\n",
    "#     # Convert the tensor to a numpy array for use with sklearn\n",
    "#     trainContext_np = trainContext.detach().numpy()\n",
    "\n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     trainContext_pca = pca.fit_transform(trainContext_np)\n",
    "\n",
    "#     # Perform t-SNE\n",
    "#     tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "#     trainContext_tsne = tsne.fit_transform(trainContext_np)\n",
    "\n",
    "#     # Plot PCA results with color-coded labels  \n",
    "#     plt.figure(figsize=(14, 7))\n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(trainContext_pca[indices, 0], trainContext_pca[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('PCA of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     # Plot t-SNE results with color-coded labels\n",
    "#     plt.subplot(1, 2, 2, position=[0.65, 0.1, 0.35, 0.8])\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(trainContext_tsne[indices, 0], trainContext_tsne[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('t-SNE of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('t-SNE Component 1')\n",
    "#     plt.ylabel('t-SNE Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78804eeb",
   "metadata": {},
   "source": [
    "<h4>Visualize node features (pre-h') with T-SNE and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c3f3a2d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     # Assuming trainContext, X_train, and labelDecoder are already defined\n",
    "#     num_instance = len(X_train[\"Emotion\"])\n",
    "\n",
    "#     # Calculate the counts for each unique label\n",
    "#     unique_labels, label_counts = np.unique(list(X_train[\"Emotion\"][:num_instance]), return_counts=True)\n",
    "\n",
    "#     # Print the counts for each unique label\n",
    "#     for label, count in zip(unique_labels, label_counts):\n",
    "#         print(f\"{labelDecoder[label]}: {count} occurrences\")\n",
    "\n",
    "#     flattened_features = torch.cat(all_features, dim=0)\n",
    "\n",
    "#     # Convert the tensor to a numpy array for use with sklearn\n",
    "#     flattened_features_np = flattened_features.detach().numpy()\n",
    "\n",
    "#     # Perform PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     flattened_features_np_pca = pca.fit_transform(flattened_features_np)\n",
    "\n",
    "#     # Perform t-SNE\n",
    "#     tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "#     flattened_features_tsne = tsne.fit_transform(flattened_features_np)\n",
    "\n",
    "#     # Plot PCA results with color-coded labels\n",
    "#     plt.figure(figsize=(14, 7))\n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(flattened_features_np_pca[indices, 0], flattened_features_np_pca[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('PCA of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     # Plot t-SNE results with color-coded labels\n",
    "#     plt.subplot(1, 2, 2, position=[0.65, 0.1, 0.35, 0.8])\n",
    "#     for label in unique_labels:\n",
    "#         indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "#         plt.scatter(flattened_features_tsne[indices, 0], flattened_features_tsne[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "#     plt.title('t-SNE of trainContext with Color-Coded Labels')\n",
    "#     plt.xlabel('t-SNE Component 1')\n",
    "#     plt.ylabel('t-SNE Component 2')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61099f3e",
   "metadata": {},
   "source": [
    "#### Objective measure on observing howuseful the data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43c18404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be546014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     neutral\n",
       "1     neutral\n",
       "2     neutral\n",
       "3     neutral\n",
       "4    surprise\n",
       "5     neutral\n",
       "6     neutral\n",
       "7     neutral\n",
       "8     neutral\n",
       "9     neutral\n",
       "Name: Emotion, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4a564d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n",
    "\n",
    "flat_trainContext = flatten_extend(trainContext)\n",
    "stacked_trainContext = torch.stack(flat_trainContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38c4adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNp = stacked_trainContext.squeeze(1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acade541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.25304386019706726\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=len(set(y_train[\"Emotion\"])))\n",
    "clusters = kmeans.fit_predict(trainNp)\n",
    "\n",
    "score = silhouette_score(trainNp, clusters)\n",
    "print(f\"Silhouette Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ef4b2",
   "metadata": {},
   "source": [
    "- Silhouette Score > 0.5: Indicates a good clustering with well-separated clusters.\n",
    "- Silhouette Score between 0 and 0.5: Indicates overlapping clusters to some degree.\n",
    "- Silhouette Score < 0: Indicates that samples might have been assigned to the wrong clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b66d8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbcf9d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabasz Index: 3835.5476772002985\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=len(set(y_train[\"Emotion\"])))\n",
    "clusters = kmeans.fit_predict(trainNp)\n",
    "\n",
    "score = calinski_harabasz_score(trainNp, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b9966",
   "metadata": {},
   "source": [
    "Higher values: Indicate better-defined clusters. There is no strict threshold for \"good\" scores, as it depends on the specific dataset and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56c959ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9885870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473: ConvergenceWarning: Number of distinct clusters (12832) found smaller than n_clusters (12840). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin Index: 2.2036955879278816e-07\n"
     ]
    }
   ],
   "source": [
    "# Assuming `embeddings` is your BERT/CNN output and `labels` are your true labels\n",
    "kmeans = KMeans(n_clusters=len(y_train[\"Emotion\"]))\n",
    "clusters = kmeans.fit_predict(trainNp)\n",
    "\n",
    "score = davies_bouldin_score(trainNp, clusters)\n",
    "print(f\"Davies-Bouldin Index: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb6c34",
   "metadata": {},
   "source": [
    "- Lower values: Indicate better clustering with well-separated clusters.\n",
    "- Higher values: Indicate poor clustering with overlapping clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
