{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from graph_context_dataset import FeatureEngineeredDataset\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import random\n",
    "from model import FCClassifier, DATASET_PATH\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b22dd",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "\n",
    " - dataset_original\n",
    " - dataset_drop_noise\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1470c3",
   "metadata": {},
   "source": [
    "<h3> Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a68406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4fb045",
   "metadata": {
    "code_folding": [
     0,
     9,
     18,
     21,
     24,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# class FCLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(FCLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# class ActivationLayer(nn.Module):\n",
    "#     def __init__(self, activation_fn):\n",
    "#         super(ActivationLayer, self).__init__()\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.activation_fn(x)\n",
    "#         return x\n",
    "\n",
    "# def tanh(x):\n",
    "#     return torch.tanh(x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return torch.sigmoid(x)\n",
    "# # loss function and its derivative\n",
    "# def mse(y_true, y_pred):\n",
    "#     return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "# def mse_prime(y_true, y_pred):\n",
    "#     return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246bf76e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oversample_data(X_train, Y_train, num_classes):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            # Calculate the number of instances to oversample for this class\n",
    "            num_to_oversample = max_class_count - len(indices)\n",
    "            # Randomly select instances with replacement to oversample\n",
    "            oversampled_indices = np.random.choice(indices, size=num_to_oversample, replace=True)\n",
    "            # Append the oversampled instances to the original data\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return torch.tensor(X_train), torch.tensor(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8349606b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    if not tensor_list:\n",
    "        raise ValueError(\"The tensor list is empty\")\n",
    "\n",
    "    feature_dim = tensor_list[0].shape[1]\n",
    "    for tensor in tensor_list:\n",
    "        if tensor.shape[1] != feature_dim:\n",
    "            raise ValueError(\"All tensors must have the same feature dimension\")\n",
    "    \n",
    "    concatenated_tensor = torch.cat(tensor_list, dim=0)\n",
    "    \n",
    "    return concatenated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503aae",
   "metadata": {},
   "source": [
    "<h4> Import labels and label decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/labels_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_dev.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_val = pickle.load(file)\n",
    "y_val = torch.tensor(y_val)\n",
    "    \n",
    "file_path = 'data/dump/' + dataset_path + '/label_decoder.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3e716",
   "metadata": {},
   "source": [
    "<h4> Import the BERT base-node outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c53ba",
   "metadata": {},
   "source": [
    "first we disregard the u' and directly train the h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396cc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_train.pkl\",\n",
    "]\n",
    "\n",
    "test_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_test.pkl\",\n",
    "]\n",
    "\n",
    "val_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_dev.pkl\",\n",
    "]\n",
    "\n",
    "dictKey = {\n",
    "    0 : 'bert',\n",
    "    1 : 'bert-select-few',\n",
    "    2 : 'bert-select-mod',\n",
    "    3 : 'bert-select-more',\n",
    "    4 : 'dgcn',\n",
    "    5 : 'dgcn-select',\n",
    "    6 : 'gatv1',\n",
    "    7 : 'gatv1-select',\n",
    "    8 : 'gatv1-edge',\n",
    "    9 : 'gatv1-edge-select',\n",
    "    10 : 'gatv2-edge',\n",
    "    11 : 'gatv2-edge-select',\n",
    "    12 : 'rgat',\n",
    "    13 : 'rgat-select',\n",
    "    14 : 'egat',\n",
    "    15 : 'egat-select',\n",
    "    16 : 'bert-select-mod-dgcn',\n",
    "    17 : 'bert-select-mod-gatv1',\n",
    "    18 : 'bert-select-mod-gatv1-edge',\n",
    "    19 : 'bert-select-mod-gatv2-edge',\n",
    "    20 : 'bert-select-mod-rgat',\n",
    "    21 : 'bert-select-mod-egat',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff1721",
   "metadata": {},
   "source": [
    "<h4> Getting BERT and GAT outputs for all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0c4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeaturesList = []\n",
    "testFeaturesList = []\n",
    "valFeaturesList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3426c4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    return torch.cat(tensor_list, dim=0)\n",
    "\n",
    "for file_path in train_file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "#         print(type(data))\n",
    "#         if isinstance(data, list):\n",
    "#             print(\"instance of list, \", data[0].shape)\n",
    "#         else:\n",
    "#             print(\"instance of tensor, \", data.shape)\n",
    "        if file_path != train_file_paths[-1]: \n",
    "            trainFeaturesList.append(concatenate_tensors(data))\n",
    "        else:\n",
    "            trainFeaturesList.append(data)\n",
    "            \n",
    "for file_path in test_file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        if file_path != test_file_paths[-1]: \n",
    "            testFeaturesList.append(concatenate_tensors(data))\n",
    "        else:\n",
    "            testFeaturesList.append(data)\n",
    "            \n",
    "for file_path in val_file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        if file_path != val_file_paths[-1]: \n",
    "            valFeaturesList.append(concatenate_tensors(data))\n",
    "        else:\n",
    "            valFeaturesList.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e174164",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "479a3b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a070ce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43fa315",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "<h3> Feature Selection and creating data combination for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Define select feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6501b577",
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def get_norm_features(encoded_features):\n",
    "    scaler = MinMaxScaler()\n",
    "#       \"FeatureSelected+BERT+GAT: \", concatenatedRepresentationTrain2.shape, \"\\n\",\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "    return torch.tensor(features_scaled)\n",
    "\n",
    "def get_selected_features(encoded_features, labels, top_n):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=100)\n",
    "\n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        # Create a binary mask indicating instances belonging to the current class\n",
    "        mask = (labels == label)\n",
    "\n",
    "        # SelectKBest with chi2 as the scoring function\n",
    "        selector = SelectKBest(score_func=chi2, k=top_n)  # Select top 20 features\n",
    "        selector.fit(features_scaled, mask)  # Fit SelectKBest to the data\n",
    "        # Get the indices of the top 20 features\n",
    "        top_features_indices = np.argsort(selector.scores_)[-top_n:]\n",
    "        scores = selector.scores_[top_features_indices]\n",
    "        # Store the indices in the dictionary\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "    # Select the desired features\n",
    "    selected_features = encoded_features[:, concatenated_features_indices]\n",
    "#     print(selected_features.shape)\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d79a94",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efccc17d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca73f7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Now prepare the data that will be ued to train the classifier, there are 20 combinations. And pick top 7 combinations yielding top F1 weighted-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61905fb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "trainList = []\n",
    "testList = []\n",
    "valList = []\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/trainList.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/testList.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/valList.pkl\"\n",
    "\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3: \n",
    "    with open(file_path1, \"rb\") as file:\n",
    "        trainList = pickle.load(file)\n",
    "    with open(file_path2, \"rb\") as file:\n",
    "        testList = pickle.load(file)\n",
    "    with open(file_path3, \"rb\") as file:\n",
    "        valList = pickle.load(file)\n",
    "else:\n",
    "    trainFeaturesList.append(data)\n",
    "    #1\n",
    "    trainList.append(trainFeaturesList[0])\n",
    "    testList.append(testFeaturesList[0])\n",
    "    valList.append(valFeaturesList[0])\n",
    "    #2\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 16)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #3\n",
    "    selectedTrainFeatures1b, indicesFeatures1b = get_selected_features(trainFeaturesList[0], y_train, 32)\n",
    "    selectedTestFeatures1b = testFeaturesList[0][:, indicesFeatures1b]\n",
    "    selectedValFeatures1b = valFeaturesList[0][:, indicesFeatures1b]\n",
    "    trainList.append(selectedTrainFeatures1b)\n",
    "    testList.append(selectedTestFeatures1b)\n",
    "    valList.append(selectedValFeatures1b)\n",
    "    #4\n",
    "    selectedTrainFeatures1c, indicesFeatures1c = get_selected_features(trainFeaturesList[0], y_train, 64)\n",
    "    selectedTestFeatures1c = testFeaturesList[0][:, indicesFeatures1c]\n",
    "    selectedValFeatures1c = valFeaturesList[0][:, indicesFeatures1c]\n",
    "    trainList.append(selectedTrainFeatures1c)\n",
    "    testList.append(selectedTestFeatures1c)\n",
    "    valList.append(selectedValFeatures1c)\n",
    "    #5\n",
    "    trainList.append(trainFeaturesList[1])\n",
    "    testList.append(testFeaturesList[1])\n",
    "    valList.append(valFeaturesList[1])\n",
    "    #6\n",
    "    selectedTrainFeatures2, indicesFeatures2 = get_selected_features(trainFeaturesList[1], y_train, 12)\n",
    "    selectedTestFeatures2 = testFeaturesList[1][:, indicesFeatures2]\n",
    "    selectedValFeatures2 = valFeaturesList[1][:, indicesFeatures2]\n",
    "    trainList.append(selectedTrainFeatures2)\n",
    "    testList.append(selectedTestFeatures2)\n",
    "    valList.append(selectedValFeatures2)\n",
    "    #7\n",
    "    trainList.append(trainFeaturesList[2])\n",
    "    testList.append(testFeaturesList[2])\n",
    "    valList.append(valFeaturesList[2])\n",
    "    #8\n",
    "    selectedTrainFeatures3, indicesFeatures3 = get_selected_features(trainFeaturesList[2], y_train, 12)\n",
    "    selectedTestFeatures3 = testFeaturesList[2][:, indicesFeatures3]\n",
    "    selectedValFeatures3 = valFeaturesList[2][:, indicesFeatures3]\n",
    "    trainList.append(selectedTrainFeatures3)\n",
    "    testList.append(selectedTestFeatures3)\n",
    "    valList.append(selectedValFeatures3)\n",
    "    #9\n",
    "    trainList.append(trainFeaturesList[3])\n",
    "    testList.append(testFeaturesList[3])\n",
    "    valList.append(valFeaturesList[3])\n",
    "    #10\n",
    "    selectedTrainFeatures4, indicesFeatures4 = get_selected_features(trainFeaturesList[3], y_train, 12)\n",
    "    selectedTestFeatures4 = testFeaturesList[3][:, indicesFeatures4]\n",
    "    selectedValFeatures4 = valFeaturesList[3][:, indicesFeatures4]\n",
    "    trainList.append(selectedTrainFeatures4)\n",
    "    testList.append(selectedTestFeatures4)\n",
    "    valList.append(selectedValFeatures4)\n",
    "    #11\n",
    "    trainList.append(trainFeaturesList[4])\n",
    "    testList.append(testFeaturesList[4])\n",
    "    valList.append(valFeaturesList[4])\n",
    "    #12\n",
    "    selectedTrainFeatures5, indicesFeatures5 = get_selected_features(trainFeaturesList[4], y_train, 12)\n",
    "    selectedTestFeatures5 = testFeaturesList[4][:, indicesFeatures5]\n",
    "    selectedValFeatures5 = valFeaturesList[4][:, indicesFeatures5]\n",
    "    trainList.append(selectedTrainFeatures5)\n",
    "    testList.append(selectedTestFeatures5)\n",
    "    valList.append(selectedValFeatures5)\n",
    "    #13\n",
    "    trainList.append(trainFeaturesList[5])\n",
    "    testList.append(testFeaturesList[5])\n",
    "    valList.append(valFeaturesList[5])\n",
    "    #14\n",
    "    selectedTrainFeatures6, indicesFeatures6 = get_selected_features(trainFeaturesList[5], y_train, 12)\n",
    "    selectedTestFeatures6 = testFeaturesList[5][:, indicesFeatures6]\n",
    "    selectedValFeatures6 = valFeaturesList[5][:, indicesFeatures6]\n",
    "    trainList.append(selectedTrainFeatures6)\n",
    "    testList.append(selectedTestFeatures6)\n",
    "    valList.append(selectedValFeatures6)\n",
    "    #15\n",
    "    trainList.append(trainFeaturesList[6])\n",
    "    testList.append(testFeaturesList[6])\n",
    "    valList.append(valFeaturesList[6])\n",
    "    #16\n",
    "    selectedTrainFeatures7, indicesFeatures7 = get_selected_features(trainFeaturesList[6][0], y_train, 12)\n",
    "    selectedTestFeatures7 = testFeaturesList[6][0][:, indicesFeatures7]\n",
    "    selectedValFeatures7 = testFeaturesList[6][0][:, indicesFeatures7]\n",
    "    trainList.append(selectedTrainFeatures7)\n",
    "    testList.append(selectedTestFeatures7)\n",
    "    valList.append(selectedValFeatures7)\n",
    "    selectedNormTrainFeatures1 = get_norm_features(selectedTrainFeatures1b)\n",
    "    selectedNormTestFeatures1 = get_norm_features(selectedTestFeatures1b)\n",
    "    selectedNormValFeatures1 = get_norm_features(selectedValFeatures1b)\n",
    "\n",
    "    #17\n",
    "    trainNormFeatures2 = get_norm_features(trainFeaturesList[1].detach().numpy())\n",
    "    testNormFeatures2 = get_norm_features(testFeaturesList[1].detach().numpy())\n",
    "    valNormFeatures2 = get_norm_features(valFeaturesList[1].detach().numpy())\n",
    "    concatenatedTrainFeatures2 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures2), dim=1)\n",
    "    concatenatedTestFeatures2 = torch.cat((selectedNormTestFeatures1, testNormFeatures2), dim=1)\n",
    "    concatenatedValFeatures2 = torch.cat((selectedNormValFeatures1, valNormFeatures2), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures2)\n",
    "    testList.append(concatenatedTestFeatures2)\n",
    "    valList.append(concatenatedValFeatures2)\n",
    "    #18\n",
    "    trainNormFeatures3 = get_norm_features(trainFeaturesList[2].detach().numpy())\n",
    "    testNormFeatures3 = get_norm_features(testFeaturesList[2].detach().numpy())\n",
    "    valNormFeatures3 = get_norm_features(valFeaturesList[2].detach().numpy())\n",
    "    concatenatedTrainFeatures3 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures3), dim=1)\n",
    "    concatenatedTestFeatures3 = torch.cat((selectedNormTestFeatures1, testNormFeatures3), dim=1)\n",
    "    concatenatedValFeatures3 = torch.cat((selectedNormValFeatures1, valNormFeatures3), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures3)\n",
    "    testList.append(concatenatedTestFeatures3)\n",
    "    valList.append(concatenatedValFeatures3)\n",
    "    #19\n",
    "    trainNormFeatures4 = get_norm_features(trainFeaturesList[3].detach().numpy())\n",
    "    testNormFeatures4 = get_norm_features(testFeaturesList[3].detach().numpy())\n",
    "    valNormFeatures4 = get_norm_features(valFeaturesList[3].detach().numpy())\n",
    "    concatenatedTrainFeatures4 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures4), dim=1)\n",
    "    concatenatedTestFeatures4 = torch.cat((selectedNormTestFeatures1, testNormFeatures4), dim=1)\n",
    "    concatenatedValFeatures4 = torch.cat((selectedNormValFeatures1, valNormFeatures4), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures4)\n",
    "    testList.append(concatenatedTestFeatures4)\n",
    "    valList.append(concatenatedValFeatures4)\n",
    "    #20\n",
    "    trainNormFeatures5 = get_norm_features(trainFeaturesList[4].detach().numpy())\n",
    "    testNormFeatures5 = get_norm_features(testFeaturesList[4].detach().numpy())\n",
    "    valNormFeatures5 = get_norm_features(valFeaturesList[4].detach().numpy())\n",
    "    concatenatedTrainFeatures5 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures5), dim=1)\n",
    "    concatenatedTestFeatures5 = torch.cat((selectedNormTestFeatures1, testNormFeatures5), dim=1)\n",
    "    concatenatedValFeatures5 = torch.cat((selectedNormValFeatures1, valNormFeatures5), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures5)\n",
    "    testList.append(concatenatedTestFeatures5)\n",
    "    valList.append(concatenatedValFeatures5)\n",
    "\n",
    "    #21\n",
    "    trainNormFeatures6 = get_norm_features(trainFeaturesList[5].detach().numpy())\n",
    "    testNormFeatures6 = get_norm_features(testFeaturesList[5].detach().numpy())\n",
    "    valNormFeatures6 = get_norm_features(valFeaturesList[5].detach().numpy())\n",
    "    concatenatedTrainFeatures6 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures6), dim=1)\n",
    "    concatenatedTestFeatures6 = torch.cat((selectedNormTestFeatures1, testNormFeatures6), dim=1)\n",
    "    concatenatedValFeatures6 = torch.cat((selectedNormValFeatures1, valNormFeatures6), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures6)\n",
    "    testList.append(concatenatedTestFeatures6)\n",
    "    valList.append(concatenatedValFeatures6)\n",
    "\n",
    "    #22\n",
    "    trainNormFeatures7 = get_norm_features(trainFeaturesList[6][0].detach().numpy())\n",
    "    testNormFeatures7 = get_norm_features(testFeaturesList[6][0].detach().numpy())\n",
    "    valNormFeatures7 = get_norm_features(valFeaturesList[6][0].detach().numpy())\n",
    "    concatenatedTrainFeatures7 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures7), dim=1)\n",
    "    concatenatedTestFeatures7 = torch.cat((selectedNormTestFeatures1, testNormFeatures7), dim=1)\n",
    "    concatenatedValFeatures7 = torch.cat((selectedNormValFeatures1, valNormFeatures7), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures7)\n",
    "    testList.append(concatenatedTestFeatures7)\n",
    "    valList.append(concatenatedValFeatures7)\n",
    "\n",
    "    with open(file_path1, 'wb') as file:\n",
    "        pickle.dump(trainList, file)\n",
    "    with open(file_path2, 'wb') as file:\n",
    "        pickle.dump(testList, file)\n",
    "    with open(file_path3, 'wb') as file:\n",
    "        pickle.dump(valList, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb893e7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def prep_data(features, labels, isOversample):\n",
    "    num_instances = len(features)\n",
    "    num_classes = 7\n",
    "\n",
    "    # Rescale input features\n",
    "    # selected_features = concatenated_representation / np.linalg.norm(concatenated_representation, axis=1, keepdims=True)\n",
    "\n",
    "    # Apply data resampling (oversampling) to balance class distribution\n",
    "    if isOversample:\n",
    "        X_set, Y_set = oversample_data(features, labels, num_classes)\n",
    "    else:\n",
    "        X_set, Y_set = features, labels\n",
    "\n",
    "    # Calculate class weights for class weighting\n",
    "#     class_counts = np.bincount(labels)\n",
    "#     total_instances = np.sum(class_counts)\n",
    "    # class_weights = torch.tensor([total_instances / (num_classes * count) for count in class_counts], dtype=torch.float32)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
    "    Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
    "    # print(X_train_tensor.shape, Y_train_tensor.shape)\n",
    "    # X_train_tensor = torch.tensor(selected_features)\n",
    "    # Y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(Y_set, return_counts=True)\n",
    "\n",
    "    # Print the counts for each unique label\n",
    "#     for label, count in zip(unique_labels, label_counts):\n",
    "#         print(f\"Label {label_decoder[label]}: {count} occurrences\")\n",
    "\n",
    "#     print(X_tensor.shape, Y_tensor.shape)\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a71c886",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train1(X_set, Y_set, num_epochs=20, batch_size=32, loss_difference_threshold=0.01, \n",
    "                 hidden_dims=[256, 128], dropout_rate=0.5, lr=0.0001, optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss):\n",
    "    output_dim = 7  # Number of classes\n",
    "    model = MyNetwork(len(X_set[0]), hidden_dims, output_dim, dropout_rate)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    print_interval = 1  # Print tqdm every epoch\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_set, Y_set)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    epoch_num = num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.float()  # Ensure inputs are float32\n",
    "                labels = labels.long()   # Ensure labels are long\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "        if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "            epoch_num = epoch\n",
    "            break\n",
    "\n",
    "        previous_loss = epoch_loss\n",
    "\n",
    "    return model, epoch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "635c4f57",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def model_train2(X_set, y_set, num_epochs=20, batch_size=32, loss_difference_threshold=0.01,\n",
    "                 hidden_dim=128, dropout_prob=0.5, learning_rate=0.0005, optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss):\n",
    "    input_dim = len(X_set[0])  # Size of the input features\n",
    "    output_dim = 7  # Number of classes\n",
    "\n",
    "    model = FCClassifier(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    dataset = TensorDataset(X_set, y_set)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    epoch_losses = []  # List to store loss values for each epoch\n",
    "    epoch_num = num_epochs\n",
    "    # Training loop\n",
    "    with tqdm(total=num_epochs, unit=\"epoch\", desc=\"Training\") as tepoch:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_features, batch_labels in dataloader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update running loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Calculate and store average loss for the epoch\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_losses.append(epoch_loss)\n",
    "\n",
    "            # Update tqdm description\n",
    "            tepoch.set_postfix(loss=epoch_loss)\n",
    "            tepoch.update()\n",
    "\n",
    "            # Check for early stopping\n",
    "            if epoch > 0 and abs(epoch_losses[-2] - epoch_losses[-1]) < loss_difference_threshold:\n",
    "                epoch_num = epoch\n",
    "                break\n",
    "\n",
    "    return model, epoch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b435c98b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_emotions(model, X_tensor, Y_tensor, typeSet, isSimpleFC, i_dict):\n",
    "    # Set the model to evaluation mode\n",
    "    if X_tensor.dtype != torch.float32:\n",
    "        X_tensor = X_tensor.float()\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    # Predict on the data\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convert predicted tensor to numpy array\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    Y_tensor = Y_tensor.cpu().numpy()\n",
    "\n",
    "    # Calculate classification report\n",
    "    report = classification_report(Y_tensor, predicted, target_names=label_decoder.values(), output_dict=True, zero_division=0)\n",
    "\n",
    "    # Extract metrics\n",
    "    accuracy = report['accuracy']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    f1_micro = report.get('micro avg', {}).get('f1-score', accuracy)\n",
    "    f1_macro = report.get('macro avg', {}).get('f1-score', 0.0) \n",
    "    \n",
    "    if typeSet == \"validation\":\n",
    "        print(\"Classified: \", dictKey[i_dict])\n",
    "    \n",
    "    return dictKey[i_dict], typeSet, isSimpleFC, accuracy, recall, weighted_f1, f1_micro, f1_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52d8752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b35df7e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|███████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 114.02batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 768])\n",
      "1 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 87])\n",
      "2 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 159])\n",
      "3 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 287])\n",
      "4 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 64])\n",
      "5 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 30])\n",
      "6 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 64])\n",
      "7 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 48])\n",
      "8 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 64])\n",
      "9 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 47])\n",
      "10 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 64])\n",
      "11 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 45])\n",
      "12 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 64])\n",
      "13 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 42])\n",
      "14 <class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 12840, 64])\n",
      "15 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 40])\n",
      "16 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 223])\n",
      "17 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 223])\n",
      "18 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 223])\n",
      "19 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 223])\n",
      "20 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 223])\n",
      "21 <class 'torch.Tensor'>\n",
      "torch.Size([12840, 223])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for trainSet, testSet, valSet in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "    print(i, type(trainSet))\n",
    "    if isinstance(trainSet, list):\n",
    "        print(type(trainSet[0]))\n",
    "        sample = trainSet[0]\n",
    "        print(sample.shape)\n",
    "    else:\n",
    "        print(trainSet.squeeze(0).shape)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e99ea",
   "metadata": {},
   "source": [
    "<b> This is where value for isSimpleFC is decided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61a58505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return torch.tensor(data)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c6a93e4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        df_results_sorted = pickle.load(file)\n",
    "else:\n",
    "    results = []\n",
    "    num_epochs = 50\n",
    "    batch_size = 8\n",
    "    i = 0\n",
    "\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(trainSet.clone().detach(), y_train, False)\n",
    "#         deepFC\n",
    "        model, _ = model_train1(X_tensor, Y_tensor, num_epochs, batch_size)\n",
    "        result = classify_emotions(model, X_tensor.clone().detach(), Y_tensor.clone().detach(), 'train', False, i)\n",
    "#         results.append(result)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(testSet.clone().detach(), y_test, False)\n",
    "        result = classify_emotions(model, X_tensor.clone().detach(), Y_tensor.clone().detach(), 'test', False, i)\n",
    "        results.append(result)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(trainSet, y_train, False)\n",
    "#         simpleFC\n",
    "        model, _ = model_train2(X_tensor, Y_tensor, num_epochs, batch_size)\n",
    "\n",
    "        result = classify_emotions(model, X_tensor, Y_tensor, 'train', True, i)\n",
    "#         results.append(result)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(testSet, y_test, False)\n",
    "        result = classify_emotions(model, X_tensor, Y_tensor, 'test', True, i)\n",
    "        results.append(result)\n",
    "        i += 1\n",
    "\n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "416f7968",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# tmp\n",
    "# columns = list(df_results_sorted.columns)\n",
    "\n",
    "# # Modify the 2nd and 3rd column names\n",
    "# columns[1] = 'typeSet'\n",
    "# columns[2] = 'isSimpleFC'\n",
    "\n",
    "# # Assign the new column names back to the DataFrame\n",
    "# df_results_sorted.columns = columns\n",
    "# df_results_sorted[\"typeSet\"] = \"test\"\n",
    "# with open(file_path, 'wb') as file:\n",
    "#     pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed1d0610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.587070</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.356340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.616471</td>\n",
       "      <td>0.616471</td>\n",
       "      <td>0.583831</td>\n",
       "      <td>0.616471</td>\n",
       "      <td>0.353195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.622059</td>\n",
       "      <td>0.622059</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>0.622059</td>\n",
       "      <td>0.349253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.574092</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.357409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>0.327099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.550271</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.314424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>0.549285</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>0.311457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bert-select-mod-dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.594706</td>\n",
       "      <td>0.594706</td>\n",
       "      <td>0.544972</td>\n",
       "      <td>0.594706</td>\n",
       "      <td>0.307191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.544878</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bert-select-mod-gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.593824</td>\n",
       "      <td>0.593824</td>\n",
       "      <td>0.540591</td>\n",
       "      <td>0.593824</td>\n",
       "      <td>0.302503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.598235</td>\n",
       "      <td>0.598235</td>\n",
       "      <td>0.540504</td>\n",
       "      <td>0.598235</td>\n",
       "      <td>0.301486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bert-select-mod-rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.597647</td>\n",
       "      <td>0.597647</td>\n",
       "      <td>0.537364</td>\n",
       "      <td>0.597647</td>\n",
       "      <td>0.296983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bert-select-mod-rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.588824</td>\n",
       "      <td>0.588824</td>\n",
       "      <td>0.529595</td>\n",
       "      <td>0.588824</td>\n",
       "      <td>0.289742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>bert-select-mod-gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.579706</td>\n",
       "      <td>0.579706</td>\n",
       "      <td>0.524664</td>\n",
       "      <td>0.579706</td>\n",
       "      <td>0.291083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bert-select-mod-gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.575588</td>\n",
       "      <td>0.575588</td>\n",
       "      <td>0.522766</td>\n",
       "      <td>0.575588</td>\n",
       "      <td>0.286713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert-select-mod-gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.575588</td>\n",
       "      <td>0.575588</td>\n",
       "      <td>0.507465</td>\n",
       "      <td>0.575588</td>\n",
       "      <td>0.269896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.503588</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.263736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert-select-mod-gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.563529</td>\n",
       "      <td>0.563529</td>\n",
       "      <td>0.489185</td>\n",
       "      <td>0.563529</td>\n",
       "      <td>0.250552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert-select-mod-dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.562941</td>\n",
       "      <td>0.562941</td>\n",
       "      <td>0.483890</td>\n",
       "      <td>0.562941</td>\n",
       "      <td>0.246169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert-select-mod-gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.562059</td>\n",
       "      <td>0.562059</td>\n",
       "      <td>0.479896</td>\n",
       "      <td>0.562059</td>\n",
       "      <td>0.243019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>egat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.518824</td>\n",
       "      <td>0.518824</td>\n",
       "      <td>0.444969</td>\n",
       "      <td>0.518824</td>\n",
       "      <td>0.220616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.525882</td>\n",
       "      <td>0.525882</td>\n",
       "      <td>0.436049</td>\n",
       "      <td>0.525882</td>\n",
       "      <td>0.207869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>egat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.515882</td>\n",
       "      <td>0.515882</td>\n",
       "      <td>0.418930</td>\n",
       "      <td>0.515882</td>\n",
       "      <td>0.190920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.520294</td>\n",
       "      <td>0.520294</td>\n",
       "      <td>0.409664</td>\n",
       "      <td>0.520294</td>\n",
       "      <td>0.180023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.383884</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.165537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dgcn-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.486176</td>\n",
       "      <td>0.486176</td>\n",
       "      <td>0.345838</td>\n",
       "      <td>0.486176</td>\n",
       "      <td>0.128666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.334212</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.117491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gatv1-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.481176</td>\n",
       "      <td>0.481176</td>\n",
       "      <td>0.331774</td>\n",
       "      <td>0.481176</td>\n",
       "      <td>0.116082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gatv1-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.479706</td>\n",
       "      <td>0.479706</td>\n",
       "      <td>0.325231</td>\n",
       "      <td>0.479706</td>\n",
       "      <td>0.110082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>0.322670</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>0.107168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.479412</td>\n",
       "      <td>0.479412</td>\n",
       "      <td>0.322159</td>\n",
       "      <td>0.479412</td>\n",
       "      <td>0.106727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gatv2-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.478235</td>\n",
       "      <td>0.478235</td>\n",
       "      <td>0.316722</td>\n",
       "      <td>0.478235</td>\n",
       "      <td>0.101959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dgcn-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.313248</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.098278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475882</td>\n",
       "      <td>0.475882</td>\n",
       "      <td>0.309869</td>\n",
       "      <td>0.475882</td>\n",
       "      <td>0.095511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475294</td>\n",
       "      <td>0.475294</td>\n",
       "      <td>0.306762</td>\n",
       "      <td>0.475294</td>\n",
       "      <td>0.092635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rgat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475294</td>\n",
       "      <td>0.475294</td>\n",
       "      <td>0.306641</td>\n",
       "      <td>0.475294</td>\n",
       "      <td>0.092856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rgat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gatv2-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gatv1-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gatv1-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              data_combination typeSet  isSimpleFC  Accuracy    Recall  \\\n",
       "25            bert-select-more    test        True  0.625000  0.625000   \n",
       "0                         bert    test       False  0.616471  0.616471   \n",
       "22                        bert    test        True  0.622059  0.622059   \n",
       "24             bert-select-mod    test        True  0.618824  0.618824   \n",
       "23             bert-select-few    test        True  0.604706  0.604706   \n",
       "2              bert-select-mod    test       False  0.603529  0.603529   \n",
       "43        bert-select-mod-egat    test        True  0.601176  0.601176   \n",
       "38        bert-select-mod-dgcn    test        True  0.594706  0.594706   \n",
       "3             bert-select-more    test       False  0.603529  0.603529   \n",
       "40  bert-select-mod-gatv1-edge    test        True  0.593824  0.593824   \n",
       "1              bert-select-few    test       False  0.598235  0.598235   \n",
       "42        bert-select-mod-rgat    test        True  0.597647  0.597647   \n",
       "20        bert-select-mod-rgat    test       False  0.588824  0.588824   \n",
       "39       bert-select-mod-gatv1    test        True  0.579706  0.579706   \n",
       "41  bert-select-mod-gatv2-edge    test        True  0.575588  0.575588   \n",
       "17       bert-select-mod-gatv1    test       False  0.575588  0.575588   \n",
       "21        bert-select-mod-egat    test       False  0.570000  0.570000   \n",
       "18  bert-select-mod-gatv1-edge    test       False  0.563529  0.563529   \n",
       "16        bert-select-mod-dgcn    test       False  0.562941  0.562941   \n",
       "19  bert-select-mod-gatv2-edge    test       False  0.562059  0.562059   \n",
       "37                 egat-select    test        True  0.518824  0.518824   \n",
       "36                        egat    test        True  0.525882  0.525882   \n",
       "15                 egat-select    test       False  0.515882  0.515882   \n",
       "14                        egat    test       False  0.520294  0.520294   \n",
       "26                        dgcn    test        True  0.499412  0.499412   \n",
       "27                 dgcn-select    test        True  0.486176  0.486176   \n",
       "30                  gatv1-edge    test        True  0.485000  0.485000   \n",
       "31           gatv1-edge-select    test        True  0.481176  0.481176   \n",
       "29                gatv1-select    test        True  0.479706  0.479706   \n",
       "32                  gatv2-edge    test        True  0.480882  0.480882   \n",
       "28                       gatv1    test        True  0.479412  0.479412   \n",
       "33           gatv2-edge-select    test        True  0.478235  0.478235   \n",
       "5                  dgcn-select    test       False  0.475000  0.475000   \n",
       "34                        rgat    test        True  0.475882  0.475882   \n",
       "4                         dgcn    test       False  0.475294  0.475294   \n",
       "35                 rgat-select    test        True  0.475294  0.475294   \n",
       "8                   gatv1-edge    test       False  0.475000  0.475000   \n",
       "6                        gatv1    test       False  0.475000  0.475000   \n",
       "13                 rgat-select    test       False  0.475000  0.475000   \n",
       "12                        rgat    test       False  0.475000  0.475000   \n",
       "11           gatv2-edge-select    test       False  0.475000  0.475000   \n",
       "10                  gatv2-edge    test       False  0.475000  0.475000   \n",
       "9            gatv1-edge-select    test       False  0.475000  0.475000   \n",
       "7                 gatv1-select    test       False  0.475000  0.475000   \n",
       "\n",
       "    Weighted-F1  F1-micro  F1-macro  \n",
       "25     0.587070  0.625000  0.356340  \n",
       "0      0.583831  0.616471  0.353195  \n",
       "22     0.574597  0.622059  0.349253  \n",
       "24     0.574092  0.618824  0.357409  \n",
       "23     0.553944  0.604706  0.327099  \n",
       "2      0.550271  0.603529  0.314424  \n",
       "43     0.549285  0.601176  0.311457  \n",
       "38     0.544972  0.594706  0.307191  \n",
       "3      0.544878  0.603529  0.306513  \n",
       "40     0.540591  0.593824  0.302503  \n",
       "1      0.540504  0.598235  0.301486  \n",
       "42     0.537364  0.597647  0.296983  \n",
       "20     0.529595  0.588824  0.289742  \n",
       "39     0.524664  0.579706  0.291083  \n",
       "41     0.522766  0.575588  0.286713  \n",
       "17     0.507465  0.575588  0.269896  \n",
       "21     0.503588  0.570000  0.263736  \n",
       "18     0.489185  0.563529  0.250552  \n",
       "16     0.483890  0.562941  0.246169  \n",
       "19     0.479896  0.562059  0.243019  \n",
       "37     0.444969  0.518824  0.220616  \n",
       "36     0.436049  0.525882  0.207869  \n",
       "15     0.418930  0.515882  0.190920  \n",
       "14     0.409664  0.520294  0.180023  \n",
       "26     0.383884  0.499412  0.165537  \n",
       "27     0.345838  0.486176  0.128666  \n",
       "30     0.334212  0.485000  0.117491  \n",
       "31     0.331774  0.481176  0.116082  \n",
       "29     0.325231  0.479706  0.110082  \n",
       "32     0.322670  0.480882  0.107168  \n",
       "28     0.322159  0.479412  0.106727  \n",
       "33     0.316722  0.478235  0.101959  \n",
       "5      0.313248  0.475000  0.098278  \n",
       "34     0.309869  0.475882  0.095511  \n",
       "4      0.306762  0.475294  0.092635  \n",
       "35     0.306641  0.475294  0.092856  \n",
       "8      0.305932  0.475000  0.092010  \n",
       "6      0.305932  0.475000  0.092010  \n",
       "13     0.305932  0.475000  0.092010  \n",
       "12     0.305932  0.475000  0.092010  \n",
       "11     0.305932  0.475000  0.092010  \n",
       "10     0.305932  0.475000  0.092010  \n",
       "9      0.305932  0.475000  0.092010  \n",
       "7      0.305932  0.475000  0.092010  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e36b6",
   "metadata": {},
   "source": [
    "<h4> Select top 10 unique data combinations then tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36973bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58e8a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1 (isSimpleFC=False): ['bert']\n",
      "Combination 2 (isSimpleFC=True): ['bert-select-more', 'bert-select-mod', 'bert-select-few', 'bert-select-mod-egat', 'bert-select-mod-dgcn', 'bert-select-mod-gatv1-edge', 'bert-select-mod-rgat', 'bert-select-mod-gatv1', 'bert-select-mod-gatv2-edge', 'egat-select', 'egat', 'dgcn', 'dgcn-select', 'gatv1-edge']\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 15\n",
    "counter = 0\n",
    "combination1 = []\n",
    "combination2 = []\n",
    "seen_combinations = set()\n",
    "\n",
    "for idx, row in df_results_sorted.iterrows():\n",
    "    if counter >= max_iterations:\n",
    "        break\n",
    "\n",
    "    if row['data_combination'] in seen_combinations:\n",
    "        continue\n",
    "\n",
    "    if row['isSimpleFC']:\n",
    "        combination2.append(row['data_combination'])\n",
    "    else:\n",
    "        combination1.append(row['data_combination'])\n",
    "\n",
    "    seen_combinations.add(row['data_combination'])\n",
    "    counter += 1\n",
    "\n",
    "# Display the results\n",
    "print(\"Combination 1 (isSimpleFC=False):\", combination1)\n",
    "print(\"Combination 2 (isSimpleFC=True):\", combination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54513411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for isSimpleFC=False: [0]\n",
      "Indices for isSimpleFC=True: [1, 2, 3, 4, 5, 8, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "indices1 = [key for key, value in dictKey.items() if value in combination1]\n",
    "indices2 = [key for key, value in dictKey.items() if value in combination2]\n",
    "\n",
    "print(\"Indices for isSimpleFC=False:\", indices1)\n",
    "print(\"Indices for isSimpleFC=True:\", indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62a5e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainDeepList = [trainList[i] for i in indices1]\n",
    "selectedTestDeepList = [testList[i] for i in indices1]\n",
    "selectedValDeepList = [valList[i] for i in indices1]\n",
    "\n",
    "len(selectedTrainDeepList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ccae3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3a1b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainSet in selectedTrainList:\n",
    "#     print(type(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fee3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainList = [trainList[i] for i in indices2]\n",
    "selectedTestList = [testList[i] for i in indices2]\n",
    "selectedValList = [valList[i] for i in indices2]\n",
    "\n",
    "len(selectedTrainList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c266050",
   "metadata": {},
   "source": [
    "<h4> Tuning using random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da2dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should call both model_train1 and 2\n",
    "\n",
    "def objective_func(X_train, X_test, X_val, \n",
    "               y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC):\n",
    "    results = []\n",
    "    hyperparams_string = (\n",
    "        f'num_epochs={hyperparams[\"num_epochs\"]} '\n",
    "        f'batch_size={hyperparams[\"batch_size\"]} '\n",
    "        f'loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]} '\n",
    "        f'hidden_dims={hyperparams[\"hidden_dims\"]} '\n",
    "        f'dropout_rate={hyperparams[\"dropout_rate\"]} '\n",
    "        f'learning_rate={hyperparams[\"learning_rate\"]} '\n",
    "        f'optimizers={hyperparams[\"optimizers\"]} '\n",
    "        f'criteria={hyperparams[\"criteria\"]}'\n",
    "    )    \n",
    "    print(hyperparams_string)\n",
    "            \n",
    "#     X_tensor, Y_tensor = prep_data(X_train.clone().detach(), y_train, False)\n",
    "    X_train_tensor = to_tensor(X_train)\n",
    "    y_train_tensor = to_tensor(y_train).long()\n",
    "    X_val_tensor = to_tensor(X_val)\n",
    "    y_val_tensor = to_tensor(y_val).long()\n",
    "    X_test_tensor = to_tensor(X_test)\n",
    "    y_test_tensor = to_tensor(y_test).long()\n",
    "# train\n",
    "    start_time = time.time()\n",
    "    if isSimpleFC:\n",
    "        model, num_epoch = model_train2(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "                            hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "                            hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "                            hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])        \n",
    "    else:\n",
    "        model, num_epoch = model_train1(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "                            hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "                            hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "                            hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "# val\n",
    "#     X_tensor, Y_tensor = prep_data(X_val.clone().detach(), y_val, False)\n",
    "    result = classify_emotions(model, X_val_tensor, y_val_tensor, \\\n",
    "                               'validation', isSimpleFC, i_dict)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    result = list(result)\n",
    "    hyperparams_string = f'num_epochs={hyperparams[\"num_epochs\"]}-batch_size={hyperparams[\"batch_size\"]}-loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]}-hidden_dims={hyperparams[\"hidden_dims\"]}-dropout_rate={hyperparams[\"dropout_rate\"]}-learning_rate={hyperparams[\"learning_rate\"]}-optimizers={hyperparams[\"optimizers\"]}-criteria={hyperparams[\"criteria\"]}'\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "# test\n",
    "#     X_tensor, Y_tensor = prep_data(X_test.clone().detach(), y_test, False)\n",
    "    result = classify_emotions(model, X_test_tensor, y_test_tensor, \\\n",
    "                               'test', isSimpleFC, i_dict)\n",
    "    \n",
    "    result = list(result)\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='data_combination', ascending=False)\n",
    "    \n",
    "    return df_results_sorted\n",
    "\n",
    "\n",
    "# def objective_func(X_train, X_test, X_val, \n",
    "#                y_train, y_test, y_val, hyperparams, i_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f998054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X_train, X_test, X_val, \\\n",
    "                  y_train, y_test, y_val, \\\n",
    "                  param_grid, isSimpleFC, i_dict, MAX_EVALS = 15):\n",
    "    \n",
    "    sub_total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    \n",
    "    for i in range(MAX_EVALS):\n",
    "        hyperparams = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        try:\n",
    "            new_results = objective_func(X_train, X_test, X_val,  y_train, y_test, y_val,\n",
    "                                hyperparams, i_dict, isSimpleFC)\n",
    "            sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparams {hyperparams}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    return sub_total_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7295744e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'num_epochs': [50, 80, 120],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'loss_difference_threshold': [0.01, 0.001],\n",
    "    'hidden_dims': [[256, 128], [128, 64], [64, 32]],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}\n",
    "param_grid2 = {\n",
    "    'num_epochs': [50, 80, 120],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'loss_difference_threshold': [0.01, 0.001],\n",
    "    'hidden_dims': [128, 256, 512],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcf2b3",
   "metadata": {},
   "source": [
    "<h5> First find the best hyperparameter combination for the DeepClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "163a1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamTuning(X_trainSet, X_testSet, X_valSet, y_train, y_test, y_val, isSimpleFC, param_grid, indices):\n",
    "    total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    for i in range(len(indices)):\n",
    "        print(\"============ PART \", i, \"============\")\n",
    "        X_train = X_trainSet[i]\n",
    "        X_test = X_testSet[i]\n",
    "        X_val = X_valSet[i]\n",
    "\n",
    "        sub_total_results = random_search(X_train, X_test, X_val, y_train, y_test, y_val,\n",
    "                     param_grid, isSimpleFC, indices[i])\n",
    "        total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e1fd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/deep_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results1_sorted = pickle.load(file)\n",
    "else:\n",
    "    total_results1 = hyperparamTuning(selectedTrainDeepList, selectedTestDeepList, selectedValDeepList, \\\n",
    "                                 y_train, y_test, y_val, False, param_grid1, indices1)\n",
    "    \n",
    "    total_results1_sorted = total_results1.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results1_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90c5d2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.576541</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.335108</td>\n",
       "      <td>2.866733</td>\n",
       "      <td>num_epochs=50-batch_size=64-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.55472</td>\n",
       "      <td>0.55472</td>\n",
       "      <td>0.495982</td>\n",
       "      <td>0.55472</td>\n",
       "      <td>0.296551</td>\n",
       "      <td>2.866733</td>\n",
       "      <td>num_epochs=50-batch_size=64-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "      <td>34.291399</td>\n",
       "      <td>num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "      <td>108.59103</td>\n",
       "      <td>num_epochs=120-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "      <td>77.765808</td>\n",
       "      <td>num_epochs=50-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "      <td>66.601807</td>\n",
       "      <td>num_epochs=80-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.387647</td>\n",
       "      <td>0.387647</td>\n",
       "      <td>0.284991</td>\n",
       "      <td>0.387647</td>\n",
       "      <td>0.094281</td>\n",
       "      <td>1.151769</td>\n",
       "      <td>num_epochs=120-batch_size=32-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>66.601807</td>\n",
       "      <td>num_epochs=80-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>108.59103</td>\n",
       "      <td>num_epochs=120-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>77.765808</td>\n",
       "      <td>num_epochs=50-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>34.291399</td>\n",
       "      <td>num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.329001</td>\n",
       "      <td>0.329001</td>\n",
       "      <td>0.219832</td>\n",
       "      <td>0.329001</td>\n",
       "      <td>0.084056</td>\n",
       "      <td>1.151769</td>\n",
       "      <td>num_epochs=120-batch_size=32-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.151765</td>\n",
       "      <td>0.151765</td>\n",
       "      <td>0.039995</td>\n",
       "      <td>0.151765</td>\n",
       "      <td>0.037648</td>\n",
       "      <td>231.448779</td>\n",
       "      <td>num_epochs=80-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.151765</td>\n",
       "      <td>0.151765</td>\n",
       "      <td>0.039995</td>\n",
       "      <td>0.151765</td>\n",
       "      <td>0.037648</td>\n",
       "      <td>164.25337</td>\n",
       "      <td>num_epochs=50-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.146375</td>\n",
       "      <td>0.146375</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.146375</td>\n",
       "      <td>0.036547</td>\n",
       "      <td>0.844553</td>\n",
       "      <td>num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.3-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.145588</td>\n",
       "      <td>0.145588</td>\n",
       "      <td>0.037004</td>\n",
       "      <td>0.145588</td>\n",
       "      <td>0.03631</td>\n",
       "      <td>0.844553</td>\n",
       "      <td>num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.3-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.03544</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.035586</td>\n",
       "      <td>164.25337</td>\n",
       "      <td>num_epochs=50-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.03544</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.035586</td>\n",
       "      <td>231.448779</td>\n",
       "      <td>num_epochs=80-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080027</td>\n",
       "      <td>0.080027</td>\n",
       "      <td>0.031445</td>\n",
       "      <td>0.080027</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>1.099385</td>\n",
       "      <td>num_epochs=120-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.065294</td>\n",
       "      <td>0.065294</td>\n",
       "      <td>0.027668</td>\n",
       "      <td>0.065294</td>\n",
       "      <td>0.031575</td>\n",
       "      <td>1.099385</td>\n",
       "      <td>num_epochs=120-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_combination     typeSet isSimpleFC  Accuracy    Recall Weighted-F1  \\\n",
       "13             bert        test      False  0.626471  0.626471    0.576541   \n",
       "12             bert  validation      False   0.55472   0.55472    0.495982   \n",
       "15             bert        test      False     0.475     0.475    0.305932   \n",
       "3              bert        test      False     0.475     0.475    0.305932   \n",
       "5              bert        test      False     0.475     0.475    0.305932   \n",
       "11             bert        test      False     0.475     0.475    0.305932   \n",
       "7              bert        test      False  0.387647  0.387647    0.284991   \n",
       "10             bert  validation      False  0.410397  0.410397    0.238834   \n",
       "2              bert  validation      False  0.410397  0.410397    0.238834   \n",
       "4              bert  validation      False  0.410397  0.410397    0.238834   \n",
       "14             bert  validation      False  0.410397  0.410397    0.238834   \n",
       "6              bert  validation      False  0.329001  0.329001    0.219832   \n",
       "9              bert        test      False  0.151765  0.151765    0.039995   \n",
       "17             bert        test      False  0.151765  0.151765    0.039995   \n",
       "18             bert  validation      False  0.146375  0.146375    0.037447   \n",
       "19             bert        test      False  0.145588  0.145588    0.037004   \n",
       "16             bert  validation      False  0.142271  0.142271     0.03544   \n",
       "8              bert  validation      False  0.142271  0.142271     0.03544   \n",
       "0              bert  validation      False  0.080027  0.080027    0.031445   \n",
       "1              bert        test      False  0.065294  0.065294    0.027668   \n",
       "\n",
       "    F1-micro  F1-macro  train_time  \\\n",
       "13  0.626471  0.335108    2.866733   \n",
       "12   0.55472  0.296551    2.866733   \n",
       "15     0.475   0.09201   34.291399   \n",
       "3      0.475   0.09201   108.59103   \n",
       "5      0.475   0.09201   77.765808   \n",
       "11     0.475   0.09201   66.601807   \n",
       "7   0.387647  0.094281    1.151769   \n",
       "10  0.410397  0.083137   66.601807   \n",
       "2   0.410397  0.083137   108.59103   \n",
       "4   0.410397  0.083137   77.765808   \n",
       "14  0.410397  0.083137   34.291399   \n",
       "6   0.329001  0.084056    1.151769   \n",
       "9   0.151765  0.037648  231.448779   \n",
       "17  0.151765  0.037648   164.25337   \n",
       "18  0.146375  0.036547    0.844553   \n",
       "19  0.145588   0.03631    0.844553   \n",
       "16  0.142271  0.035586   164.25337   \n",
       "8   0.142271  0.035586  231.448779   \n",
       "0   0.080027  0.036259    1.099385   \n",
       "1   0.065294  0.031575    1.099385   \n",
       "\n",
       "                                                                                                                                                                                                                     hyperparams  \\\n",
       "13  num_epochs=50-batch_size=64-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "12  num_epochs=50-batch_size=64-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "15              num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "3            num_epochs=120-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "5             num_epochs=50-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "11            num_epochs=80-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "7       num_epochs=120-batch_size=32-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "10            num_epochs=80-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "2            num_epochs=120-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "4             num_epochs=50-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[128, 64]-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "14              num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "6       num_epochs=120-batch_size=32-loss_difference_threshold=0.01-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "9                num_epochs=80-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "17              num_epochs=50-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "18               num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.3-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "19               num_epochs=50-batch_size=64-loss_difference_threshold=0.01-hidden_dims=[128, 64]-dropout_rate=0.3-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "16              num_epochs=50-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[256, 128]-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "8                num_epochs=80-batch_size=4-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "0     num_epochs=120-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "1     num_epochs=120-batch_size=32-loss_difference_threshold=0.001-hidden_dims=[64, 32]-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "\n",
       "   num_epoch  \n",
       "13         2  \n",
       "12         2  \n",
       "15        50  \n",
       "3        120  \n",
       "5         50  \n",
       "11        80  \n",
       "7          1  \n",
       "10        80  \n",
       "2        120  \n",
       "4         50  \n",
       "14        50  \n",
       "6          1  \n",
       "9         80  \n",
       "17        50  \n",
       "18         1  \n",
       "19         1  \n",
       "16        50  \n",
       "8         80  \n",
       "0          1  \n",
       "1          1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "\n",
    "total_results1_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d089c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ PART  0 ============\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████| 120/120 [02:29<00:00,  1.25s/epoch, loss=-3.35e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████| 50/50 [00:31<00:00,  1.60epoch/s, loss=-61.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=80 batch_size=16 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████| 80/80 [01:27<00:00,  1.09s/epoch, loss=-617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 50/50 [00:40<00:00,  1.24epoch/s, loss=-3.26e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████| 120/120 [01:36<00:00,  1.24epoch/s, loss=-1.72e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████| 80/80 [00:24<00:00,  3.30epoch/s, loss=-3.42e+17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|██▎                                                        | 2/50 [00:00<00:17,  2.80epoch/s, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████| 80/80 [00:35<00:00,  2.27epoch/s, loss=-8.16e+35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|███████████▌                                              | 10/50 [00:03<00:14,  2.69epoch/s, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████| 120/120 [02:28<00:00,  1.24s/epoch, loss=-2.6e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|██████████▌                                                | 9/50 [00:04<00:19,  2.07epoch/s, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|███▌                                                       | 3/50 [00:00<00:15,  3.05epoch/s, loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|████████████████████▉                                    | 44/120 [00:35<01:00,  1.25epoch/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|█████▏                                                     | 7/80 [00:06<01:03,  1.15epoch/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████| 120/120 [00:53<00:00,  2.24epoch/s, loss=-2.6e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-few\n",
      "============ PART  1 ============\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████| 120/120 [03:14<00:00,  1.62s/epoch, loss=-8.73e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▉                                                         | 2/120 [00:00<00:40,  2.94epoch/s, loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████| 120/120 [00:40<00:00,  2.94epoch/s, loss=-inf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|███████████████████████████████████████████████████████▌   | 113/120 [00:48<00:03,  2.31epoch/s, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Classified:  bert-select-mod\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|██████████████████████                                   | 31/80 [00:19<00:31,  1.55epoch/s, loss=0.604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▉                                                     | 8/80 [00:05<00:48,  1.49epoch/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▉                                                       | 2/120 [00:01<01:26,  1.36epoch/s, loss=0.0188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████████████████████████████████████████████████▎    | 46/50 [00:17<00:01,  2.70epoch/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████| 120/120 [01:27<00:00,  1.38epoch/s, loss=-1.8e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 50/50 [00:44<00:00,  1.13epoch/s, loss=-5.67e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|█████████████████████████████████████                     | 32/50 [00:14<00:08,  2.14epoch/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████| 120/120 [02:42<00:00,  1.35s/epoch, loss=-5.18e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▉                                                         | 2/120 [00:00<00:42,  2.77epoch/s, loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████| 50/50 [00:15<00:00,  3.17epoch/s, loss=-6.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▉                                                      | 2/120 [00:00<00:48,  2.41epoch/s, loss=-.00697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "============ PART  2 ============\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.17s/epoch, loss=-2.75e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=50 batch_size=16 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.02epoch/s, loss=-56.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██████████████▋                                          | 31/120 [00:15<00:43,  2.03epoch/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|██████████████████████                                    | 19/50 [00:10<00:16,  1.88epoch/s, loss=1.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=50 batch_size=16 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|████████████▊                                             | 11/50 [00:17<01:00,  1.56s/epoch, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█████████▌                                               | 20/120 [00:23<01:56,  1.16s/epoch, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|███▊                                                      | 8/120 [00:12<02:50,  1.52s/epoch, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████| 120/120 [02:01<00:00,  1.01s/epoch, loss=-2.88e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████████████████████████▊                              | 24/50 [00:18<00:20,  1.29epoch/s, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=80 batch_size=16 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███████▎                                                  | 10/80 [00:19<02:19,  1.99s/epoch, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=50 batch_size=16 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██████████████▏                                            | 12/50 [00:11<00:35,  1.07epoch/s, loss=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████▊                                       | 26/80 [00:22<00:46,  1.17epoch/s, loss=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|█████▏                                                     | 7/80 [00:03<00:38,  1.92epoch/s, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=80 batch_size=16 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|████▍                                                      | 6/80 [00:12<02:30,  2.03s/epoch, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|██████████████████████████████████████████████████           | 41/50 [00:19<00:04,  2.14epoch/s, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Classified:  bert-select-more\n",
      "============ PART  3 ============\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|████▊                                                    | 10/120 [00:37<06:50,  3.73s/epoch, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████| 50/50 [03:02<00:00,  3.65s/epoch, loss=-3.11e+15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 80/80 [05:12<00:00,  3.91s/epoch, loss=-4.56e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▉                                                     | 5/50 [00:18<02:45,  3.68s/epoch, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███████▌                                                    | 15/120 [00:51<06:03,  3.46s/epoch, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|██████▋                                                    | 9/80 [00:34<04:29,  3.80s/epoch, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|██████████▍                                              | 22/120 [01:41<07:30,  4.60s/epoch, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████| 120/120 [06:51<00:00,  3.43s/epoch, loss=-3.49e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|███▋                                                       | 5/80 [00:17<04:25,  3.54s/epoch, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▉                                                     | 8/80 [00:28<04:13,  3.53s/epoch, loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|███████▉                                                  | 11/80 [00:37<03:56,  3.43s/epoch, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████▉                     | 51/80 [03:06<01:46,  3.66s/epoch, loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|█████▏                                                     | 7/80 [00:30<05:16,  4.33s/epoch, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███████████████████████▏                                  | 20/50 [01:06<01:39,  3.33s/epoch, loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|███████████████████████████████████████▋                     | 52/80 [02:41<01:27,  3.11s/epoch, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Classified:  dgcn\n",
      "============ PART  4 ============\n",
      "num_epochs=80 batch_size=32 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██████████████                                             | 19/80 [00:13<00:42,  1.44epoch/s, loss=1.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|██████████▌                                                | 9/50 [00:03<00:15,  2.73epoch/s, loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=80 batch_size=16 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 80/80 [01:30<00:00,  1.13s/epoch, loss=-4.92e+7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=80 batch_size=16 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|████████▋                                                 | 12/80 [00:09<00:54,  1.26epoch/s, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=50 batch_size=64 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 50/50 [00:16<00:00,  2.99epoch/s, loss=-9.97e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|███████████████████████████████████▉                      | 31/50 [00:16<00:09,  1.92epoch/s, loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████████████████████████████████████████▉                 | 36/50 [00:13<00:05,  2.60epoch/s, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Classified:  dgcn-select\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|███████████████▍                                           | 21/80 [00:06<00:18,  3.12epoch/s, loss=1.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████████████████████████████████████████████████▌        | 69/80 [00:21<00:03,  3.24epoch/s, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "NaN loss encountered. Skipping this batch.\n",
      "Classified:  dgcn-select\n",
      "num_epochs=50 batch_size=32 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 50/50 [00:31<00:00,  1.57epoch/s, loss=-2.85e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▉                                                     | 8/80 [00:03<00:34,  2.10epoch/s, loss=1.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▋                                                   | 12/120 [00:09<01:25,  1.26epoch/s, loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|██▍                                                       | 5/120 [00:02<00:52,  2.17epoch/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███████▏                                                 | 15/120 [00:13<01:31,  1.15epoch/s, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "num_epochs=80 batch_size=16 loss_difference_threshold=0.01 hidden_dims=128 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|██████                                                      | 8/80 [00:06<00:57,  1.26epoch/s, loss=2.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn-select\n",
      "============ PART  5 ============\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|█▍                                                         | 2/80 [00:06<04:02,  3.11s/epoch, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=80 batch_size=16 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████| 80/80 [07:02<00:00,  5.28s/epoch, loss=-1.37e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▋                                                   | 12/120 [00:43<06:30,  3.62s/epoch, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████| 120/120 [12:13<00:00,  6.11s/epoch, loss=-2.1e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=120 batch_size=16 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▉                                                         | 2/120 [00:11<11:21,  5.77s/epoch, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████| 80/80 [06:34<00:00,  4.93s/epoch, loss=-13.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=120 batch_size=32 loss_difference_threshold=0.01 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████| 120/120 [08:39<00:00,  4.33s/epoch, loss=-554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|███████████████████████████                              | 38/80 [02:13<02:27,  3.51s/epoch, loss=-.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv1-edge\n",
      "num_epochs=120 batch_size=64 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|███████████████████████████▌                            | 59/120 [03:22<03:26,  3.38s/epoch, loss=-6.88]"
     ]
    }
   ],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/simple_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results2_sorted = pickle.load(file)\n",
    "else: \n",
    "    total_results2 = hyperparamTuning(selectedTrainList, selectedTestList, selectedValList, \\\n",
    "                                     y_train, y_test, y_val, True, param_grid2, indices2)\n",
    "    \n",
    "    total_results2_sorted = total_results2.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results2_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "    \n",
    "total_results2_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
