{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from graph_context_dataset import FeatureEngineeredDataset\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import random\n",
    "from model import FCClassifier, MyNetwork, DATASET_PATH, MatchingAttention\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b22dd",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "\n",
    " - dataset_original\n",
    " - dataset_drop_noise\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1470c3",
   "metadata": {},
   "source": [
    "<h3> Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68406d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4fb045",
   "metadata": {
    "code_folding": [
     0,
     9,
     18,
     21,
     24,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# class FCLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(FCLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# class ActivationLayer(nn.Module):\n",
    "#     def __init__(self, activation_fn):\n",
    "#         super(ActivationLayer, self).__init__()\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.activation_fn(x)\n",
    "#         return x\n",
    "\n",
    "# def tanh(x):\n",
    "#     return torch.tanh(x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return torch.sigmoid(x)\n",
    "# # loss function and its derivative\n",
    "# def mse(y_true, y_pred):\n",
    "#     return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "# def mse_prime(y_true, y_pred):\n",
    "#     return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246bf76e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oversample_data(X_train, Y_train, num_classes):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            # Calculate the number of instances to oversample for this class\n",
    "            num_to_oversample = max_class_count - len(indices)\n",
    "            # Randomly select instances with replacement to oversample\n",
    "            oversampled_indices = np.random.choice(indices, size=num_to_oversample, replace=True)\n",
    "            # Append the oversampled instances to the original data\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return torch.tensor(X_train), torch.tensor(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8349606b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    if not tensor_list:\n",
    "        raise ValueError(\"The tensor list is empty\")\n",
    "\n",
    "    feature_dim = tensor_list[0].shape[1]\n",
    "    for tensor in tensor_list:\n",
    "        if tensor.shape[1] != feature_dim:\n",
    "            raise ValueError(\"All tensors must have the same feature dimension\")\n",
    "    \n",
    "    concatenated_tensor = torch.cat(tensor_list, dim=0)\n",
    "    \n",
    "    return concatenated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503aae",
   "metadata": {},
   "source": [
    "<h4> Import labels and label decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/labels_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_dev.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_val = pickle.load(file)\n",
    "y_val = torch.tensor(y_val)\n",
    "    \n",
    "file_path = 'data/dump/' + dataset_path + '/label_decoder.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b721a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f3e716",
   "metadata": {},
   "source": [
    "<h4> Import the BERT base-node outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c53ba",
   "metadata": {},
   "source": [
    "first we disregard the u' and directly train the h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396cc340",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_EGAT_train.pkl\",\n",
    "]\n",
    "\n",
    "test_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_EGAT_test.pkl\",\n",
    "]\n",
    "\n",
    "val_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_EGAT_dev.pkl\",\n",
    "]\n",
    "\n",
    "dictKey = {\n",
    "    0 : 'bert',\n",
    "    1 : 'bert-select-few',\n",
    "    2 : 'bert-select-mod',\n",
    "    3 : 'bert-select-more',\n",
    "    4 : 'dgcn',\n",
    "    5 : 'dgcn-select',\n",
    "    6 : 'gatv1',\n",
    "    7 : 'gatv1-select',\n",
    "    8 : 'gatv1-edge',\n",
    "    9 : 'gatv1-edge-select',\n",
    "    10 : 'gatv2-edge',\n",
    "    11 : 'gatv2-edge-select',\n",
    "    12 : 'rgat',\n",
    "    13 : 'rgat-select',\n",
    "    14 : 'egat',\n",
    "    15 : 'egat-select',\n",
    "    16 : 'bert-select-mod-dgcn',\n",
    "    17 : 'bert-select-mod-gatv1',\n",
    "    18 : 'bert-select-mod-gatv1-edge',\n",
    "    19 : 'bert-select-mod-gatv2-edge',\n",
    "    20 : 'bert-select-mod-rgat',\n",
    "    21 : 'bert-select-mod-egat',\n",
    "}\n",
    "selected_combination = [0]\n",
    "# selected_combination = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff1721",
   "metadata": {},
   "source": [
    "<h4> Getting BERT and GAT outputs for all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c366b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFeaturesList[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3426c4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n"
     ]
    }
   ],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    return torch.cat(tensor_list, dim=0)\n",
    "\n",
    "def import_h_prime(file_paths):\n",
    "    featuresList = []\n",
    "    attList = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "#             print(\"Check... \", len(data))\n",
    "            if isinstance(data, list):\n",
    "                print(type(data), \" instance of list: \", len(data))\n",
    "                featuresList.append(concatenate_tensors(data))\n",
    "            else:\n",
    "                print(type(data), \" instance of tensor, \", data.shape)\n",
    "                featuresList.append(data)\n",
    "                \n",
    "    return featuresList\n",
    "    \n",
    "trainFeaturesList = import_h_prime(train_file_paths)\n",
    "\n",
    "testFeaturesList = import_h_prime(test_file_paths)\n",
    "\n",
    "valFeaturesList = import_h_prime(val_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041a26d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeaturesList[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33882e42",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getNodalAttn():\n",
    "    nodalAttList = []\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    return nodalAttList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d84c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodalAttn = getNodalAttn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e174164",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "479a3b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a070ce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43fa315",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "<h3> Feature Selection and creating data combination for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Define select feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6501b577",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_norm_features(encoded_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "    return torch.tensor(features_scaled)\n",
    "\n",
    "def get_selected_features(encoded_features, labels, top_n):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "\n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        # Create a binary mask indicating instances belonging to the current class\n",
    "        mask = (labels == label)\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=top_n) \n",
    "        selector.fit(features_scaled, mask)  \n",
    "\n",
    "        top_features_indices = np.argsort(selector.scores_)[-top_n:]\n",
    "        scores = selector.scores_[top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "\n",
    "    selected_features = encoded_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3346eac",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Function to train the autoencoder\n",
    "def train_autoencoder(encoded_features, hidden_dim=100, num_epochs=20, lr=0.001):\n",
    "    input_dim = encoded_features.shape[1]\n",
    "    autoencoder = Autoencoder(input_dim, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    X_tensor = torch.tensor(encoded_features, dtype=torch.float32)\n",
    "    train_loader = torch.utils.data.DataLoader(X_tensor, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Autoencoder\", unit=\"epoch\"):\n",
    "        total_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs = data\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = autoencoder(inputs)\n",
    "            loss = criterion(decoded, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_features, _ = autoencoder(torch.tensor(encoded_features, dtype=torch.float32))\n",
    "    \n",
    "    return encoded_features, autoencoder\n",
    "\n",
    "def get_selected_features_autoencoder(autoencoder, encoded_features, labels, top_n=100):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "\n",
    "    reduced_features = autoencoder.encoder(torch.tensor(encoded_features, dtype=torch.float32)).detach().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(reduced_features)\n",
    "    \n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        mask = (labels == label)\n",
    "        selector = chi2(features_scaled, mask)\n",
    "        top_features_indices = np.argsort(selector[0])[-top_n:]\n",
    "        scores = selector[0][top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "    selected_features = reduced_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46d79a94",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efccc17d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca73f7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Now prepare the data that will be ued to train the classifier, there are 20 combinations. And pick top 7 combinations yielding top F1 weighted-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b7d2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in trainFeaturesList[6][:10]:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76693e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61905fb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "trainList = []\n",
    "testList = []\n",
    "valList = []\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/trainList.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/testList.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/valList.pkl\"\n",
    "\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3: \n",
    "    with open(file_path1, \"rb\") as file:\n",
    "        trainList = pickle.load(file)\n",
    "    with open(file_path2, \"rb\") as file:\n",
    "        testList = pickle.load(file)\n",
    "    with open(file_path3, \"rb\") as file:\n",
    "        valList = pickle.load(file)\n",
    "else:\n",
    "#     trainFeaturesList.append(data)\n",
    "    #1\n",
    "    trainList.append(trainFeaturesList[0])\n",
    "    testList.append(testFeaturesList[0])\n",
    "    valList.append(valFeaturesList[0])\n",
    "    #2\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 16)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #3\n",
    "    selectedTrainFeatures1b, indicesFeatures1b = get_selected_features(trainFeaturesList[0], y_train, 32)\n",
    "    selectedTestFeatures1b = testFeaturesList[0][:, indicesFeatures1b]\n",
    "    selectedValFeatures1b = valFeaturesList[0][:, indicesFeatures1b]\n",
    "    trainList.append(selectedTrainFeatures1b)\n",
    "    testList.append(selectedTestFeatures1b)\n",
    "    valList.append(selectedValFeatures1b)\n",
    "    #4\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 64)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #5\n",
    "    trainList.append(trainFeaturesList[1])\n",
    "    testList.append(testFeaturesList[1])\n",
    "    valList.append(valFeaturesList[1])\n",
    "    #6\n",
    "    selectedTrainFeatures2, indicesFeatures2 = get_selected_features(trainFeaturesList[1], y_train, 12)\n",
    "    selectedTestFeatures2 = testFeaturesList[1][:, indicesFeatures2]\n",
    "    selectedValFeatures2 = valFeaturesList[1][:, indicesFeatures2]\n",
    "    trainList.append(selectedTrainFeatures2)\n",
    "    testList.append(selectedTestFeatures2)\n",
    "    valList.append(selectedValFeatures2)\n",
    "    #7\n",
    "    trainList.append(trainFeaturesList[2])\n",
    "    testList.append(testFeaturesList[2])\n",
    "    valList.append(valFeaturesList[2])\n",
    "    #8\n",
    "    selectedTrainFeatures3, indicesFeatures3 = get_selected_features(trainFeaturesList[2], y_train, 12)\n",
    "    selectedTestFeatures3 = testFeaturesList[2][:, indicesFeatures3]\n",
    "    selectedValFeatures3 = valFeaturesList[2][:, indicesFeatures3]\n",
    "    trainList.append(selectedTrainFeatures3)\n",
    "    testList.append(selectedTestFeatures3)\n",
    "    valList.append(selectedValFeatures3)\n",
    "    #9\n",
    "    trainList.append(trainFeaturesList[3])\n",
    "    testList.append(testFeaturesList[3])\n",
    "    valList.append(valFeaturesList[3])\n",
    "    #10\n",
    "    selectedTrainFeatures4, indicesFeatures4 = get_selected_features(trainFeaturesList[3], y_train, 12)\n",
    "    selectedTestFeatures4 = testFeaturesList[3][:, indicesFeatures4]\n",
    "    selectedValFeatures4 = valFeaturesList[3][:, indicesFeatures4]\n",
    "    trainList.append(selectedTrainFeatures4)\n",
    "    testList.append(selectedTestFeatures4)\n",
    "    valList.append(selectedValFeatures4)\n",
    "    #11\n",
    "    trainList.append(trainFeaturesList[4])\n",
    "    testList.append(testFeaturesList[4])\n",
    "    valList.append(valFeaturesList[4])\n",
    "    #12\n",
    "    selectedTrainFeatures5, indicesFeatures5 = get_selected_features(trainFeaturesList[4], y_train, 12)\n",
    "    selectedTestFeatures5 = testFeaturesList[4][:, indicesFeatures5]\n",
    "    selectedValFeatures5 = valFeaturesList[4][:, indicesFeatures5]\n",
    "    trainList.append(selectedTrainFeatures5)\n",
    "    testList.append(selectedTestFeatures5)\n",
    "    valList.append(selectedValFeatures5)\n",
    "    #13\n",
    "    trainList.append(trainFeaturesList[5])\n",
    "    testList.append(testFeaturesList[5])\n",
    "    valList.append(valFeaturesList[5])\n",
    "    #14\n",
    "    selectedTrainFeatures6, indicesFeatures6 = get_selected_features(trainFeaturesList[5], y_train, 12)\n",
    "    selectedTestFeatures6 = testFeaturesList[5][:, indicesFeatures6]\n",
    "    selectedValFeatures6 = valFeaturesList[5][:, indicesFeatures6]\n",
    "    trainList.append(selectedTrainFeatures6)\n",
    "    testList.append(selectedTestFeatures6)\n",
    "    valList.append(selectedValFeatures6)\n",
    "    #15\n",
    "    trainList.append(trainFeaturesList[6])\n",
    "    testList.append(testFeaturesList[6])\n",
    "    valList.append(valFeaturesList[6])\n",
    "    #16\n",
    "    selectedTrainFeatures7, indicesFeatures7 = get_selected_features(trainFeaturesList[6], y_train, 12)\n",
    "    selectedTestFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    selectedValFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    trainList.append(selectedTrainFeatures7)\n",
    "    testList.append(selectedTestFeatures7)\n",
    "    valList.append(selectedValFeatures7)\n",
    "    selectedNormTrainFeatures1 = get_norm_features(selectedTrainFeatures1b)\n",
    "    selectedNormTestFeatures1 = get_norm_features(selectedTestFeatures1b)\n",
    "    selectedNormValFeatures1 = get_norm_features(selectedValFeatures1b)\n",
    "\n",
    "    #17\n",
    "    trainNormFeatures2 = get_norm_features(trainFeaturesList[1].detach().numpy())\n",
    "    testNormFeatures2 = get_norm_features(testFeaturesList[1].detach().numpy())\n",
    "    valNormFeatures2 = get_norm_features(valFeaturesList[1].detach().numpy())\n",
    "    concatenatedTrainFeatures2 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures2), dim=1)\n",
    "    concatenatedTestFeatures2 = torch.cat((selectedNormTestFeatures1, testNormFeatures2), dim=1)\n",
    "    concatenatedValFeatures2 = torch.cat((selectedNormValFeatures1, valNormFeatures2), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures2)\n",
    "    testList.append(concatenatedTestFeatures2)\n",
    "    valList.append(concatenatedValFeatures2)\n",
    "    #18\n",
    "    trainNormFeatures3 = get_norm_features(trainFeaturesList[2].detach().numpy())\n",
    "    testNormFeatures3 = get_norm_features(testFeaturesList[2].detach().numpy())\n",
    "    valNormFeatures3 = get_norm_features(valFeaturesList[2].detach().numpy())\n",
    "    concatenatedTrainFeatures3 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures3), dim=1)\n",
    "    concatenatedTestFeatures3 = torch.cat((selectedNormTestFeatures1, testNormFeatures3), dim=1)\n",
    "    concatenatedValFeatures3 = torch.cat((selectedNormValFeatures1, valNormFeatures3), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures3)\n",
    "    testList.append(concatenatedTestFeatures3)\n",
    "    valList.append(concatenatedValFeatures3)\n",
    "    #19\n",
    "    trainNormFeatures4 = get_norm_features(trainFeaturesList[3].detach().numpy())\n",
    "    testNormFeatures4 = get_norm_features(testFeaturesList[3].detach().numpy())\n",
    "    valNormFeatures4 = get_norm_features(valFeaturesList[3].detach().numpy())\n",
    "    concatenatedTrainFeatures4 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures4), dim=1)\n",
    "    concatenatedTestFeatures4 = torch.cat((selectedNormTestFeatures1, testNormFeatures4), dim=1)\n",
    "    concatenatedValFeatures4 = torch.cat((selectedNormValFeatures1, valNormFeatures4), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures4)\n",
    "    testList.append(concatenatedTestFeatures4)\n",
    "    valList.append(concatenatedValFeatures4)\n",
    "    #20\n",
    "    trainNormFeatures5 = get_norm_features(trainFeaturesList[4].detach().numpy())\n",
    "    testNormFeatures5 = get_norm_features(testFeaturesList[4].detach().numpy())\n",
    "    valNormFeatures5 = get_norm_features(valFeaturesList[4].detach().numpy())\n",
    "    concatenatedTrainFeatures5 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures5), dim=1)\n",
    "    concatenatedTestFeatures5 = torch.cat((selectedNormTestFeatures1, testNormFeatures5), dim=1)\n",
    "    concatenatedValFeatures5 = torch.cat((selectedNormValFeatures1, valNormFeatures5), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures5)\n",
    "    testList.append(concatenatedTestFeatures5)\n",
    "    valList.append(concatenatedValFeatures5)\n",
    "\n",
    "    #21\n",
    "    trainNormFeatures6 = get_norm_features(trainFeaturesList[5].detach().numpy())\n",
    "    testNormFeatures6 = get_norm_features(testFeaturesList[5].detach().numpy())\n",
    "    valNormFeatures6 = get_norm_features(valFeaturesList[5].detach().numpy())\n",
    "    concatenatedTrainFeatures6 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures6), dim=1)\n",
    "    concatenatedTestFeatures6 = torch.cat((selectedNormTestFeatures1, testNormFeatures6), dim=1)\n",
    "    concatenatedValFeatures6 = torch.cat((selectedNormValFeatures1, valNormFeatures6), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures6)\n",
    "    testList.append(concatenatedTestFeatures6)\n",
    "    valList.append(concatenatedValFeatures6)\n",
    "\n",
    "    #22\n",
    "    trainNormFeatures7 = get_norm_features(trainFeaturesList[6].detach().numpy())\n",
    "    testNormFeatures7 = get_norm_features(testFeaturesList[6].detach().numpy())\n",
    "    valNormFeatures7 = get_norm_features(valFeaturesList[6].detach().numpy())\n",
    "    concatenatedTrainFeatures7 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures7), dim=1)\n",
    "    concatenatedTestFeatures7 = torch.cat((selectedNormTestFeatures1, testNormFeatures7), dim=1)\n",
    "    concatenatedValFeatures7 = torch.cat((selectedNormValFeatures1, valNormFeatures7), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures7)\n",
    "    testList.append(concatenatedTestFeatures7)\n",
    "    valList.append(concatenatedValFeatures7)\n",
    "\n",
    "    trainList_tensors = [torch.tensor(item) for item in trainList]\n",
    "    testList_tensors = [torch.tensor(item) for item in testList]\n",
    "    valList_tensors = [torch.tensor(item) for item in valList]\n",
    "\n",
    "    with open(file_path1, 'wb') as file:\n",
    "        pickle.dump(trainList_tensors, file)\n",
    "    with open(file_path2, 'wb') as file:\n",
    "        pickle.dump(testList_tensors, file)\n",
    "    with open(file_path3, 'wb') as file:\n",
    "        pickle.dump(valList_tensors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6688b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 96])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainList[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28b8647e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# for item in trainFeaturesList:\n",
    "#     print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb893e7",
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def to_tensor(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return torch.tensor(data)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")\n",
    "        \n",
    "def prep_data(features, labels, isOversample):\n",
    "    num_classes = 7\n",
    "\n",
    "    if isOversample:\n",
    "        X_set, Y_set = oversample_data(features, labels, num_classes)\n",
    "    else:\n",
    "        X_set, Y_set = features, labels\n",
    "\n",
    "    if isinstance(X_set, torch.Tensor):\n",
    "        X_tensor = X_set.float()\n",
    "    else:\n",
    "        X_tensor = torch.tensor(X_set, dtype=torch.float32)\n",
    "    \n",
    "    if isinstance(Y_set, torch.Tensor):\n",
    "        Y_tensor = Y_set.long()\n",
    "    else:\n",
    "        Y_tensor = torch.tensor(Y_set, dtype=torch.long)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(Y_set, return_counts=True)\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4bff2",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a71c886",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def model_train1(X_set, Y_set, num_epochs=20, batch_size=32, loss_difference_threshold=0.01, \n",
    "#                  hidden_dims=[256, 128], dropout_rate=0.5, lr=0.0001, \n",
    "#                  optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, matchAtt\n",
    "#                  ):\n",
    "#     output_dim = 7  # Number of classes\n",
    "#     model = MyNetwork(len(X_set[0]), hidden_dims, output_dim, dropout_rate)\n",
    "#     criterion = criterion_class()\n",
    "#     optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "#     loss_history = []\n",
    "#     accuracy_history = []\n",
    "# #     print_interval = 1  # Print tqdm every epoch\n",
    "#     previous_loss = float('inf')\n",
    "\n",
    "#     # Create dataset and dataloader\n",
    "#     dataset = TensorDataset(X_set, Y_set)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     epoch_num = num_epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         correct_predictions = 0\n",
    "#         total_instances = 0\n",
    "#         with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "#             for inputs, labels in dataloader:\n",
    "#                 inputs = inputs.float()  # Ensure inputs are float32\n",
    "#                 labels = labels.long()   # Ensure labels are long\n",
    "#                 optimizer.zero_grad()\n",
    "# #                 TODO\n",
    "#                 outputs = model(inputs)\n",
    "#                 outputs = outputs.squeeze()\n",
    "#                 labels = labels.squeeze()\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # Check for NaN loss values\n",
    "#                 if torch.isnan(loss):\n",
    "#                     print(\"NaN loss encountered. Skipping this batch.\")\n",
    "#                     break\n",
    "                \n",
    "#                 # Apply gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, dim=1)\n",
    "#                 correct_predictions += (predicted == labels).sum().item()\n",
    "#                 total_instances += labels.size(0)\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#         epoch_loss = total_loss / total_instances\n",
    "#         epoch_accuracy = correct_predictions / total_instances\n",
    "#         loss_history.append(epoch_loss)\n",
    "#         accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "#         if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "#             epoch_num = epoch\n",
    "#             break\n",
    "\n",
    "#         previous_loss = epoch_loss\n",
    "\n",
    "#     return model, epoch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0827f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X_set, Y_set):\n",
    "    indices = np.arange(len(X_set))\n",
    "    np.random.shuffle(indices)\n",
    "    return X_set[indices], Y_set[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5237e4fd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train1(X_set=None, Y_set=None, num_epochs=50, loss_difference_threshold=0.0001, \n",
    "                 hidden_dims=[64, 32], dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = MyNetwork(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=7, dropout_rate=0.5)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "                if nodalAtt:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                else:\n",
    "                    loss.backward(retain_graph=False)\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "        if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "            epoch_num = epoch\n",
    "            break\n",
    "\n",
    "        previous_loss = epoch_loss\n",
    "\n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae7cd0",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a03e2cd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train2(X_set=None, Y_set=None, num_epochs=20, loss_difference_threshold=0.001, \n",
    "                 hidden_dims=128, dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    output_dim = 7  # Number of classes\n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = FCClassifier(input_dim, hidden_dims, output_dim, dropout_rate)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "#                 print(outputs.shape, labels.shape)\n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                if nodalAtt:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                else:\n",
    "                    loss.backward(retain_graph=False)\n",
    "                    \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(log_prob, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "                \n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "        \n",
    "        if epoch > 0:\n",
    "            loss_diff = abs(epoch_loss - previous_loss)\n",
    "            if loss_diff < loss_difference_threshold:\n",
    "                print(f\"Training stopped early at epoch {epoch+1}.\")\n",
    "                print(f\"Loss difference ({loss_diff}) is below the threshold ({loss_difference_threshold}).\")\n",
    "                epoch_num = epoch + 1\n",
    "                break\n",
    "        \n",
    "        previous_loss = epoch_loss\n",
    "    \n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b435c98b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_emotions(model=None, X_set=None, Y_set=None, typeSet=None, \n",
    "                      isSimpleFC=False, i_dict=None,\n",
    "                      nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize empty lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    i = 0\n",
    "    # Iterate over the given ranges\n",
    "    for start, end in ranges:\n",
    "        X_batch = X_set[start:end+1].float()\n",
    "        Y_batch = Y_set[start:end+1].long()\n",
    "\n",
    "#         if X_batch.dtype != torch.float32:\n",
    "#             X_batch = X_batch.float()\n",
    "\n",
    "        # Use no_grad to save memory and computations\n",
    "        with torch.no_grad():\n",
    "            inputs = X_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            labels = Y_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            last_idx = seq_len[i][0]\n",
    "            \n",
    "            umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "            outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "\n",
    "            outputs = outputs.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "            log_prob = F.log_softmax(outputs, 1)\n",
    "\n",
    "            _, predicted = torch.max(log_prob, 1)\n",
    "\n",
    "            # Append the predictions and labels to the lists\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            i = i+1\n",
    "\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'true_label': all_labels,\n",
    "        'predicted_label': all_predictions\n",
    "    })\n",
    "    \n",
    "    \n",
    "    file_name = f\"data/dump/{dataset_path}/BERT_data_for_classifier/{dictKey[i_dict]}_predictedTest.pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(df_predictions, file)\n",
    "        \n",
    "    # Generate the classification report\n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_decoder.values(), \n",
    "                                   output_dict=True, zero_division=0, digits=4)\n",
    "    print(report)\n",
    "    # Calculate the required metrics\n",
    "    accuracy = report['accuracy']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    f1_micro = report.get('micro avg', {}).get('f1-score', accuracy)\n",
    "    f1_macro = report.get('macro avg', {}).get('f1-score', 0.0) \n",
    "\n",
    "    if typeSet == \"validation\":\n",
    "        print(\"Classified: \", dictKey[i_dict])\n",
    "    \n",
    "    return dictKey[i_dict], typeSet, isSimpleFC, accuracy, recall, weighted_f1, f1_micro, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b35df7e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for trainSet, testSet, valSet, _, _, _ in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "#     print(i, type(trainSet))\n",
    "#     if isinstance(trainSet, list):\n",
    "#         print(type(trainSet[0]))\n",
    "#         sample = trainSet[0]\n",
    "#         print(sample.shape)\n",
    "#     else:\n",
    "#         print(trainSet.squeeze(0).shape)\n",
    "#     i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9231db5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getGraphComponents(file_path):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    if checkFile:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths = pickle.load(file)  \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a83d29f",
   "metadata": {
    "code_folding": [
     4,
     7,
     10
    ]
   },
   "outputs": [],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/pre_h_prime_BERT_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/pre_h_prime_BERT_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/pre_h_prime_BERT_dev.pkl'\n",
    "\n",
    "train_umask, train_seq_lengths, train_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path1)\n",
    "\n",
    "test_umask, test_seq_lengths, test_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path2)\n",
    "\n",
    "val_umask, val_seq_lengths, val_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1984b",
   "metadata": {},
   "source": [
    "UpdateJune 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "741daeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160 2160\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_umask), len(train_seq_lengths))\n",
    "print(train_seq_lengths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "163d2e11",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getSpeakersAndRanges(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        encodedSpeakers, ranges = pickle.load(file)\n",
    "    file.close()\n",
    "    return encodedSpeakers, ranges\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\"\n",
    "\n",
    "encodedSpeakersTrain, rangesTrain = getSpeakersAndRanges(file_path1)\n",
    "encodedSpeakersTest, rangesTest = getSpeakersAndRanges(file_path2)\n",
    "encodedSpeakersDev, rangesDev = getSpeakersAndRanges(file_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32d7a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in rangesTrain:\n",
    "#     if item[0]+1 == item[1]:\n",
    "#         print(\"Dup detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60304bfd",
   "metadata": {},
   "source": [
    "Sample run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f1571",
   "metadata": {},
   "source": [
    "Verifying the attention in batch before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9385da0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class MatchingAttention(nn.Module):\n",
    "#     def __init__(self, mem_dim, cand_dim, alpha_dim, att_type='general2'):\n",
    "#         super(MatchingAttention, self).__init__()\n",
    "#         self.mem_dim = mem_dim\n",
    "#         self.cand_dim = cand_dim\n",
    "#         self.alpha_dim = alpha_dim\n",
    "#         self.att_type = att_type\n",
    "\n",
    "#         if self.att_type == 'general2':\n",
    "#             self.transform = nn.Linear(self.mem_dim, self.cand_dim * self.alpha_dim)\n",
    "\n",
    "#     def forward(self, M, x, mask):\n",
    "#         M_ = M.permute(1, 2, 0)  # (batch, mem_dim, seq_len)\n",
    "#         x_ = self.transform(x).unsqueeze(1)  # (batch, 1, cand_dim * alpha_dim)\n",
    "#         mask_ = mask.unsqueeze(2).repeat(1, 1, self.mem_dim).transpose(1, 2)  # (batch, mem_dim, seq_len)\n",
    "        \n",
    "#         M_ = M_ * mask_\n",
    "#         alpha = torch.bmm(x_, M_)  # (batch, 1, seq_len)\n",
    "        \n",
    "#         alpha = F.softmax(alpha, dim=-1)  # Apply softmax to get attention weights\n",
    "#         attended = torch.bmm(alpha, M.permute(1, 0, 2))  # (batch, 1, mem_dim)\n",
    "        \n",
    "#         return attended.squeeze(1), alpha\n",
    "    \n",
    "# def attentive_node_features(emotions, seq_lengths, umask, matchatt_layer):\n",
    "#     max_len = max(seq_lengths)\n",
    "#     batch_size = len(seq_lengths)\n",
    "#     mem_dim = emotions.size(1)\n",
    "\n",
    "#     padded_emotions = []\n",
    "#     for i in range(batch_size):\n",
    "#         length = seq_lengths[i]\n",
    "#         # Assuming emotions is already a 2D tensor of shape (seq_len, mem_dim)\n",
    "#         padded_emotion = F.pad(emotions[:length], (0, 0, 0, max_len - length), \"constant\", 0)\n",
    "#         padded_emotions.append(padded_emotion)\n",
    "\n",
    "#     emotions_padded = torch.stack(padded_emotions, dim=1)  # (max_len, batch_size, mem_dim)\n",
    "    \n",
    "#     att_emotions = []\n",
    "#     alpha_list = []\n",
    "#     for t in range(max_len):\n",
    "#         att_em, alpha = matchatt_layer(emotions_padded, emotions_padded[t], umask)\n",
    "#         att_emotions.append(att_em)\n",
    "#         alpha_list.append(alpha)\n",
    "\n",
    "#     att_emotions = torch.stack(att_emotions, dim=0)  # (max_len, batch_size, mem_dim)\n",
    "\n",
    "#     # Remove the singleton dimension for batch size 1\n",
    "#     att_emotions = att_emotions.squeeze(1)  # (seq_len, mem_dim)\n",
    "\n",
    "#     return att_emotions, alpha_list\n",
    "    \n",
    "# class MyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "#         super(MyNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dims[0])  # Adjust input_dim to match flattened shape\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=64, cand_dim=64, alpha_dim=1, att_type='general2')\n",
    "\n",
    "#     def forward(self, x, nodalAtt, seq_lengths, umask):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "        \n",
    "#         x = self.fc1(att_emotions)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# class FCClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "#         super(FCClassifier, self).__init__()\n",
    "#         self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=input_dim, cand_dim=input_dim, alpha_dim=1, att_type='general2')\n",
    "#     def forward(self, x=None, nodalAtt=None, seq_lengths=None, umask=None, no_cuda=True):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "            \n",
    "#         x = F.relu(self.linear1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.linear2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef46abb",
   "metadata": {},
   "source": [
    "part a test case (uncomment the top part to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a77dc9f6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# seq_lengths = [14]  # Sequence length matches the input tensor\n",
    "# umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "# inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "# # Initialize MatchingAttention\n",
    "# matchatt_layer = MatchingAttention(mem_dim=inputs.shape[1], cand_dim=inputs.shape[1], alpha_dim=1, att_type='general2')\n",
    "\n",
    "# # Call attentive_node_features\n",
    "# att_emotions, alpha_list = attentive_node_features(inputs, seq_lengths, umask, matchatt_layer)\n",
    "\n",
    "# # Print shapes for verification\n",
    "# print(\"att_emotions shape:\", att_emotions.shape)  # Should be (seq_len, mem_dim)\n",
    "# print(\"alpha_list shape:\", len(alpha_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69e91b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [14]\n",
    "umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "model = MyNetwork(input_dim=inputs.shape[1], hidden_dims=[128, 30], output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  #.Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea58147",
   "metadata": {},
   "source": [
    "part b test case (uncomment the part and the function to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fcc9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7])\n"
     ]
    }
   ],
   "source": [
    "model = FCClassifier(input_dim=inputs.shape[1], hidden_dim=64, output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  # Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e24dbc",
   "metadata": {},
   "source": [
    "end of sample experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b7940",
   "metadata": {},
   "source": [
    "Actual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4db90480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "459175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in rangesTrain:\n",
    "#     if item[0] == item[1]:\n",
    "#         print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c6a93e4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curerntly at bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 6.\n",
      "Loss difference (0.0005825146748408838) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.5714285714285714, 'recall': 0.2868217054263566, 'f1-score': 0.38193548387096776, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.4820717131474104, 'recall': 0.4888888888888889, 'f1-score': 0.485456369107322, 'support': 495.0}, 'neutral': {'precision': 0.6516804888694893, 'recall': 0.9244582043343653, 'f1-score': 0.7644649257552484, 'support': 1615.0}, 'sadness': {'precision': 0.7, 'recall': 0.026615969581749048, 'f1-score': 0.05128205128205128, 'support': 263.0}, 'surprise': {'precision': 0.5266272189349113, 'recall': 0.5056818181818182, 'f1-score': 0.5159420289855072, 'support': 352.0}, 'accuracy': 0.6082352941176471, 'macro avg': {'precision': 0.4188297131971975, 'recall': 0.31892379805902543, 'f1-score': 0.31415440842872805, 'support': 3400.0}, 'weighted avg': {'precision': 0.5751233563101249, 'recall': 0.6082352941176471, 'f1-score': 0.549143894461867, 'support': 3400.0}}\n",
      "Curerntly at bert-select-few\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod\n",
      "This is skipped\n",
      "Curerntly at bert-select-more\n",
      "This is skipped\n",
      "Curerntly at dgcn\n",
      "This is skipped\n",
      "Curerntly at dgcn-select\n",
      "This is skipped\n",
      "Curerntly at gatv1\n",
      "This is skipped\n",
      "Curerntly at gatv1-select\n",
      "This is skipped\n",
      "Curerntly at gatv1-edge\n",
      "This is skipped\n",
      "Curerntly at gatv1-edge-select\n",
      "This is skipped\n",
      "Curerntly at gatv2-edge\n",
      "This is skipped\n",
      "Curerntly at gatv2-edge-select\n",
      "This is skipped\n",
      "Curerntly at rgat\n",
      "This is skipped\n",
      "Curerntly at rgat-select\n",
      "This is skipped\n",
      "Curerntly at egat\n",
      "This is skipped\n",
      "Curerntly at egat-select\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-dgcn\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv1\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv1-edge\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv2-edge\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-rgat\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-egat\n",
      "This is skipped\n"
     ]
    }
   ],
   "source": [
    "dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        df_results_sorted = pickle.load(file)\n",
    "else:\n",
    "    results = []\n",
    "    num_epochs = 30\n",
    "    i = 0\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor = to_tensor(trainSet)\n",
    "        Y_tensor = to_tensor(y_train)\n",
    "        \n",
    "        print(\"Curerntly at\", dictKey[i])\n",
    "#         print(X_tensor.shape, Y_tensor.shape)\n",
    "#         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#                                    isSimpleFC=False, i_dict=i, \n",
    "#                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#                                    ranges=rangesTrain )\n",
    "#         results.append(result1)\n",
    "        if i in selected_combination:\n",
    "            model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "                                     umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "                                   seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "            X_tensor = to_tensor(testSet)\n",
    "            Y_tensor = to_tensor(y_test)\n",
    "            result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "                                       isSimpleFC=True, i_dict=i, \n",
    "                                       nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "                                       ranges=rangesTest)\n",
    "    #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "    #                                    isSimpleFC=True, i_dict=i, \n",
    "    #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "    #                                    ranges=rangesTrain)\n",
    "            results.append(result2)\n",
    "        else:\n",
    "            print(\"This is skipped\")\n",
    "        i = i+1\n",
    "\n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "    df_results = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2013cdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.608235</td>\n",
       "      <td>0.608235</td>\n",
       "      <td>0.549144</td>\n",
       "      <td>0.608235</td>\n",
       "      <td>0.314154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_combination typeSet  isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "0             bert    test        True  0.608235  0.608235     0.549144   \n",
       "\n",
       "   F1-micro  F1-macro  \n",
       "0  0.608235  0.314154  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4ed51",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e36b6",
   "metadata": {},
   "source": [
    "<h4> Select top 10 unique data combinations then tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36973bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58e8a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1 (isSimpleFC=False): []\n",
      "Combination 2 (isSimpleFC=True): ['bert', 'egat']\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 9\n",
    "counter = 0\n",
    "combination1 = []\n",
    "combination2 = []\n",
    "seen_combinations = set()\n",
    "\n",
    "for idx, row in df_results_sorted.iterrows():\n",
    "    if counter >= max_iterations:\n",
    "        break\n",
    "\n",
    "    if row['data_combination'] in seen_combinations:\n",
    "        continue\n",
    "\n",
    "    if row['isSimpleFC']:\n",
    "        combination2.append(row['data_combination'])\n",
    "    else:\n",
    "        combination1.append(row['data_combination'])\n",
    "\n",
    "    seen_combinations.add(row['data_combination'])\n",
    "    counter += 1\n",
    "\n",
    "# Display the results\n",
    "print(\"Combination 1 (isSimpleFC=False):\", combination1)\n",
    "print(\"Combination 2 (isSimpleFC=True):\", combination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54513411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for isSimpleFC=False: []\n",
      "Indices for isSimpleFC=True: [0, 14]\n"
     ]
    }
   ],
   "source": [
    "indices1 = [key for key, value in dictKey.items() if value in combination1]\n",
    "indices2 = [key for key, value in dictKey.items() if value in combination2]\n",
    "\n",
    "print(\"Indices for isSimpleFC=False:\", indices1)\n",
    "print(\"Indices for isSimpleFC=True:\", indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62a5e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainDeepList = [trainList[i] for i in indices1]\n",
    "selectedTestDeepList = [testList[i] for i in indices1]\n",
    "selectedValDeepList = [valList[i] for i in indices1]\n",
    "\n",
    "len(selectedTrainDeepList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ccae3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3a1b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainSet in selectedTrainList:\n",
    "#     print(type(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fee3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainList = [trainList[i] for i in indices2]\n",
    "selectedTestList = [testList[i] for i in indices2]\n",
    "selectedValList = [valList[i] for i in indices2]\n",
    "\n",
    "len(selectedTrainList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c266050",
   "metadata": {},
   "source": [
    "<h4> Tuning using random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da2dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should call both model_train1 and 2\n",
    "\n",
    "def objective_func(X_train, X_test, X_val, y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,\n",
    "                  train_umask, test_umask, val_umask,\n",
    "                  train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                  rangesTrain, rangesTest, rangesDev, nodalAtt):\n",
    "    results = []\n",
    "    hyperparams_string = (\n",
    "        f'num_epochs={hyperparams[\"num_epochs\"]} '\n",
    "        f'loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]} '\n",
    "        f'hidden_dims={hyperparams[\"hidden_dims\"]} '\n",
    "        f'dropout_rate={hyperparams[\"dropout_rate\"]} '\n",
    "        f'learning_rate={hyperparams[\"learning_rate\"]} '\n",
    "        f'optimizers={hyperparams[\"optimizers\"]} '\n",
    "        f'criteria={hyperparams[\"criteria\"]}'\n",
    "    )    \n",
    "    print(hyperparams_string)\n",
    "            \n",
    "    X_train_tensor = to_tensor(X_train)\n",
    "    y_train_tensor = to_tensor(y_train).long()\n",
    "    X_val_tensor = to_tensor(X_val)\n",
    "    y_val_tensor = to_tensor(y_val).long()\n",
    "    X_test_tensor = to_tensor(X_test)\n",
    "    y_test_tensor = to_tensor(y_test).long()\n",
    "# train\n",
    "    start_time = time.time()\n",
    "    if isSimpleFC:\n",
    "        model, num_epoch = model_train2(X_set=X_train_tensor, Y_set=y_train_tensor, \n",
    "                            num_epochs=hyperparams[\"num_epochs\"], loss_difference_threshold=hyperparams[\"loss_difference_threshold\"], \n",
    "                            hidden_dims=hyperparams[\"hidden_dims\"], dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "                            lr=hyperparams[\"learning_rate\"], optimizer_class=hyperparams[\"optimizers\"], \n",
    "                            criterion_class=hyperparams[\"criteria\"],\n",
    "                            umask=train_umask, seq_len=train_seq_lengths, ranges=rangesTrain, nodalAtt=nodalAtt)        \n",
    "#     else:\n",
    "#         model, num_epoch = model_train1(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "#                             hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "#                             hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "#                             hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "# val\n",
    "    result = classify_emotions(model=model, X_set=X_val_tensor, Y_set=y_val_tensor, typeSet='validation', \n",
    "                               isSimpleFC=isSimpleFC, i_dict=i_dict,\n",
    "                               nodalAtt=nodalAtt, umask=val_umask, seq_len=val_seq_lengths, ranges=rangesDev)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    result = list(result)\n",
    "# test\n",
    "\n",
    "    hyperparams_string = f'num_epochs={hyperparams[\"num_epochs\"]}-loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]}-hidden_dims={hyperparams[\"hidden_dims\"]}-dropout_rate={hyperparams[\"dropout_rate\"]}-learning_rate={hyperparams[\"learning_rate\"]}-optimizers={hyperparams[\"optimizers\"]}-criteria={hyperparams[\"criteria\"]}'\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "# test\n",
    "    result = classify_emotions(model=model, X_set=X_test_tensor, Y_set=y_test_tensor, typeSet='test', \n",
    "                               isSimpleFC=isSimpleFC, i_dict=i_dict,\n",
    "                               nodalAtt=nodalAtt, umask=test_umask, seq_len=test_seq_lengths, ranges=rangesTest)\n",
    "    result = list(result)\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "#     result = classify_emotions(model, X_test_tensor, y_test_tensor, \\\n",
    "#                                'test', isSimpleFC, i_dict)\n",
    "    \n",
    "#     result = list(result)\n",
    "#     result.append(elapsed_time)\n",
    "#     result.append(hyperparams_string)\n",
    "#     result.append(num_epoch)\n",
    "#     results.append(result)\n",
    "    \n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='data_combination', ascending=False)\n",
    "    \n",
    "    return df_results_sorted\n",
    "\n",
    "\n",
    "# def objective_func(X_train, X_test, X_val, \n",
    "#                y_train, y_test, y_val, hyperparams, i_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f998054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X_train=None, X_test=None, X_val=None, \n",
    "                  y_train=None, y_test=None, y_val=None, \n",
    "                  param_grid=None, isSimpleFC=True, i_dict=None,\n",
    "                  train_umask=None, test_umask=None, val_umask=None,\n",
    "                  train_seq_lengths=None, test_seq_lengths=None, val_seq_lengths=None,\n",
    "                  rangesTrain=None, rangesTest=None, rangesDev=None, nodalAtt=False\n",
    "                 ):\n",
    "    sub_total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    MAX_EVALS = 5\n",
    "    for i in range(MAX_EVALS):\n",
    "        hyperparams = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        try:\n",
    "            new_results = objective_func(X_train, X_test, X_val,  y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,  \n",
    "                                        train_umask, test_umask, val_umask,\n",
    "                                        train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                        rangesTrain, rangesTest, rangesDev, nodalAtt)\n",
    "            sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparams {hyperparams}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    return sub_total_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7295744e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [[256, 128], [128, 64], [64, 32]],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}\n",
    "param_grid2 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [128, 256, 512],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcf2b3",
   "metadata": {},
   "source": [
    "<h5> First find the best hyperparameter combination for the DeepClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "163a1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamTuning(X_trainSet, X_testSet, X_valSet, y_train, y_test, y_val, isSimpleFC, param_grid, indices,\n",
    "                    train_umask, test_umask, val_umask,\n",
    "                    train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                    rangesTrain, rangesTest, rangesDev, nodalAttn):\n",
    "    \n",
    "    total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    \n",
    "    for i in indices:\n",
    "        print(\"============\", dictKey[i], \"============\")\n",
    "        X_train = X_trainSet[i]\n",
    "        X_test = X_testSet[i]\n",
    "        X_val = X_valSet[i]\n",
    "\n",
    "        sub_total_results = random_search(X_train, X_test, X_val, y_train, y_test, y_val,\n",
    "                     param_grid, isSimpleFC, i,\n",
    "                     train_umask, test_umask, val_umask,\n",
    "                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                     rangesTrain, rangesTest, rangesDev, nodalAttn[i])\n",
    "\n",
    "        total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be82a8",
   "metadata": {},
   "source": [
    "Uncomment below next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3e1fd4d3",
   "metadata": {
    "code_folding": [
     3,
     6
    ]
   },
   "outputs": [],
   "source": [
    "# file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/deep_classifier_tuned_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         total_results1_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     total_results1 = hyperparamTuning(selectedTrainDeepList, selectedTestDeepList, selectedValDeepList, \\\n",
    "#                                  y_train, y_test, y_val, False, param_grid1, indices1)\n",
    "    \n",
    "#     total_results1_sorted = total_results1.sort_values(by='Weighted-F1', ascending=False)\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(total_results1_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "90c5d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "# pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "# pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "\n",
    "# total_results1_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1ea75ad",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "# dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         df_results_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     results = []\n",
    "#     num_epochs = 30\n",
    "#     i = 0\n",
    "#     for trainSet, testSet, valSet in dataLoader:\n",
    "#         if isinstance(trainSet, list):\n",
    "#             trainSet = trainSet[0].squeeze(0)\n",
    "#             testSet = testSet[0].squeeze(0)\n",
    "#             valSet = valSet[0].squeeze(0)\n",
    "#         else:\n",
    "#             trainSet = trainSet.squeeze(0)\n",
    "#             testSet = testSet.squeeze(0)\n",
    "#             valSet = valSet.squeeze(0)\n",
    "\n",
    "#         X_tensor = to_tensor(trainSet)\n",
    "#         Y_tensor = to_tensor(y_train)\n",
    "\n",
    "#         print(\"Curerntly at\", dictKey[i])\n",
    "# #         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "# #                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "# #                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "# #         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "# #                                    isSimpleFC=False, i_dict=i, \n",
    "# #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "# #                                    ranges=rangesTrain )\n",
    "# #         results.append(result1)\n",
    "#         if i in selected_combination:\n",
    "#             model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                      umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                    seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#             X_tensor = to_tensor(testSet)\n",
    "#             Y_tensor = to_tensor(y_test)\n",
    "#             result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "#                                        isSimpleFC=True, i_dict=i, \n",
    "#                                        nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "#                                        ranges=rangesTest)\n",
    "#     #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#     #                                    isSimpleFC=True, i_dict=i, \n",
    "#     #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#     #                                    ranges=rangesTrain)\n",
    "#             results.append(result2)\n",
    "#         else:\n",
    "#             print(\"This is skipped\")\n",
    "#         i = i+1\n",
    "\n",
    "#     columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "#     df_results = pd.DataFrame(results, columns=columns)\n",
    "#     df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6d089c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ bert ============\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 9.\n",
      "Loss difference (0.0008873431461738357) is below the threshold (0.001).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_2984\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.42613636363636365, 'recall': 0.3605769230769231, 'f1-score': 0.390625, 'support': 208.0}, 'disgust': {'precision': 0.5, 'recall': 0.06451612903225806, 'f1-score': 0.11428571428571428, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.5308056872037915, 'recall': 0.5233644859813084, 'f1-score': 0.5270588235294118, 'support': 214.0}, 'neutral': {'precision': 0.6335329341317365, 'recall': 0.8816666666666667, 'f1-score': 0.7372822299651568, 'support': 600.0}, 'sadness': {'precision': 0.41304347826086957, 'recall': 0.1310344827586207, 'f1-score': 0.19895287958115182, 'support': 145.0}, 'surprise': {'precision': 0.5873015873015873, 'recall': 0.5285714285714286, 'f1-score': 0.556390977443609, 'support': 210.0}, 'accuracy': 0.5800273597811217, 'macro avg': {'precision': 0.44154572150490695, 'recall': 0.35567573086960075, 'f1-score': 0.360656517829292, 'support': 1462.0}, 'weighted avg': {'precision': 0.53424978034075, 'recall': 0.5800273597811217, 'f1-score': 0.5373755514088032, 'support': 1462.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.5486284289276808, 'recall': 0.4263565891472868, 'f1-score': 0.4798255179934569, 'support': 516.0}, 'disgust': {'precision': 1.0, 'recall': 0.04040404040404041, 'f1-score': 0.07766990291262135, 'support': 99.0}, 'fear': {'precision': 0.5, 'recall': 0.05, 'f1-score': 0.09090909090909091, 'support': 60.0}, 'joy': {'precision': 0.5266524520255863, 'recall': 0.498989898989899, 'f1-score': 0.5124481327800829, 'support': 495.0}, 'neutral': {'precision': 0.6848859315589354, 'recall': 0.8922600619195047, 'f1-score': 0.7749394998655552, 'support': 1615.0}, 'sadness': {'precision': 0.42105263157894735, 'recall': 0.12167300380228137, 'f1-score': 0.1887905604719764, 'support': 263.0}, 'surprise': {'precision': 0.538235294117647, 'recall': 0.5198863636363636, 'f1-score': 0.5289017341040463, 'support': 352.0}, 'accuracy': 0.6264705882352941, 'macro avg': {'precision': 0.6027792483155424, 'recall': 0.36422427969991084, 'f1-score': 0.37906920557668994, 'support': 3400.0}, 'weighted avg': {'precision': 0.6114916700534424, 'recall': 0.6264705882352941, 'f1-score': 0.5887494938074369, 'support': 3400.0}}\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.45161290322580644, 'recall': 0.33653846153846156, 'f1-score': 0.3856749311294766, 'support': 208.0}, 'disgust': {'precision': 1.0, 'recall': 0.06451612903225806, 'f1-score': 0.12121212121212122, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.502262443438914, 'recall': 0.5186915887850467, 'f1-score': 0.5103448275862069, 'support': 214.0}, 'neutral': {'precision': 0.6457073760580411, 'recall': 0.89, 'f1-score': 0.7484232655921513, 'support': 600.0}, 'sadness': {'precision': 0.48, 'recall': 0.16551724137931034, 'f1-score': 0.24615384615384617, 'support': 145.0}, 'surprise': {'precision': 0.5845410628019324, 'recall': 0.5761904761904761, 'f1-score': 0.580335731414868, 'support': 210.0}, 'accuracy': 0.5896032831737346, 'macro avg': {'precision': 0.5234462550749562, 'recall': 0.3644934138465076, 'f1-score': 0.37030638901266716, 'support': 1462.0}, 'weighted avg': {'precision': 0.5555387794734102, 'recall': 0.5896032831737346, 'f1-score': 0.5470646547063447, 'support': 1462.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.5517241379310345, 'recall': 0.37209302325581395, 'f1-score': 0.4444444444444444, 'support': 516.0}, 'disgust': {'precision': 1.0, 'recall': 0.020202020202020204, 'f1-score': 0.039603960396039604, 'support': 99.0}, 'fear': {'precision': 0.6, 'recall': 0.05, 'f1-score': 0.09230769230769231, 'support': 60.0}, 'joy': {'precision': 0.5196850393700787, 'recall': 0.5333333333333333, 'f1-score': 0.5264207377866401, 'support': 495.0}, 'neutral': {'precision': 0.6942148760330579, 'recall': 0.8842105263157894, 'f1-score': 0.7777777777777778, 'support': 1615.0}, 'sadness': {'precision': 0.40384615384615385, 'recall': 0.1596958174904943, 'f1-score': 0.22888283378746593, 'support': 263.0}, 'surprise': {'precision': 0.4973404255319149, 'recall': 0.53125, 'f1-score': 0.5137362637362637, 'support': 352.0}, 'accuracy': 0.6229411764705882, 'macro avg': {'precision': 0.6095443761017485, 'recall': 0.3643978172282073, 'f1-score': 0.3747391014623319, 'support': 3400.0}, 'weighted avg': {'precision': 0.6115782772655188, 'recall': 0.6229411764705882, 'f1-score': 0.5872097980552262, 'support': 3400.0}}\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 7.\n",
      "Loss difference (0.0007081887850137747) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 214.0}, 'neutral': {'precision': 0.4103967168262654, 'recall': 1.0, 'f1-score': 0.5819592628516004, 'support': 600.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 145.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 210.0}, 'accuracy': 0.4103967168262654, 'macro avg': {'precision': 0.0586281024037522, 'recall': 0.14285714285714285, 'f1-score': 0.08313703755022862, 'support': 1462.0}, 'weighted avg': {'precision': 0.16842546518177787, 'recall': 0.4103967168262654, 'f1-score': 0.2388341708009304, 'support': 1462.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 495.0}, 'neutral': {'precision': 0.475, 'recall': 1.0, 'f1-score': 0.6440677966101694, 'support': 1615.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 263.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 352.0}, 'accuracy': 0.475, 'macro avg': {'precision': 0.06785714285714285, 'recall': 0.14285714285714285, 'f1-score': 0.0920096852300242, 'support': 3400.0}, 'weighted avg': {'precision': 0.225625, 'recall': 0.475, 'f1-score': 0.30593220338983046, 'support': 3400.0}}\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.4732142857142857, 'recall': 0.2548076923076923, 'f1-score': 0.33125, 'support': 208.0}, 'disgust': {'precision': 1.0, 'recall': 0.03225806451612903, 'f1-score': 0.0625, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.4973544973544973, 'recall': 0.4392523364485981, 'f1-score': 0.4665012406947891, 'support': 214.0}, 'neutral': {'precision': 0.588683351468988, 'recall': 0.9016666666666666, 'f1-score': 0.7123107307439105, 'support': 600.0}, 'sadness': {'precision': 0.42857142857142855, 'recall': 0.06206896551724138, 'f1-score': 0.10843373493975904, 'support': 145.0}, 'surprise': {'precision': 0.5570776255707762, 'recall': 0.580952380952381, 'f1-score': 0.5687645687645687, 'support': 210.0}, 'accuracy': 0.560875512995896, 'macro avg': {'precision': 0.5064144555257107, 'recall': 0.32442944377267263, 'f1-score': 0.32139432502043247, 'support': 1462.0}, 'weighted avg': {'precision': 0.5254456930619336, 'recall': 0.560875512995896, 'f1-score': 0.5015175478535264, 'support': 1462.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.5547445255474452, 'recall': 0.29457364341085274, 'f1-score': 0.3848101265822785, 'support': 516.0}, 'disgust': {'precision': 1.0, 'recall': 0.030303030303030304, 'f1-score': 0.058823529411764705, 'support': 99.0}, 'fear': {'precision': 0.5, 'recall': 0.08333333333333333, 'f1-score': 0.14285714285714285, 'support': 60.0}, 'joy': {'precision': 0.5320088300220751, 'recall': 0.4868686868686869, 'f1-score': 0.5084388185654009, 'support': 495.0}, 'neutral': {'precision': 0.6505757307351638, 'recall': 0.9095975232198142, 'f1-score': 0.7585850761683449, 'support': 1615.0}, 'sadness': {'precision': 0.38461538461538464, 'recall': 0.03802281368821293, 'f1-score': 0.06920415224913495, 'support': 263.0}, 'surprise': {'precision': 0.48404255319148937, 'recall': 0.5170454545454546, 'f1-score': 0.5, 'support': 352.0}, 'accuracy': 0.6064705882352941, 'macro avg': {'precision': 0.5865695748730797, 'recall': 0.3371063550527693, 'f1-score': 0.3461026922620095, 'support': 3400.0}, 'weighted avg': {'precision': 0.5884732870758673, 'recall': 0.6064705882352941, 'f1-score': 0.5541028789832123, 'support': 3400.0}}\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 6.\n",
      "Loss difference (0.0007243424810678134) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.41, 'recall': 0.1971153846153846, 'f1-score': 0.2662337662337662, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.47639484978540775, 'recall': 0.5186915887850467, 'f1-score': 0.4966442953020134, 'support': 214.0}, 'neutral': {'precision': 0.5833333333333334, 'recall': 0.945, 'f1-score': 0.7213740458015268, 'support': 600.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 145.0}, 'surprise': {'precision': 0.5159235668789809, 'recall': 0.38571428571428573, 'f1-score': 0.44141689373297005, 'support': 210.0}, 'accuracy': 0.5471956224350205, 'macro avg': {'precision': 0.2836645357139603, 'recall': 0.292360179873531, 'f1-score': 0.2750955715814681, 'support': 1462.0}, 'weighted avg': {'precision': 0.44156802113451654, 'recall': 0.5471956224350205, 'f1-score': 0.47002768654999594, 'support': 1462.0}}\n",
      "Classified:  bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_2984\\1322761683.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.5792079207920792, 'recall': 0.22674418604651161, 'f1-score': 0.32590529247910865, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.45036764705882354, 'recall': 0.494949494949495, 'f1-score': 0.4716073147256978, 'support': 495.0}, 'neutral': {'precision': 0.643101970865467, 'recall': 0.9294117647058824, 'f1-score': 0.7601924537857685, 'support': 1615.0}, 'sadness': {'precision': 0.5, 'recall': 0.0038022813688212928, 'f1-score': 0.007547169811320755, 'support': 263.0}, 'surprise': {'precision': 0.48427672955974843, 'recall': 0.4375, 'f1-score': 0.4597014925373134, 'support': 352.0}, 'accuracy': 0.5935294117647059, 'macro avg': {'precision': 0.37956489546801686, 'recall': 0.29891538958153, 'f1-score': 0.2892791033341727, 'support': 3400.0}, 'weighted avg': {'precision': 0.5477583424045857, 'recall': 0.5935294117647059, 'f1-score': 0.5273892340017553, 'support': 3400.0}}\n",
      "============ egat ============\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 6.\n",
      "Loss difference (4.058421308006377e-05) is below the threshold (0.0001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 214.0}, 'neutral': {'precision': 0.4103967168262654, 'recall': 1.0, 'f1-score': 0.5819592628516004, 'support': 600.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 145.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 210.0}, 'accuracy': 0.4103967168262654, 'macro avg': {'precision': 0.0586281024037522, 'recall': 0.14285714285714285, 'f1-score': 0.08313703755022862, 'support': 1462.0}, 'weighted avg': {'precision': 0.16842546518177787, 'recall': 0.4103967168262654, 'f1-score': 0.2388341708009304, 'support': 1462.0}}\n",
      "Classified:  egat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_2984\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 495.0}, 'neutral': {'precision': 0.475, 'recall': 1.0, 'f1-score': 0.6440677966101694, 'support': 1615.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 263.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 352.0}, 'accuracy': 0.475, 'macro avg': {'precision': 0.06785714285714285, 'recall': 0.14285714285714285, 'f1-score': 0.0920096852300242, 'support': 3400.0}, 'weighted avg': {'precision': 0.225625, 'recall': 0.475, 'f1-score': 0.30593220338983046, 'support': 3400.0}}\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 6.\n",
      "Loss difference (8.351109295245962e-05) is below the threshold (0.0001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 214.0}, 'neutral': {'precision': 0.4103967168262654, 'recall': 1.0, 'f1-score': 0.5819592628516004, 'support': 600.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 145.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 210.0}, 'accuracy': 0.4103967168262654, 'macro avg': {'precision': 0.0586281024037522, 'recall': 0.14285714285714285, 'f1-score': 0.08313703755022862, 'support': 1462.0}, 'weighted avg': {'precision': 0.16842546518177787, 'recall': 0.4103967168262654, 'f1-score': 0.2388341708009304, 'support': 1462.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 495.0}, 'neutral': {'precision': 0.475, 'recall': 1.0, 'f1-score': 0.6440677966101694, 'support': 1615.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 263.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 352.0}, 'accuracy': 0.475, 'macro avg': {'precision': 0.06785714285714285, 'recall': 0.14285714285714285, 'f1-score': 0.0920096852300242, 'support': 3400.0}, 'weighted avg': {'precision': 0.225625, 'recall': 0.475, 'f1-score': 0.30593220338983046, 'support': 3400.0}}\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0006364866464494412) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 214.0}, 'neutral': {'precision': 0.4103967168262654, 'recall': 1.0, 'f1-score': 0.5819592628516004, 'support': 600.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 145.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 210.0}, 'accuracy': 0.4103967168262654, 'macro avg': {'precision': 0.0586281024037522, 'recall': 0.14285714285714285, 'f1-score': 0.08313703755022862, 'support': 1462.0}, 'weighted avg': {'precision': 0.16842546518177787, 'recall': 0.4103967168262654, 'f1-score': 0.2388341708009304, 'support': 1462.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 495.0}, 'neutral': {'precision': 0.475, 'recall': 1.0, 'f1-score': 0.6440677966101694, 'support': 1615.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 263.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 352.0}, 'accuracy': 0.475, 'macro avg': {'precision': 0.06785714285714285, 'recall': 0.14285714285714285, 'f1-score': 0.0920096852300242, 'support': 3400.0}, 'weighted avg': {'precision': 0.225625, 'recall': 0.475, 'f1-score': 0.30593220338983046, 'support': 3400.0}}\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 214.0}, 'neutral': {'precision': 0.4103967168262654, 'recall': 1.0, 'f1-score': 0.5819592628516004, 'support': 600.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 145.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 210.0}, 'accuracy': 0.4103967168262654, 'macro avg': {'precision': 0.0586281024037522, 'recall': 0.14285714285714285, 'f1-score': 0.08313703755022862, 'support': 1462.0}, 'weighted avg': {'precision': 0.16842546518177787, 'recall': 0.4103967168262654, 'f1-score': 0.2388341708009304, 'support': 1462.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 495.0}, 'neutral': {'precision': 0.475, 'recall': 1.0, 'f1-score': 0.6440677966101694, 'support': 1615.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 263.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 352.0}, 'accuracy': 0.475, 'macro avg': {'precision': 0.06785714285714285, 'recall': 0.14285714285714285, 'f1-score': 0.0920096852300242, 'support': 3400.0}, 'weighted avg': {'precision': 0.225625, 'recall': 0.475, 'f1-score': 0.30593220338983046, 'support': 3400.0}}\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0008145828856114812) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 214.0}, 'neutral': {'precision': 0.4103967168262654, 'recall': 1.0, 'f1-score': 0.5819592628516004, 'support': 600.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 145.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 210.0}, 'accuracy': 0.4103967168262654, 'macro avg': {'precision': 0.0586281024037522, 'recall': 0.14285714285714285, 'f1-score': 0.08313703755022862, 'support': 1462.0}, 'weighted avg': {'precision': 0.16842546518177787, 'recall': 0.4103967168262654, 'f1-score': 0.2388341708009304, 'support': 1462.0}}\n",
      "Classified:  egat\n",
      "{'anger': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 495.0}, 'neutral': {'precision': 0.475, 'recall': 1.0, 'f1-score': 0.6440677966101694, 'support': 1615.0}, 'sadness': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 263.0}, 'surprise': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 352.0}, 'accuracy': 0.475, 'macro avg': {'precision': 0.06785714285714285, 'recall': 0.14285714285714285, 'f1-score': 0.0920096852300242, 'support': 3400.0}, 'weighted avg': {'precision': 0.225625, 'recall': 0.475, 'f1-score': 0.30593220338983046, 'support': 3400.0}}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/simple_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results2_sorted = pickle.load(file)\n",
    "else: \n",
    "\n",
    "    total_results2 = hyperparamTuning(trainList, testList, valList, \n",
    "                                     y_train, y_test, y_val, True, param_grid2, indices2,\n",
    "                                     train_umask, test_umask, val_umask,\n",
    "                                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                     rangesTrain, rangesTest, rangesDev, nodalAttn)\n",
    "    \n",
    "    total_results2_sorted = total_results2.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results2_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7797910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.588749</td>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.379069</td>\n",
       "      <td>49.790972</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>0.587210</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>0.374739</td>\n",
       "      <td>110.768788</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.606471</td>\n",
       "      <td>0.606471</td>\n",
       "      <td>0.554103</td>\n",
       "      <td>0.606471</td>\n",
       "      <td>0.346103</td>\n",
       "      <td>255.620198</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.589603</td>\n",
       "      <td>0.589603</td>\n",
       "      <td>0.547065</td>\n",
       "      <td>0.589603</td>\n",
       "      <td>0.370306</td>\n",
       "      <td>110.768788</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.580027</td>\n",
       "      <td>0.580027</td>\n",
       "      <td>0.537376</td>\n",
       "      <td>0.580027</td>\n",
       "      <td>0.360657</td>\n",
       "      <td>49.790972</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.593529</td>\n",
       "      <td>0.593529</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>0.593529</td>\n",
       "      <td>0.289279</td>\n",
       "      <td>13.496133</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.560876</td>\n",
       "      <td>0.560876</td>\n",
       "      <td>0.501518</td>\n",
       "      <td>0.560876</td>\n",
       "      <td>0.321394</td>\n",
       "      <td>255.620198</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.547196</td>\n",
       "      <td>0.547196</td>\n",
       "      <td>0.470028</td>\n",
       "      <td>0.547196</td>\n",
       "      <td>0.275096</td>\n",
       "      <td>13.496133</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "      <td>483.034221</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "      <td>53.089077</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "      <td>55.132212</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "      <td>72.986308</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "      <td>58.851738</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.092010</td>\n",
       "      <td>11.840963</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>483.034221</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>53.089077</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>72.986308</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>11.840963</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>58.851738</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>55.132212</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_combination     typeSet isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "11             bert        test       True  0.626471  0.626471     0.588749   \n",
       "13             bert        test       True  0.622941  0.622941     0.587210   \n",
       "17             bert        test       True  0.606471  0.606471     0.554103   \n",
       "12             bert  validation       True  0.589603  0.589603     0.547065   \n",
       "10             bert  validation       True  0.580027  0.580027     0.537376   \n",
       "19             bert        test       True  0.593529  0.593529     0.527389   \n",
       "16             bert  validation       True  0.560876  0.560876     0.501518   \n",
       "18             bert  validation       True  0.547196  0.547196     0.470028   \n",
       "7              egat        test       True  0.475000  0.475000     0.305932   \n",
       "9              egat        test       True  0.475000  0.475000     0.305932   \n",
       "1              egat        test       True  0.475000  0.475000     0.305932   \n",
       "5              egat        test       True  0.475000  0.475000     0.305932   \n",
       "3              egat        test       True  0.475000  0.475000     0.305932   \n",
       "15             bert        test       True  0.475000  0.475000     0.305932   \n",
       "6              egat  validation       True  0.410397  0.410397     0.238834   \n",
       "8              egat  validation       True  0.410397  0.410397     0.238834   \n",
       "4              egat  validation       True  0.410397  0.410397     0.238834   \n",
       "14             bert  validation       True  0.410397  0.410397     0.238834   \n",
       "2              egat  validation       True  0.410397  0.410397     0.238834   \n",
       "0              egat  validation       True  0.410397  0.410397     0.238834   \n",
       "\n",
       "    F1-micro  F1-macro  train_time  \\\n",
       "11  0.626471  0.379069   49.790972   \n",
       "13  0.622941  0.374739  110.768788   \n",
       "17  0.606471  0.346103  255.620198   \n",
       "12  0.589603  0.370306  110.768788   \n",
       "10  0.580027  0.360657   49.790972   \n",
       "19  0.593529  0.289279   13.496133   \n",
       "16  0.560876  0.321394  255.620198   \n",
       "18  0.547196  0.275096   13.496133   \n",
       "7   0.475000  0.092010  483.034221   \n",
       "9   0.475000  0.092010   53.089077   \n",
       "1   0.475000  0.092010   55.132212   \n",
       "5   0.475000  0.092010   72.986308   \n",
       "3   0.475000  0.092010   58.851738   \n",
       "15  0.475000  0.092010   11.840963   \n",
       "6   0.410397  0.083137  483.034221   \n",
       "8   0.410397  0.083137   53.089077   \n",
       "4   0.410397  0.083137   72.986308   \n",
       "14  0.410397  0.083137   11.840963   \n",
       "2   0.410397  0.083137   58.851738   \n",
       "0   0.410397  0.083137   55.132212   \n",
       "\n",
       "                                                                                                                                                                                                  hyperparams  \\\n",
       "11   num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "13  num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "17   num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "12  num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "10   num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "19               num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "16   num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "18               num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "7    num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "9             num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "1               num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "5     num_epochs=30-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "3     num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "15              num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "6    num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "8             num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "4     num_epochs=30-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "14              num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "2     num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "0               num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "\n",
       "   num_epoch  \n",
       "11         9  \n",
       "13        20  \n",
       "17        30  \n",
       "12        20  \n",
       "10         9  \n",
       "19         6  \n",
       "16        30  \n",
       "18         6  \n",
       "7         20  \n",
       "9          3  \n",
       "1          6  \n",
       "5          3  \n",
       "3          6  \n",
       "15         7  \n",
       "6         20  \n",
       "8          3  \n",
       "4          3  \n",
       "14         7  \n",
       "2          6  \n",
       "0          6  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "    \n",
    "total_results2_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7f728",
   "metadata": {},
   "source": [
    "### Use the tuned result to predict the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3c6ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 8.\n",
      "Loss difference (8.681618849343264e-05) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.3755868544600939, 'recall': 0.38461538461538464, 'f1-score': 0.38004750593824227, 'support': 208.0}, 'disgust': {'precision': 0.5, 'recall': 0.06451612903225806, 'f1-score': 0.11428571428571428, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.4830508474576271, 'recall': 0.5327102803738317, 'f1-score': 0.5066666666666667, 'support': 214.0}, 'neutral': {'precision': 0.6385542168674698, 'recall': 0.8833333333333333, 'f1-score': 0.7412587412587412, 'support': 600.0}, 'sadness': {'precision': 0.40384615384615385, 'recall': 0.14482758620689656, 'f1-score': 0.2131979695431472, 'support': 145.0}, 'surprise': {'precision': 0.626984126984127, 'recall': 0.3761904761904762, 'f1-score': 0.47023809523809523, 'support': 210.0}, 'accuracy': 0.5649794801641587, 'macro avg': {'precision': 0.43257459994506736, 'recall': 0.34088474139316866, 'f1-score': 0.346527813275801, 'support': 1462.0}, 'weighted avg': {'precision': 0.5269164406145502, 'recall': 0.5649794801641587, 'f1-score': 0.5235556466372635, 'support': 1462.0}}\n",
      "Classified:  bert\n",
      "{'anger': {'precision': 0.5177453027139874, 'recall': 0.4806201550387597, 'f1-score': 0.4984924623115578, 'support': 516.0}, 'disgust': {'precision': 1.0, 'recall': 0.04040404040404041, 'f1-score': 0.07766990291262135, 'support': 99.0}, 'fear': {'precision': 0.3333333333333333, 'recall': 0.05, 'f1-score': 0.08695652173913043, 'support': 60.0}, 'joy': {'precision': 0.5271317829457365, 'recall': 0.5494949494949495, 'f1-score': 0.5380811078140455, 'support': 495.0}, 'neutral': {'precision': 0.6924191211974892, 'recall': 0.8879256965944272, 'f1-score': 0.7780792186652198, 'support': 1615.0}, 'sadness': {'precision': 0.4326923076923077, 'recall': 0.17110266159695817, 'f1-score': 0.2452316076294278, 'support': 263.0}, 'surprise': {'precision': 0.6359447004608295, 'recall': 0.39204545454545453, 'f1-score': 0.4850615114235501, 'support': 352.0}, 'accuracy': 0.6305882352941177, 'macro avg': {'precision': 0.5913237926205263, 'recall': 0.36737042252494134, 'f1-score': 0.3870817617850789, 'support': 3400.0}, 'weighted avg': {'precision': 0.6185277355817032, 'recall': 0.6305882352941177, 'f1-score': 0.596563080466286, 'support': 3400.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_2984\\4238940293.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.564979</td>\n",
       "      <td>0.564979</td>\n",
       "      <td>0.523556</td>\n",
       "      <td>0.564979</td>\n",
       "      <td>0.346528</td>\n",
       "      <td>74.815786</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.596563</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.387082</td>\n",
       "      <td>74.815786</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_combination     typeSet isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "0             bert  validation       True  0.564979  0.564979     0.523556   \n",
       "1             bert        test       True  0.630588  0.630588     0.596563   \n",
       "\n",
       "   F1-micro  F1-macro  train_time  \\\n",
       "0  0.564979  0.346528   74.815786   \n",
       "1  0.630588  0.387082   74.815786   \n",
       "\n",
       "                                         hyperparams num_epoch  \n",
       "0  num_epochs=30-loss_difference_threshold=0.001-...         8  \n",
       "1  num_epochs=30-loss_difference_threshold=0.001-...         8  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 30,\n",
    "    'loss_difference_threshold': 0.001,\n",
    "    'hidden_dims': 512,\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[0], testList[0], valList[0],  y_train, y_test, y_val, hyperparams, 0, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[0])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be810eb",
   "metadata": {},
   "source": [
    "rerun again to correct the model name in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a23a83fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (6.66746559242204e-05) is below the threshold (0.0001).\n",
      "{'anger': {'precision': 0.4178082191780822, 'recall': 0.2932692307692308, 'f1-score': 0.3446327683615819, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.49767441860465117, 'recall': 0.5, 'f1-score': 0.4988344988344988, 'support': 214.0}, 'neutral': {'precision': 0.6055734190782422, 'recall': 0.9416666666666667, 'f1-score': 0.7371167645140247, 'support': 600.0}, 'sadness': {'precision': 0.5714285714285714, 'recall': 0.05517241379310345, 'f1-score': 0.10062893081761007, 'support': 145.0}, 'surprise': {'precision': 0.564935064935065, 'recall': 0.4142857142857143, 'f1-score': 0.47802197802197804, 'support': 210.0}, 'accuracy': 0.5663474692202463, 'macro avg': {'precision': 0.3796313847463732, 'recall': 0.31491343221638785, 'f1-score': 0.30846213436424197, 'support': 1462.0}, 'weighted avg': {'precision': 0.5186347422099099, 'recall': 0.5663474692202463, 'f1-score': 0.5032011406507356, 'support': 1462.0}}\n",
      "Classified:  bert-select-more\n",
      "{'anger': {'precision': 0.5394321766561514, 'recall': 0.3313953488372093, 'f1-score': 0.4105642256902761, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.4868995633187773, 'recall': 0.4505050505050505, 'f1-score': 0.46799580272822666, 'support': 495.0}, 'neutral': {'precision': 0.6506550218340611, 'recall': 0.9226006191950464, 'f1-score': 0.7631241997439181, 'support': 1615.0}, 'sadness': {'precision': 0.6666666666666666, 'recall': 0.030418250950570342, 'f1-score': 0.05818181818181818, 'support': 263.0}, 'surprise': {'precision': 0.5108359133126935, 'recall': 0.46875, 'f1-score': 0.4888888888888889, 'support': 352.0}, 'accuracy': 0.605, 'macro avg': {'precision': 0.40778419168405, 'recall': 0.3148098956411252, 'f1-score': 0.3126792764618754, 'support': 3400.0}, 'weighted avg': {'precision': 0.5662699182584644, 'recall': 0.605, 'f1-score': 0.548042750724644, 'support': 3400.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_2984\\439966065.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.566347</td>\n",
       "      <td>0.566347</td>\n",
       "      <td>0.503201</td>\n",
       "      <td>0.566347</td>\n",
       "      <td>0.308462</td>\n",
       "      <td>16.634499</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.312679</td>\n",
       "      <td>16.634499</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_combination     typeSet isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "0  bert-select-more  validation       True  0.566347  0.566347     0.503201   \n",
       "1  bert-select-more        test       True  0.605000  0.605000     0.548043   \n",
       "\n",
       "   F1-micro  F1-macro  train_time  \\\n",
       "0  0.566347  0.308462   16.634499   \n",
       "1  0.605000  0.312679   16.634499   \n",
       "\n",
       "                                         hyperparams num_epoch  \n",
       "0  num_epochs=20-loss_difference_threshold=0.0001...         5  \n",
       "1  num_epochs=20-loss_difference_threshold=0.0001...         5  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 20,\n",
    "    'loss_difference_threshold': 0.0001,\n",
    "    'hidden_dims': 128,\n",
    "    'dropout_rate': 0.5,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[3], testList[3], valList[3],  y_train, y_test, y_val, hyperparams, 3, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[3])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7fde5ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 8.\n",
      "Loss difference (4.013472982331012e-05) is below the threshold (0.0001).\n",
      "{'anger': {'precision': 0.4, 'recall': 0.2980769230769231, 'f1-score': 0.3415977961432507, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.4557522123893805, 'recall': 0.48130841121495327, 'f1-score': 0.4681818181818182, 'support': 214.0}, 'neutral': {'precision': 0.5963003264417845, 'recall': 0.9133333333333333, 'f1-score': 0.7215273206056616, 'support': 600.0}, 'sadness': {'precision': 0.5555555555555556, 'recall': 0.034482758620689655, 'f1-score': 0.06493506493506493, 'support': 145.0}, 'surprise': {'precision': 0.5098039215686274, 'recall': 0.37142857142857144, 'f1-score': 0.4297520661157025, 'support': 210.0}, 'accuracy': 0.5444596443228454, 'macro avg': {'precision': 0.3596302879936212, 'recall': 0.29980428538206727, 'f1-score': 0.28942772371164255, 'support': 1462.0}, 'weighted avg': {'precision': 0.49666590177931963, 'recall': 0.5444596443228454, 'f1-score': 0.48141119107522845, 'support': 1462.0}}\n",
      "Classified:  bert-select-few\n",
      "{'anger': {'precision': 0.5370370370370371, 'recall': 0.3372093023255814, 'f1-score': 0.4142857142857143, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 60.0}, 'joy': {'precision': 0.468503937007874, 'recall': 0.4808080808080808, 'f1-score': 0.4745762711864407, 'support': 495.0}, 'neutral': {'precision': 0.6536412078152753, 'recall': 0.9114551083591331, 'f1-score': 0.7613136798551849, 'support': 1615.0}, 'sadness': {'precision': 0.6666666666666666, 'recall': 0.015209125475285171, 'f1-score': 0.02973977695167286, 'support': 263.0}, 'surprise': {'precision': 0.5161290322580645, 'recall': 0.45454545454545453, 'f1-score': 0.48338368580060426, 'support': 352.0}, 'accuracy': 0.6023529411764705, 'macro avg': {'precision': 0.40599684011213105, 'recall': 0.31417529593050497, 'f1-score': 0.3090427325828024, 'support': 3400.0}, 'weighted avg': {'precision': 0.5651946656587795, 'recall': 0.6023529411764705, 'f1-score': 0.5459355572102773, 'support': 3400.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_2984\\3109196931.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.544460</td>\n",
       "      <td>0.544460</td>\n",
       "      <td>0.481411</td>\n",
       "      <td>0.544460</td>\n",
       "      <td>0.289428</td>\n",
       "      <td>22.454779</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.602353</td>\n",
       "      <td>0.602353</td>\n",
       "      <td>0.545936</td>\n",
       "      <td>0.602353</td>\n",
       "      <td>0.309043</td>\n",
       "      <td>22.454779</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_combination     typeSet isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "0  bert-select-few  validation       True  0.544460  0.544460     0.481411   \n",
       "1  bert-select-few        test       True  0.602353  0.602353     0.545936   \n",
       "\n",
       "   F1-micro  F1-macro  train_time  \\\n",
       "0  0.544460  0.289428   22.454779   \n",
       "1  0.602353  0.309043   22.454779   \n",
       "\n",
       "                                         hyperparams num_epoch  \n",
       "0  num_epochs=40-loss_difference_threshold=0.0001...         8  \n",
       "1  num_epochs=40-loss_difference_threshold=0.0001...         8  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 40,\n",
    "    'loss_difference_threshold': 0.0001,\n",
    "    'hidden_dims': 128,\n",
    "    'dropout_rate': 0.5,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[1], testList[1], valList[1],  y_train, y_test, y_val, hyperparams, 1, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[1])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56b74053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 4.\n",
      "Loss difference (0.0003139507074802761) is below the threshold (0.001).\n",
      "{'anger': {'precision': 0.3950617283950617, 'recall': 0.3076923076923077, 'f1-score': 0.34594594594594597, 'support': 208.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 31.0}, 'fear': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 54.0}, 'joy': {'precision': 0.5428571428571428, 'recall': 0.4439252336448598, 'f1-score': 0.4884318766066838, 'support': 214.0}, 'neutral': {'precision': 0.5991140642303433, 'recall': 0.9016666666666666, 'f1-score': 0.7198935462408517, 'support': 600.0}, 'sadness': {'precision': 0.4666666666666667, 'recall': 0.04827586206896552, 'f1-score': 0.0875, 'support': 145.0}, 'surprise': {'precision': 0.5242718446601942, 'recall': 0.5142857142857142, 'f1-score': 0.5192307692307693, 'support': 210.0}, 'accuracy': 0.5574555403556771, 'macro avg': {'precision': 0.36113877811562983, 'recall': 0.316549397765502, 'f1-score': 0.3087145911463215, 'support': 1462.0}, 'weighted avg': {'precision': 0.5031302740500102, 'recall': 0.5574555403556771, 'f1-score': 0.4994139997493568, 'support': 1462.0}}\n",
      "Classified:  bert-select-mod\n",
      "{'anger': {'precision': 0.5054347826086957, 'recall': 0.36046511627906974, 'f1-score': 0.42081447963800905, 'support': 516.0}, 'disgust': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99.0}, 'fear': {'precision': 0.6, 'recall': 0.05, 'f1-score': 0.09230769230769231, 'support': 60.0}, 'joy': {'precision': 0.5241379310344828, 'recall': 0.46060606060606063, 'f1-score': 0.49032258064516127, 'support': 495.0}, 'neutral': {'precision': 0.6646816307833256, 'recall': 0.8984520123839009, 'f1-score': 0.7640863612427593, 'support': 1615.0}, 'sadness': {'precision': 0.6923076923076923, 'recall': 0.06844106463878327, 'f1-score': 0.1245674740484429, 'support': 263.0}, 'surprise': {'precision': 0.4856396866840731, 'recall': 0.5284090909090909, 'f1-score': 0.5061224489795918, 'support': 352.0}, 'accuracy': 0.6094117647058823, 'macro avg': {'precision': 0.49602881763118134, 'recall': 0.33805333497384366, 'f1-score': 0.3426030052659509, 'support': 3400.0}, 'weighted avg': {'precision': 0.5831575147626306, 'recall': 0.6094117647058823, 'f1-score': 0.5618541857569536, 'support': 3400.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_2984\\1846461806.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.499414</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.308715</td>\n",
       "      <td>19.460354</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>0.561854</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>0.342603</td>\n",
       "      <td>19.460354</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_combination     typeSet isSimpleFC  Accuracy    Recall  Weighted-F1  \\\n",
       "0  bert-select-mod  validation       True  0.557456  0.557456     0.499414   \n",
       "1  bert-select-mod        test       True  0.609412  0.609412     0.561854   \n",
       "\n",
       "   F1-micro  F1-macro  train_time  \\\n",
       "0  0.557456  0.308715   19.460354   \n",
       "1  0.609412  0.342603   19.460354   \n",
       "\n",
       "                                         hyperparams num_epoch  \n",
       "0  num_epochs=40-loss_difference_threshold=0.001-...         4  \n",
       "1  num_epochs=40-loss_difference_threshold=0.001-...         4  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'num_epochs': 40,\n",
    "    'loss_difference_threshold': 0.001,\n",
    "    'hidden_dims': 512,\n",
    "    'dropout_rate': 0.7,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizers': optim.Adam,\n",
    "    'criteria': nn.NLLLoss\n",
    "}\n",
    "\n",
    "tuned_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "           'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "\n",
    "new_results = objective_func(trainList[2], testList[2], valList[2],  y_train, y_test, y_val, hyperparams, 2, True,  \n",
    "                            train_umask, test_umask, val_umask,\n",
    "                            train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                            rangesTrain, rangesTest, rangesDev, nodalAttn[2])\n",
    "tuned_results = pd.concat([tuned_results, new_results], ignore_index=True)\n",
    "tuned_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
