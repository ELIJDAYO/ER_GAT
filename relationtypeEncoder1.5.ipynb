{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c64aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, math, pickle, sys, random, time\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "import dgl,numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import RGCNConv, GraphConv\n",
    "from model import DialogueGCN_MELDModel, GraphNetwork_RGCN, GraphNetwork_GAT, \\\n",
    "GraphNetwork_GAT_EdgeFeat, GraphNetwork_GATv2, GraphNetwork_GATv2_EdgeFeat, GraphNetwork_RGAT\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from graph_context_dataset import GraphContextDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f920f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153dd45",
   "metadata": {},
   "source": [
    "<b>Make sure to specify which dataset to use\n",
    "<br>\n",
    " - dataset_original\n",
    "<br>\n",
    " - dataset_drop_noise\n",
    "<br>\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a74ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa9fe91",
   "metadata": {
    "code_folding": [
     0,
     38,
     60
    ]
   },
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        torch.manual_seed(42)\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "        \n",
    "        # Edge type embeddings\n",
    "        torch.manual_seed(42)\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "        \n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of neighbor node representations\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).mean(dim=2)\n",
    "\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores\n",
    "\n",
    "class EGATConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_node_feats,\n",
    "                 in_edge_feats,\n",
    "                 out_node_feats,\n",
    "                 out_edge_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 **kw_args):\n",
    "\n",
    "        super().__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._out_node_feats = out_node_feats\n",
    "        self._out_edge_feats = out_edge_feats\n",
    "        \n",
    "        self.fc_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=bias)\n",
    "        self.fc_ni = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_nj = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        \n",
    "        # Attention parameter\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.manual_seed(42)\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.xavier_normal_(self.fc_node.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "        init.xavier_normal_(self.attn, gain=gain)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "        with graph.local_scope():\n",
    "            graph.edata['f'] = efeats\n",
    "            graph.ndata['h'] = nfeats\n",
    "            \n",
    "            f_ni = self.fc_ni(nfeats)\n",
    "            f_nj = self.fc_nj(nfeats)\n",
    "            f_fij = self.fc_fij(efeats)\n",
    "            graph.srcdata.update({'f_ni' : f_ni})\n",
    "            graph.dstdata.update({'f_nj' : f_nj})\n",
    "            \n",
    "            graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "            f_out = graph.edata.pop('f_tmp') + f_fij\n",
    "            \n",
    "            if self.bias is not None:\n",
    "                f_out += self.bias\n",
    "            f_out = nn.functional.leaky_relu(f_out)\n",
    "            f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "            \n",
    "            e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.edata['a'] = edge_softmax(graph, e)\n",
    "            graph.ndata['h_out'] = self.fc_node(nfeats).view(-1, self._num_heads, self._out_node_feats)\n",
    "            \n",
    "            graph.update_all(fn.u_mul_e('h_out', 'a', 'm'), fn.sum('m', 'h_out'))\n",
    "\n",
    "            h_out = graph.ndata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "            if get_attention:\n",
    "                return h_out, f_out, graph.edata.pop('a')\n",
    "            else:\n",
    "                return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7adf37e",
   "metadata": {
    "code_folding": [
     0,
     11,
     29,
     31,
     50
    ]
   },
   "outputs": [],
   "source": [
    "def get_ohe(edge_types):\n",
    "    one_hot_encoding = []\n",
    "    for edge_type in edge_types:\n",
    "        if edge_type == 0:\n",
    "            one_hot_encoding.append([1., 0., 0.])\n",
    "        elif edge_type == 1:\n",
    "            one_hot_encoding.append([0., 1., 0.])\n",
    "        elif edge_type == 2:\n",
    "            one_hot_encoding.append([0., 0., 1.])\n",
    "    return torch.tensor(one_hot_encoding)\n",
    "\n",
    "def get_inferred_edgetypes_GAT(dialog, edge_types):\n",
    "    inferred_edge_types = []\n",
    "    inferred_edge_indices = []\n",
    "    for target_node in dialog.values():\n",
    "        if len(target_node) == 1:\n",
    "            inferred_edge_types.append(0)\n",
    "            inferred_edge_indices.append(0)\n",
    "        else:\n",
    "            edge_index = target_node[0][0]\n",
    "            highest_attention = target_node[0][1]\n",
    "            for src_node in target_node[1:]:\n",
    "                if highest_attention < src_node[1]:\n",
    "                    highest_attention = src_node[1]\n",
    "                    edge_index = src_node[0]\n",
    "            inferred_edge_indices.append(edge_index)\n",
    "            inferred_edge_types.append(edge_types[edge_index].tolist())\n",
    "    return inferred_edge_indices, inferred_edge_types\n",
    "\n",
    "def get_inferred_edgetypes_EGAT(edges_target_nodes, sample_edge_types, size_dialog, dialog_id):\n",
    "    inferred_edge_types = []\n",
    "    for target_idx in range(size_dialog):\n",
    "        num_edges = len(edges_target_nodes[target_idx])\n",
    "        if num_edges == 1:\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "        else:\n",
    "            highest_attn_score = max(edges_target_nodes[target_idx][0][1])\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            for sample_edge in range(1, num_edges):\n",
    "                cur_highest_attn_score = max(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                if cur_highest_attn_score > highest_attn_score:\n",
    "                    highest_attn_score = cur_highest_attn_score\n",
    "                    edgetype_idx = np.argmax(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                    edge_idx = edges_target_nodes[target_idx][sample_edge][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "    return inferred_edge_types\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7639e77",
   "metadata": {
    "code_folding": [
     0,
     15,
     25,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 0\n",
    "        while (val <= 3) and (i+val <= end_idx):\n",
    "            target_idx = i+val\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "            val = val+1\n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "    return adjacency_list_dict\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    for i in range(dialogs_len):\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "        for j in range(dialog_len):\n",
    "            src_node = dialog_nodes_pairs[j]\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k]\n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))\n",
    "    return whole_edge_type_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed53f3f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddb394",
   "metadata": {},
   "source": [
    "<h3> Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c424c",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Train, Test and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8a8f87",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "# encodedSpeakersTrain = []\n",
    "# rangesTrain = []\n",
    "\n",
    "# if not checkFile:\n",
    "#     print(\"Run first the contextEncoder1 or 2 to generate this file\")\n",
    "# else:\n",
    "#     with open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\") as file:\n",
    "#         encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "\n",
    "# checkFile = os.path.isfile(\"data/dump/\" + dataset_path +\"/adjListTrain.pkl\")\n",
    "# adjacencyListTrain = []\n",
    "\n",
    "# if key:\n",
    "#     adjacencyListTrain = get_all_adjacency_list(rangesTrain)\n",
    "# else:\n",
    "#     with open('data/dump/' + dataset_path + '/adjListTrain', \"rb\") as file:\n",
    "#         adjacencyListTrain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5170d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeakersAndRanges(file_path):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    encodedSpeakers = []\n",
    "#     ranges = []\n",
    "    if not checkFile:\n",
    "        print(\"Run first the contextEncoder1.5 to generate this file\")\n",
    "        return None\n",
    "    else:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            encodedSpeakers, ranges = pickle.load(file)\n",
    "        return encodedSpeakers, ranges\n",
    "    \n",
    "def getAdjacencyList(file_path, ranges):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    adjacencyList = []\n",
    "\n",
    "    if key:\n",
    "        adjacencyList = get_all_adjacency_list(ranges)\n",
    "    else:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            adjacencyList = pickle.load(file)\n",
    "    \n",
    "    return adjacencyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7be6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = \"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\"\n",
    "\n",
    "encodedSpeakersTrain, rangesTrain = getSpeakersAndRanges(file_path1)\n",
    "encodedSpeakersTest, rangesTest = getSpeakersAndRanges(file_path2)\n",
    "encodedSpeakersDev, rangesDev = getSpeakersAndRanges(file_path3)\n",
    "\n",
    "file_path1 = 'data/dump/' + dataset_path + '/adjListTrain'\n",
    "file_path2 = 'data/dump/' + dataset_path + '/adjListTest'\n",
    "file_path3 = 'data/dump/' + dataset_path + '/adjListDev'\n",
    "\n",
    "adjacencyListTrain = getAdjacencyList(file_path1, rangesTrain)\n",
    "adjacencyListTest = getAdjacencyList(file_path1, rangesTest)\n",
    "adjacencyListDev = getAdjacencyList(file_path1, rangesDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771b8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/u_prime_CNNBiLSTM_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/u_prime_CNNBiLSTM_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/u_prime_CNNBiLSTM_dev.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "def getFeatures(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        emotions = pickle.load(file)\n",
    "    return emotions\n",
    "\n",
    "contextualEmbeddingsTrain = getFeatures(file_path1)\n",
    "contextualEmbeddingsTest = getFeatures(file_path2)\n",
    "contextualEmbeddingsDev = getFeatures(file_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6698b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12840, 1, 200]) torch.Size([3400, 1, 200]) torch.Size([1462, 1, 200])\n"
     ]
    }
   ],
   "source": [
    "print(contextualEmbeddingsTrain.shape, contextualEmbeddingsTest.shape, contextualEmbeddingsDev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6055971",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain)\n",
    "edgeTypesTrain = get_all_edge_type_list(edgeIndicesTrain, encodedSpeakersTrain)\n",
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain, key=1)\n",
    "\n",
    "edgeIndicesTest = get_all_adjacency_list(rangesTest)\n",
    "edgeTypesTest = get_all_edge_type_list(edgeIndicesTest, encodedSpeakersTest)\n",
    "edgeIndicesTest = get_all_adjacency_list(rangesTest, key=1)\n",
    "\n",
    "edgeIndicesDev = get_all_adjacency_list(rangesDev)\n",
    "edgeTypesDev = get_all_edge_type_list(edgeIndicesDev, encodedSpeakersDev)\n",
    "edgeIndicesDev = get_all_adjacency_list(rangesDev, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b48a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e18118d",
   "metadata": {},
   "source": [
    "<h4> Creating \"SAMPLE\" graph features based on various graph networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a469bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rangesTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b197fe2",
   "metadata": {},
   "source": [
    "Start of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc943c7d",
   "metadata": {},
   "source": [
    "<h5>DGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9caf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.features = [torch.rand(14, 200)]\n",
    "        self.edge_index = [torch.randint(0, 14, (2, 69))]\n",
    "        self.edge_type = [torch.randint(0, 4, (69,))]\n",
    "        self.edge_index_lengths = [torch.tensor([69])]\n",
    "        self.umask = [torch.randint(0, 2, (1, 14))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.edge_index[idx], self.edge_type[idx], self.edge_index_lengths[idx], self.umask[idx])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = SampleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4874963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN Representation Shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "# D_m = 200\n",
    "# D_g = 100\n",
    "# D_p = 100\n",
    "# D_e = 100\n",
    "# D_h = 100\n",
    "# D_a = 100\n",
    "# graph_hidden_size = 64\n",
    "# n_speakers = 2\n",
    "# max_seq_len = 110\n",
    "# window_past = 0\n",
    "# window_future = 5\n",
    "# n_classes = 7\n",
    "# dropout_rec = 0.5\n",
    "# dropout = 0.5\n",
    "nodal_attention = True\n",
    "avec = False\n",
    "no_cuda = False\n",
    "\n",
    "features = torch.randn(14, 200)\n",
    "edge_index = [torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0]])]\n",
    "edge_type = [torch.tensor([1, 0, 2, 2, 2, 0, 1, 3, 2, 2, 0, 2, 2, 2])]\n",
    "seq_lengths  = torch.tensor([[14]])\n",
    "umask = torch.ones(1, 1, 14)\n",
    "\n",
    "nodal_attn = False\n",
    "avec = False\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphNetwork_RGCN(num_features=200, num_classes=7, num_relations=4, max_seq_len=14)\n",
    "gcn_representation = model(features, edge_index, edge_type, seq_lengths, umask, nodal_attn, avec)\n",
    "print(\"GCN Representation Shape:\", gcn_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3846a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d48a0",
   "metadata": {},
   "source": [
    "<h5>GAT w/o edge feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e6cdb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4  # This parameter is not used with GATConv but kept for compatibility\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_type = [torch.randint(0, num_relations, (20,))]  # Example edge types\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "928e2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862804c",
   "metadata": {},
   "source": [
    "<h5>GAT with edge feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "276cee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))\n",
    "edge_index = [torch.randint(0, 14, (2, 20))]\n",
    "edge_attr = [torch.randn((20, num_relations))]\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1869288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_attr = torch.randint(0, 2, (20, 1)).float()  # Example binary edge features\n",
    "# edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b632",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/o edgetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab557a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n",
      "Output shape with attention: torch.Size([14, 64])\n",
      "Attention weights shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2(num_features, num_classes, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Forward pass with attention weights\n",
    "out, (edge_index, attention_weights) = model(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "print(\"Output shape with attention:\", out.shape)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5781299",
   "metadata": {},
   "source": [
    "<h5>GATv2 with edge type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b5ae9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_attr = torch.randn((20, num_relations))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d91e3",
   "metadata": {},
   "source": [
    "<h5>RGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d536b5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  0,  4, 11,  2,  0,  7,  1,  2,  9,  6, 12,  2,  9,  0, 13,  9, 12,\n",
       "          4, 11],\n",
       "        [11,  7,  9,  8, 12,  8,  2, 12,  3,  6,  9,  3, 13,  5,  7, 12, 12,  3,\n",
       "          1, 12]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e05a3a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 3  # Example number of relations\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "edge_dim = 1  # Dimensionality of edge attributes\n",
    "no_cuda = False\n",
    "\n",
    "\n",
    "model = GraphNetwork_RGAT(num_features, num_classes, num_relations, hidden_size, num_heads, dropout, edge_dim, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_type=edge_type)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94a009",
   "metadata": {},
   "source": [
    "End of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317848a",
   "metadata": {},
   "source": [
    "<h4> Encode speaker to train, test, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7bc233c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class GraphContextDataset(Dataset):\n",
    "#     def __init__(self, rangeSet, labels, features, edge_index, edge_type, \\\n",
    "#                  edge_index_lengths, umask, seq_lengths):\n",
    "#         self.rangeSet = rangeSet\n",
    "#         self.labels = [torch.tensor(label) for label in labels]\n",
    "#         self.features = [torch.tensor(feature) for feature in features]\n",
    "#         self.edge_index = [torch.tensor(edge) for edge in edge_index]\n",
    "#         self.edge_type = [torch.tensor(edge) for edge in edge_type]\n",
    "#         self.edge_index_lengths = [torch.tensor(length) for length in edge_index_lengths]\n",
    "#         self.umask = umask\n",
    "#         self.seq_lengths = seq_lengths\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.rangeSet)  # Use rangeSet for length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         startIdx, endIdx = self.rangeSet[idx]\n",
    "#         return (\n",
    "#             self.labels[startIdx: endIdx+1],\n",
    "#             self.features[idx],\n",
    "#             self.edge_index[idx],\n",
    "#             self.edge_type[idx],\n",
    "#             self.edge_index_lengths[idx],\n",
    "#             self.umask[idx],\n",
    "#             self.seq_lengths[idx]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbbdf11a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# file_path1 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl'\n",
    "# file_path2 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl'\n",
    "# file_path3 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl'\n",
    "\n",
    "# with open(file_path1, 'rb') as file1:\n",
    "#      all_umask, \\\n",
    "#      all_seq_lengths,\\\n",
    "#      all_features, \\\n",
    "#      all_edge_index, \\\n",
    "#      all_edge_norm, \\\n",
    "#      all_edge_type, \\\n",
    "#      all_edge_index_lengths = pickle.load(file1)\n",
    "\n",
    "# file_path2 = 'data/dump/train_labels.pkl'\n",
    "# with open(file_path2, 'rb') as file2:\n",
    "#     trainLabels = pickle.load(file2)\n",
    "\n",
    "# trainDataset = GraphContextDataset(rangesTrain, trainLabels,\n",
    "#                                    all_features, all_edge_index,\n",
    "#                                    all_edge_type,\n",
    "#                                    all_edge_index_lengths,\n",
    "#                                    all_umask, all_seq_lengths)\n",
    "# dataLoader = DataLoader(trainDataset, batch_size=1, shuffle=False, num_workers=4)  # Use num_workers for parallel data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69ad33d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.features = [torch.tensor(feature) for feature in features]\n",
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_index = [torch.tensor(edge) for edge in edge_index]\n",
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_type = [torch.tensor(edge) for edge in edge_type]\n"
     ]
    }
   ],
   "source": [
    "def getDataLoaderAndLabels(file_path, ranges):\n",
    "    with open(file_path[0], 'rb') as file:\n",
    "         all_umask, \\\n",
    "         all_seq_lengths,\\\n",
    "         all_features, \\\n",
    "         all_edge_index, \\\n",
    "         all_edge_norm, \\\n",
    "         all_edge_type, \\\n",
    "         all_edge_index_lengths = pickle.load(file)\n",
    "\n",
    "    with open(file_path[1], 'rb') as file:\n",
    "        labels = pickle.load(file)\n",
    "\n",
    "    dataset = GraphContextDataset(ranges, labels,\n",
    "                                       all_features, all_edge_index,\n",
    "                                       all_edge_type,\n",
    "                                       all_edge_index_lengths,\n",
    "                                       all_umask, all_seq_lengths)\n",
    "    dataLoader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "    return dataLoader, labels\n",
    "\n",
    "file_path1 = ['embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl', 'data/dump/' + dataset_path + '/labels_train.pkl']\n",
    "file_path2 = ['embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl' , 'data/dump/' + dataset_path + '/labels_test.pkl']\n",
    "file_path3 = ['embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl', 'data/dump/' + dataset_path + '/labels_dev.pkl']\n",
    "\n",
    "dataLoaderTrain, trainLabels = getDataLoaderAndLabels(file_path1, rangesTrain)\n",
    "dataLoaderTest, testLabels = getDataLoaderAndLabels(file_path2, rangesTest)\n",
    "dataLoaderDev, devLabels = getDataLoaderAndLabels(file_path3, rangesDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76fc8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RelationEncoding(file_path, dataLoader, model, config):\n",
    "    all_h_prime = []\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    j = 1\n",
    "    for _, features_in, edge_index_in, edge_type_in, _, umask, seq_lengths_in in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        if config == \"dgcn\":\n",
    "            avec, no_cuda, nodal_attn = False, False, False\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            seq_lengths = torch.tensor(seq_lengths_in).view(1, 1)\n",
    "            graph_representation = model(feature, [edge_index], [edge_type], seq_lengths, umask, nodal_attn, avec)\n",
    "        elif config == \"GATv1_noAttn\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            graph_representation = model(feature, [edge_index])\n",
    "            \n",
    "        elif config == \"GATv1\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            num_edge_types = 8\n",
    "            edge_attr = torch.zeros((edge_type.size(0), num_edge_types))\n",
    "            edge_attr.scatter_(1, edge_type.view(-1, 1), 1)\n",
    "            graph_representation = model(feature, [edge_index], [edge_attr])\n",
    "            \n",
    "        elif config == \"GATv2_noAttn\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            graph_representation = model(feature, edge_index)\n",
    "        \n",
    "        elif config == \"GATv2\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            num_edge_types = 8\n",
    "            edge_attr = torch.zeros((edge_type.size(0), num_edge_types))\n",
    "            edge_attr.scatter_(1, edge_type.view(-1, 1), 1)\n",
    "            graph_representation = model(feature, edge_index, edge_attr)\n",
    "        \n",
    "        elif config == \"RGAT\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            graph_representation = model(feature, edge_index, edge_type)\n",
    "        \n",
    "        all_h_prime.append(graph_representation.cpu())\n",
    "        \n",
    "        i = i + 1\n",
    "        if i % 500 == 0 and config == \"RGAT\":\n",
    "            pt_file_path = file_path + str(j) + \".pkl\"\n",
    "            with open(pt_file_path, 'wb') as file:  # Corrected the file path\n",
    "                pickle.dump(all_h_prime, file)\n",
    "            all_h_prime = []\n",
    "            j = j + 1\n",
    "            \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train\", config)\n",
    "    \n",
    "    if config == \"RGAT\":\n",
    "        pt_file_path = file_path + str(j) + \".pkl\"\n",
    "        with open(pt_file_path, 'wb') as file:\n",
    "            pickle.dump(all_h_prime, file)\n",
    "    else:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(all_h_prime, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3073ff8",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# features = torch.randn(14, 200)\n",
    "# edge_index = [torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "#                             [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0]])]\n",
    "# edge_type = [torch.tensor([1, 0, 2, 2, 2, 0, 1, 3, 2, 2, 0, 2, 2, 2])]\n",
    "# seq_lengths  = torch.tensor([[14]])\n",
    "# umask = torch.ones(1, 1, 14)\n",
    "# print(\"features: \", features.shape)\n",
    "# print(\"edge_index.shape: \", edge_index[0].shape)\n",
    "# print(\"edge_index: \", edge_index)\n",
    "# print(\"edge_type.shape: \", edge_type[0].shape)\n",
    "# print(\"edge_type: \", edge_type)\n",
    "# print(\"seq_lengths.shape: \", seq_lengths.shape)\n",
    "# print(\"seq_lengths: \", seq_lengths)\n",
    "# print(\"umask.shape: \", umask.shape)\n",
    "# print(\"umask: \", umask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d64f3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# edge_type_mapping = {}\n",
    "\n",
    "# for j in range(2):\n",
    "#     for k in range(2):\n",
    "#         edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "#         edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)\n",
    "# edge_type_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648061a8",
   "metadata": {},
   "source": [
    "<h5>DGCN_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75c77723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:24<00:00, 88.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 24.423157691955566 seconds to encode train dgcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:11<00:00, 50.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 11.41119384765625 seconds to encode train dgcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:09<00:00, 29.95batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9.017629146575928 seconds to encode train dgcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_DGCN_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_DGCN_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_DGCN_dev.pkl'\n",
    "\n",
    "model = GraphNetwork_RGCN(num_features=200, num_classes=7, num_relations=8, max_seq_len=30)\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"dgcn\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"dgcn\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"dgcn\")\n",
    "\n",
    "# gcn_representation = model(features, edge_index, edge_type, seq_lengths, umask, nodal_attn, avec)\n",
    "# print(\"GCN Representation Shape:\", gcn_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46527c",
   "metadata": {},
   "source": [
    "<h5>GAT w/o edge_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0bcdb",
   "metadata": {},
   "source": [
    "reviwing the types and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31e9a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_edge_type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18aa4149",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_attr = [torch.randn((20, num_relations))]  # Example edge features\n",
    "# print(\"x.shape: \", x.shape)\n",
    "# print(\"edge_index.shape: \", edge_index[0].shape)\n",
    "# print(\"edge_attr.shape: \", edge_attr[0].shape)\n",
    "# print(\"x: \", x)\n",
    "# print(\"edge_index: \", edge_index)\n",
    "# print(\"edge_attr: \", edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "758ba04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:27<00:00, 79.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 27.22583794593811 seconds to encode train GATv1_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:11<00:00, 49.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 11.748422384262085 seconds to encode train GATv1_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:09<00:00, 27.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9.770514249801636 seconds to encode train GATv1_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_dev.pkl'\n",
    "\n",
    "model = GraphNetwork_GAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv1_noAttn\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv1_noAttn\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv1_noAttn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb39da0",
   "metadata": {},
   "source": [
    "<h5>GAT w/ edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01092d30",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_attr = [torch.randn((20, num_relations))]  # Example edge features\n",
    "# print(edge_index[0].shape)\n",
    "# print(edge_attr[0].shape)\n",
    "# print(edge_index)\n",
    "# print(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "195e2641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:25<00:00, 84.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 25.52353549003601 seconds to encode train GATv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:10<00:00, 54.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.63730525970459 seconds to encode train GATv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:08<00:00, 30.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8.825482368469238 seconds to encode train GATv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 8  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_edgeAttr_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_edgeAttr_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_edgeAttr_dev.pkl'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv1\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv1\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06c4f7",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/o edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3525a0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "# print(x.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(edge_attr.shape)\n",
    "# print(x)\n",
    "# print(edge_index)\n",
    "# print(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a40b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:25<00:00, 85.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 25.33076047897339 seconds to encode train GATv2_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:10<00:00, 53.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.841038227081299 seconds to encode train GATv2_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:09<00:00, 29.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9.120386123657227 seconds to encode train GATv2_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "model = GraphNetwork_GATv2(num_features, num_classes, hidden_size, num_heads, dropout, no_cuda)\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_dev.pkl'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv2_noAttn\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv2_noAttn\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv2_noAttn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb8298",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/ edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea9a6dcd",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_attr = torch.randn((20, num_relations))  # Example edge features\n",
    "# print(x.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "245ee655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:30<00:00, 70.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 30.588493585586548 seconds to encode train GATv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:12<00:00, 46.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 12.483478784561157 seconds to encode train GATv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:09<00:00, 27.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9.955040216445923 seconds to encode train GATv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 8\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_edgeAttr_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_edgeAttr_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_edgeAttr_dev.pkl'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv2\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv2\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609069ea",
   "metadata": {},
   "source": [
    "<h5> RGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db77a2c4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "# # edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "# print(edge_type.shape)\n",
    "# print(edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b92fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [01:46<00:00, 20.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 106.3809425830841 seconds to encode train RGAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:30<00:00, 18.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 30.93975305557251 seconds to encode train RGAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:14<00:00, 19.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 14.052267789840698 seconds to encode train RGAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 8\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "edge_dim = 1  # Dimensionality of edge attributes\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_RGAT(num_features, num_classes, num_relations, hidden_size, num_heads, dropout, edge_dim, no_cuda)\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_train'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_test'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_dev'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"RGAT\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"RGAT\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"RGAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36a4c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinePartitionedData(file_paths, output_file_path):\n",
    "    combined_data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    data = pickle.load(file)\n",
    "                    combined_data.extend(data)\n",
    "            except (pickle.UnpicklingError, EOFError) as e:\n",
    "                print(f\"Error loading data from {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred while processing {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {file_path} does not exist and will be skipped.\")\n",
    "\n",
    "    # Save the combined data to a new pickle file\n",
    "    try:\n",
    "        with open(output_file_path, 'wb') as file:\n",
    "            pickle.dump(combined_data, file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving combined data to {output_file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Delete the original files\n",
    "    for file_path in file_paths:\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except OSError as e:\n",
    "                print(f\"Error deleting file {file_path}: {e}\")\n",
    "        \n",
    "file_paths1 = ['embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_train1.pkl', 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_train2.pkl', 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_train3.pkl', 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_train4.pkl', 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_train5.pkl']\n",
    "file_paths2 = ['embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_test1.pkl', 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_test2.pkl']\n",
    "file_paths3 = ['embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_dev1.pkl']\n",
    "output_file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_train.pkl'\n",
    "output_file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_test.pkl'\n",
    "output_file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_RGAT_dev.pkl'\n",
    "\n",
    "combinePartitionedData(file_paths1, output_file_path1)\n",
    "combinePartitionedData(file_paths2, output_file_path2)\n",
    "combinePartitionedData(file_paths3, output_file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016df95",
   "metadata": {},
   "source": [
    "end of encoding train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da874fb",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # change D_m into\n",
    "# D_m = 100\n",
    "# D_g = 150\n",
    "# D_p = 150\n",
    "# D_e = 100\n",
    "# D_h = 100\n",
    "# D_a = 100\n",
    "# graph_h=100\n",
    "# seed_everything()\n",
    "# model = DialogueGCN_MELDModel(\n",
    "#                                D_m, D_g, D_p, D_e, D_h, D_a, graph_h,\n",
    "#                                n_speakers=2,\n",
    "#                                max_seq_len=110,\n",
    "#                                window_past=0,\n",
    "#                                window_future=5,\n",
    "#                                n_classes=7,\n",
    "#                                listener_state=False,\n",
    "#                                context_attention='general',\n",
    "#                                dropout=0.5,\n",
    "#                                nodal_attention=False,\n",
    "#                                no_cuda=False\n",
    "#                                )\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e759176",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# num_features = 200\n",
    "# num_classes = 7\n",
    "# num_relations = 3  # Example number of relations\n",
    "# max_seq_len = 30\n",
    "# hidden_size = 64\n",
    "# num_heads = 8\n",
    "# dropout = 0.5\n",
    "# no_cuda = False\n",
    "\n",
    "# model = GraphNetwork4WithRGAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# # Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "# edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# # Forward pass\n",
    "# out = model(x, edge_index, edge_type=edge_type, edge_attr=edge_attr)\n",
    "# print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18936917",
   "metadata": {
    "code_folding": [
     0,
     69
    ]
   },
   "outputs": [],
   "source": [
    "# def train_or_eval_graph_model(model, loss_function, dataloader, epoch, cuda, optimizer=None, train=False):\n",
    "#     losses, preds, labels = [], [], []\n",
    "#     scores, vids = [], []\n",
    "    \n",
    "#     ei, et, en, el = torch.empty(0).type(torch.LongTensor), torch.empty(0).type(torch.LongTensor), torch.empty(0), []\n",
    "    \n",
    "#     assert not train or optimizer != None\n",
    "#     if train:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "        \n",
    "#     seed_everything()\n",
    "#     for data in dataloader:\n",
    "#         if train:\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#         labels, features, edge_index, edge_type, edge_index_lengths, umask, _ = data\n",
    "#         print(\"features: \", features[0].shape)\n",
    "#         print(\"edge_index: \", edge_index[0].shape)\n",
    "# #         print(edge_norm[0].shape)\n",
    "#         print(\"edge_type: \", edge_type[0].shape)\n",
    "#         print(\"edge_index_lengths: \", edge_index_lengths[0])\n",
    "#         print(\"umask: \", umask[0].shape)\n",
    "#         print(\"-------------------------------\")\n",
    "#         log_prob, e_i, e_n, e_t, e_l = model(features, edge_index,\n",
    "# #                                              edge_norm,\n",
    "#                                              edge_type, edge_index_lengths, umask)\n",
    "#         label = torch.cat([label[j][:lengths[j]] for j in range(len(label))])\n",
    "#         loss = loss_function(log_prob, label)\n",
    "\n",
    "#         ei = torch.cat([ei, e_i], dim=1)\n",
    "#         et = torch.cat([et, e_t])\n",
    "#         en = torch.cat([en, e_n])\n",
    "#         el += e_l\n",
    "\n",
    "#         preds.append(torch.argmax(log_prob, 1).cpu().numpy())\n",
    "#         labels.append(label.cpu().numpy())\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         if train:\n",
    "#             loss.backward()\n",
    "#             if args.tensorboard:\n",
    "#                 for param in model.named_parameters():\n",
    "#                     writer.add_histogram(param[0], param[1].grad, epoch)\n",
    "#             optimizer.step()\n",
    "            \n",
    "#     if preds != []:\n",
    "#         preds = np.concatenate(preds)\n",
    "#         labels = np.concatenate(labels)\n",
    "#     else:\n",
    "#         return float('nan'), float('nan'), [], [], float('nan'), [], [], [], [], []\n",
    "    \n",
    "#     ei = ei.data.cpu().numpy()\n",
    "#     et = et.data.cpu().numpy()\n",
    "#     en = en.data.cpu().numpy()\n",
    "#     el = np.array(el)\n",
    "#     labels = np.array(labels)\n",
    "#     preds = np.array(preds)\n",
    "\n",
    "#     avg_loss = round(np.sum(losses) / len(losses), 4)\n",
    "#     avg_accuracy = round(accuracy_score(labels, preds) * 100, 2)\n",
    "#     avg_fscore = round(f1_score(labels, preds, average='macro') * 100, 2)\n",
    "#     # Add precision and recall\n",
    "#     precision = round(precision_score(labels, preds, average='macro') * 100, 2)\n",
    "#     recall = round(recall_score(labels, preds, average='macro') * 100, 2)\n",
    "#     return avg_loss, avg_accuracy, labels, preds, avg_fscore, ei, et, en, el, precision, recall\n",
    "    \n",
    "    \n",
    "# for e in range(60):\n",
    "#     train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_graph_model(model, loss_function,\n",
    "#                                                                           dataloader, e,\n",
    "#                                                                           optimizer, True)\n",
    "#     print(\n",
    "#     'epoch: {}, train_loss: {}, train_acc: {}, train_fscore: {}, train_precision: {}, train_recall: {}, '. \\\n",
    "#     format(e + 1, train_loss, train_acc, train_fscore, train_precision, train_recall))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871b0b4",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data (NO NEED FOR THIS SECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de3baa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/speaker_encoder_test.pkl\")\n",
    "# encodedSpeakersTest = []\n",
    "# rangesTest = []\n",
    "\n",
    "# if not checkFile:\n",
    "#     print(\"Run first the contextEncoder2 to generate this file\")\n",
    "# else:\n",
    "#     with open('data/dump/speaker_encoder_test.pkl', \"rb\") as file:\n",
    "#         encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "\n",
    "# checkFile = os.path.isfile(\"data/dump/adjListTest.pkl\")\n",
    "# adjacencyListTest = []\n",
    "\n",
    "# if key:\n",
    "#     adjacencyListTest = get_all_adjacency_list(rangesTest)\n",
    "# else:\n",
    "#     with open('data/dump/adjListTest', \"rb\") as file:\n",
    "#         adjacencyListTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0b9fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "\n",
    "# # Load the list from the file using pickle\n",
    "# with open(file_path, 'rb') as file:\n",
    "#     contextualEmbeddingsTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29deca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edgeIndicesTest = get_all_adjacency_list(rangesTest)\n",
    "# edgeTypesTest = get_all_edge_type_list(edgeIndicesTest, encodedSpeakersTest)\n",
    "# edgeIndicesTest = get_all_adjacency_list(rangesTest, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e5796",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0634150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO repeat the one above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36c94f",
   "metadata": {},
   "source": [
    "<h3> Get GAT output from each set of data (DISCONTINUED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f024704",
   "metadata": {},
   "source": [
    "<h4> Instantiating the GAT (1st implementation) for 1 sample train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cacafa9a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# num_in_features = len(contextualEmbeddingsTrain[0][0])\n",
    "# num_out_features = len(contextualEmbeddingsTrain[0][0])\n",
    "# num_heads = 4\n",
    "# num_edge_types = 3\n",
    "# gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98a8775c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i = 0  # dialogue id\n",
    "# relationalEmbedding, attentionCoef = gat_layer((contextualEmbeddingsTrain[i], edgeIndicesTrain[i]), edgeTypesTrain[i])\n",
    "# print(\"h_prime shape: \", relationalEmbedding.shape, \"attention_coef shape: \", attentionCoef.shape)\n",
    "\n",
    "# targetNodes = edgeIndicesTrain[i][1].tolist()\n",
    "\n",
    "# sample = {}\n",
    "# sampleEdgetypes = []\n",
    "\n",
    "# for target_i in sorted(set(targetNodes)):\n",
    "#     sample[target_i] = []\n",
    "\n",
    "# for targetNode, idx in zip(targetNodes, range(len(targetNodes))):\n",
    "#     sample[targetNode].append([idx, relationalEmbedding[targetNode][idx].tolist()])\n",
    "\n",
    "# listEdgeIdxTrain, inferredEdgeTypes = get_inferred_edgetypes_GAT(sample, edgeTypesTrain[i])\n",
    "# sampleEdgetypes.append(inferredEdgeTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc9c28c3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "label_decoder = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "label_decoder = list(label_decoder.values())\n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6003c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/labels_train.pkl\")\n",
    "\n",
    "# if checkFile is False:\n",
    "#     print(\"Please run the contextEncoder1 notebook to save the label file\")\n",
    "# else:\n",
    "#     file = open('data/dump/labels_train.pkl', 'rb')\n",
    "#     y_train = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f789e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# y_train[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3e8e1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/labels_test.pkl\")\n",
    "\n",
    "# if checkFile is False:\n",
    "#     print(\"Please run the contextEncoder2 notebook to save the label file\")\n",
    "# else:\n",
    "#     file = open('data/dump/labels_test.pkl', 'rb')\n",
    "#     y_test = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223125",
   "metadata": {},
   "source": [
    "<h5>Unsupervised Visualizarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a41a39",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming h_prime contains the node embeddings\n",
    "# utt_size = 13\n",
    "# labels = torch.tensor(y_train[:utt_size + 1])\n",
    "\n",
    "# cherrypicked_nodes = []\n",
    "# for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#     cherrypicked_nodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "# cherrypicked_nodes = torch.tensor(cherrypicked_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a97231",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# h_prime_np = cherrypicked_nodes.detach().numpy()\n",
    "\n",
    "# # Perform dimensionality reduction using t-SNE\n",
    "# tsne = TSNE(n_components=3, perplexity=5, random_state=42)\n",
    "# h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# # Plot the node embeddings with different colors for each label\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#     indices = (labels == label).nonzero().squeeze()\n",
    "#     plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "# plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "# plt.xlabel('Dimension 1', color=\"white\")\n",
    "# plt.ylabel('Dimension 2', color=\"white\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8043a6a",
   "metadata": {},
   "source": [
    "<h4> Now get new representations of all train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588776d9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "# #            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "# #            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "\n",
    "# def get_GAT_representation(filePath, contextualEmbeddings, edgeIndices, edgeTypes):\n",
    "# #     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-GAT_train.pkl\") #replace it with key when deployed\n",
    "#     if key:\n",
    "#         print(\"Start of getting output of 1st GAT\")\n",
    "#         allInferredEdgetypes = []\n",
    "#         listAllEdgeIdx = []\n",
    "#         cherrypickedNodes = []\n",
    "#         for dialog, dialog_id in zip(contextualEmbeddings, range(len(contextualEmbeddings))):\n",
    "#             h_prime, attention_coef = gat_layer((dialog, edgeIndices[dialog_id]), edgeTypes[dialog_id])\n",
    "#             target_nodes = edgeIndices[dialog_id][1].tolist() # first idx represents dialogue id\n",
    "\n",
    "#             sample_edgetypes = {}\n",
    "#             for i in set(target_nodes):\n",
    "#                 sample_edgetypes[i] = []\n",
    "\n",
    "#             for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "#                 sample_edgetypes[target_node].append([edge_idx, h_prime[target_node][edge_idx].tolist()])\n",
    "\n",
    "#             list_edge_idx, inferred_edgetypes = get_inferred_edgetypes_GAT(sample_edgetypes,  edgeTypes[dialog_id])\n",
    "#             listAllEdgeIdx.append(list_edge_idx)\n",
    "#             allInferredEdgetypes.append(inferred_edgetypes)\n",
    "\n",
    "#             for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#                 cherrypickedNodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "\n",
    "#         cherrypickedNodes = torch.tensor(cherrypickedNodes)\n",
    "#         cherrypickedNodes.shape\n",
    "#         print(\"End of getting output of 1st GAT\")\n",
    "\n",
    "#         pickle.dump([cherrypickedNodes, allInferredEdgetypes],\n",
    "#                     open(filePath, 'wb'))\n",
    "\n",
    "#     else:\n",
    "#         file = open(filePath, 'rb')\n",
    "#         cherrypickedNodes, allInferredEdgetypes = pickle.load(file)\n",
    "#         file.close()\n",
    "\n",
    "#     return cherrypickedNodes, allInferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef42f2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # train data\n",
    "# cherrypickedNodesTrain, allInferredEdgetypesTrain = get_GAT_representation(\n",
    "#                                                     \"embed/h_prime_CNNBiLSTM-GAT_train.pkl\",\n",
    "#                                                     contextualEmbeddingsTrain,\n",
    "#                                                     edgeIndicesTrain,\n",
    "#                                                     edgeTypesTrain)\n",
    "# # only save the pickle data for test and validation\n",
    "# _, _ = get_GAT_representation(\"embed/h_prime_CNNBiLSTM-GAT_test.pkl\",\n",
    "#                         contextualEmbeddingsTest,\n",
    "#                         edgeIndicesTest,\n",
    "#                         edgeTypesTest)\n",
    "# # TODO add valid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de95f55",
   "metadata": {},
   "source": [
    "<h5> Visualize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e56aa1c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# labels = torch.tensor(trainLabels)\n",
    "# h_prime_np = cherrypickedNodesTrain.detach().numpy() (discontinued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5bd42",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# if runTSNE:\n",
    "#     # List of perplexity values to loop over\n",
    "#     perplexity_values = [30, 100]\n",
    "\n",
    "#     # Loop over each perplexity value\n",
    "#     for perplexity in perplexity_values:\n",
    "#         # Initialize t-SNE with the current perplexity value\n",
    "#         tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "#         # Fit and transform the data using t-SNE\n",
    "#         h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "#         # Plot the node embeddings with different colors for each label\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#             indices = (labels == label).nonzero().squeeze()\n",
    "#             plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "#         plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "#         plt.xlabel('Dimension 1', color=\"white\")\n",
    "#         plt.ylabel('Dimension 2', color=\"white\")\n",
    "#         plt.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caea68",
   "metadata": {},
   "source": [
    "<h4> Analyze the edgetypes of all train nodes in the context of a dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236b455",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming `all_inferred_edgetypes` and `y_train` are defined\n",
    "# df_eda = pd.DataFrame(\n",
    "#     {'edgetype': flatten_extend(allInferredEdgetypesTrain),\n",
    "#      'label': y_train,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7f357",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming `df_eda` and `CrosstabResult` are defined\n",
    "# CrosstabResult = pd.crosstab(index=df_eda['edgetype'], columns=df_eda['label'])\n",
    "\n",
    "# print(\"Crosstab Result:\")\n",
    "# print(CrosstabResult)\n",
    "# print()\n",
    "\n",
    "# # Performing Chi-squared test\n",
    "# ChiSqResult = chi2_contingency(CrosstabResult)\n",
    "\n",
    "# # P-Value is the Probability of H0 being True\n",
    "# # If P-Value > 0.05 then only we Accept the assumption(H0)\n",
    "# # H0: The variables are not correlated with each other.\n",
    "\n",
    "# print('The P-Value of the Chi-Squared Test is:', ChiSqResult[1])\n",
    "\n",
    "# if ChiSqResult[1] > 0.05:\n",
    "#     print(\"Variables are not correlated with each other\")\n",
    "# else:\n",
    "#     print(\"Two variables are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb0a7",
   "metadata": {},
   "source": [
    "<h3> Get EGAT output from each set of data (train, test, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60061c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "egat = EGATConv(in_node_feats=len(contextualEmbeddingsTrain[0][0]),\n",
    "                    in_edge_feats=3,\n",
    "                    out_node_feats=64,\n",
    "                    out_edge_feats=3,\n",
    "                    num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc9167ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EGAT_representations(filePath, contextualEmbeddings, edgeIndices, edgeTypes, ranges):\n",
    "#     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-EGAT_train.pkl\")\n",
    "    if key:\n",
    "#         print(\"Start of getting output of 2nd GAT\")\n",
    "        inferredEdgetypes = []\n",
    "        allNodeFeats = []\n",
    "\n",
    "        # Iterate over each dialogue\n",
    "        for dialog_id in tqdm(range(len(edgeIndices)), desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "            startIdx, endIdx = ranges[dialog_id][0], ranges[dialog_id][1]\n",
    "            # Create a DGL graph\n",
    "            graph = dgl.graph((edgeIndices[dialog_id][0], edgeIndices[dialog_id][1]))\n",
    "\n",
    "            # Get one-hot encoded edge features\n",
    "            edge_feats = get_ohe(edgeTypes[dialog_id])\n",
    "\n",
    "            # Get outputs from the second GAT layer\n",
    "            egat_output = egat(graph, contextualEmbeddings[startIdx: endIdx+1], edge_feats)\n",
    "            new_node_feats, new_edge_feats = egat_output\n",
    "\n",
    "            # Compute mean edge features\n",
    "            mean_edge_feats = new_edge_feats.mean(dim=1)\n",
    "            allNodeFeats.append(new_node_feats.mean(dim=1).tolist())\n",
    "\n",
    "            # Prepare edge features for inference\n",
    "            target_nodes = edgeIndices[dialog_id][1].tolist()\n",
    "            sample_edgetypes = {}\n",
    "            for i in set(target_nodes):\n",
    "                sample_edgetypes[i] = []\n",
    "            for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "                sample_edgetypes[target_node].append([edge_idx, \n",
    "                                                      mean_edge_feats[edge_idx].tolist()])\n",
    "\n",
    "            # Infer edge types\n",
    "            sample_edgetypes = get_inferred_edgetypes_EGAT(sample_edgetypes, edgeTypes[dialog_id], \n",
    "                                                           len(contextualEmbeddings[startIdx: endIdx+1]), \n",
    "                                                           dialog_id)\n",
    "            inferredEdgetypes.append(sample_edgetypes)\n",
    "\n",
    "        # Flatten and convert node features to tensor\n",
    "        allNodeFeats = torch.tensor(flatten_extend(allNodeFeats))\n",
    "\n",
    "#         print(\"End of getting output of 2nd GAT\")\n",
    "\n",
    "        # Save the data to a pickle file\n",
    "        pickle.dump([allNodeFeats, inferredEdgetypes], open(filePath, 'wb'))\n",
    "    else:\n",
    "        # Load data from the existing pickle file\n",
    "        file = open(filePath, 'rb')\n",
    "        all_node_feats, inferredEdgetypes = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return allNodeFeats, inferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0afe0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 200])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor_squeezedTrain.shape\n",
    "# torch.Size([12840, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a62f0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edgeTypesTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938645cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contextualEmbeddingsTrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tensor_squeezedTrain \u001b[38;5;241m=\u001b[39m \u001b[43mcontextualEmbeddingsTrain\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m allNodeFeatsTrain, inferredEdgetypesTrain \u001b[38;5;241m=\u001b[39m get_EGAT_representations(\n\u001b[0;32m      3\u001b[0m                                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/h_prime_CNNBiLSTM-EGAT_train.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m                                         tensor_squeezedTrain,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m                                         rangesTrain\n\u001b[0;32m      8\u001b[0m                                 )\n\u001b[0;32m     10\u001b[0m tensor_squeezedTest \u001b[38;5;241m=\u001b[39m contextualEmbeddingsTest\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'contextualEmbeddingsTrain' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_squeezedTrain = contextualEmbeddingsTrain.squeeze(1)\n",
    "allNodeFeatsTrain, inferredEdgetypesTrain = get_EGAT_representations(\n",
    "                                        \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM-EGAT_train.pkl\",\n",
    "                                        tensor_squeezedTrain,\n",
    "                                        edgeIndicesTrain,\n",
    "                                        edgeTypesTrain,\n",
    "                                        rangesTrain\n",
    "                                )\n",
    "\n",
    "tensor_squeezedTest = contextualEmbeddingsTest.squeeze(1)\n",
    "_, _ = get_EGAT_representations(\n",
    "        \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM-EGAT_test.pkl\",\n",
    "        tensor_squeezedTest,\n",
    "        edgeIndicesTest,\n",
    "        edgeTypesTest, \n",
    "        rangesTest\n",
    ")\n",
    "\n",
    "tensor_squeezedDev = contextualEmbeddingsDev.squeeze(1)\n",
    "_, _ = get_EGAT_representations(\n",
    "        \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM-EGAT_dev.pkl\",\n",
    "        tensor_squeezedDev,\n",
    "        edgeIndicesDev,\n",
    "        edgeTypesDev, \n",
    "        rangesDev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d83d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda2 = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(inferredEdgetypesTrain),\n",
    "     'label': trainLabels,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf36e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab Result:\n",
      " label        0    1    2     3     4    5     6\n",
      "edgetype                                       \n",
      "0         1091  258  242  1644  4347  627  1136\n",
      "1          409  106   96   668  1613  249   354\n",
      "The P-Value of the ChiSq Test is: 0.03080562512893102\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from your data (df_eda2)\n",
    "# Assuming df_eda2 is already defined\n",
    "\n",
    "# Crosstabulation\n",
    "CrosstabResult2 = pd.crosstab(index=df_eda2['edgetype'], columns=df_eda2['label'])\n",
    "print(\"Crosstab Result:\\n\", CrosstabResult2)\n",
    "\n",
    "# Performing Chi-squared test\n",
    "ChiSqResult2 = chi2_contingency(CrosstabResult2)\n",
    "\n",
    "# Print the p-value of the Chi-squared test\n",
    "print('The P-Value of the ChiSq Test is:', ChiSqResult2[1])\n",
    "\n",
    "# Interpret the p-value\n",
    "if ChiSqResult2[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213c68a",
   "metadata": {},
   "source": [
    "Testing on 1 dialog data before scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84641d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Node Features Shape: torch.Size([14, 4, 64])\n",
      "New Edge Features Shape: torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "dialog_id = 0\n",
    "\n",
    "# Create a DGL graph\n",
    "graph = dgl.graph((edgeIndicesTrain[dialog_id][0], edgeIndicesTrain[dialog_id][1]))\n",
    "\n",
    "# Obtain one-hot encoded edge features\n",
    "edge_feats = get_ohe(edgeTypesTrain[dialog_id])\n",
    "\n",
    "# Pass the graph, node representations, and edge features through the EGAT model\n",
    "tensor_squeezedTrain = contextualEmbeddingsTrain.squeeze(1)\n",
    "newNodeFeats, newEdgeFeats = egat(graph, tensor_squeezedTrain[0:14], edge_feats)\n",
    "\n",
    "# Print the shapes of the new node and edge features\n",
    "print(\"New Node Features Shape:\", newNodeFeats.shape)\n",
    "print(\"New Edge Features Shape:\", newEdgeFeats.shape)\n",
    "\n",
    "# Calculate the mean of node features along the second dimension (number of nodes)\n",
    "h_prime_mean = newNodeFeats.mean(dim=1)\n",
    "\n",
    "# Assuming you want to select only a subset of labels for visualization\n",
    "utt_size = 13\n",
    "labels = torch.tensor(trainLabels[:utt_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bbcf4d9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "h_prime_np = h_prime_mean.detach().numpy()\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# Plot the node embeddings with different colors for each label\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "    indices = (labels == label).nonzero().squeeze()\n",
    "    plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "plt.xlabel('Dimension 1', color=\"white\")\n",
    "plt.ylabel('Dimension 2', color=\"white\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e75975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "851a250c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # Convert tensor to numpy array\n",
    "    h_prime_np = allNodeFeatsTrain.detach().numpy()\n",
    "    labels = torch.tensor(trainLabels)\n",
    "    \n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "        print(h_prime_tsne.shape)\n",
    "        \n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
