{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c64aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, math, pickle, sys, random, time\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "import dgl,numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import RGCNConv, GraphConv\n",
    "from model import DialogueGCN_MELDModel, GraphNetwork_RGCN, GraphNetwork_GAT, \\\n",
    "GraphNetwork_GAT_EdgeFeat, GraphNetwork_GATv2, GraphNetwork_GATv2_EdgeFeat, GraphNetwork_RGAT\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from graph_context_dataset import GraphContextDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f920f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153dd45",
   "metadata": {},
   "source": [
    "<b>Make sure to specify which dataset to use\n",
    "<br>\n",
    " - dataset_original\n",
    "<br>\n",
    " - dataset_drop_noise\n",
    "<br>\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_original\"\n",
    "dataset_path = \"dataset_drop_noise\"\n",
    "dataset_path = \"dataset_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa9fe91",
   "metadata": {
    "code_folding": [
     0,
     38,
     60
    ]
   },
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        torch.manual_seed(42)\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "        \n",
    "        # Edge type embeddings\n",
    "        torch.manual_seed(42)\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "        \n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of neighbor node representations\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).mean(dim=2)\n",
    "\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores\n",
    "\n",
    "class EGATConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_node_feats,\n",
    "                 in_edge_feats,\n",
    "                 out_node_feats,\n",
    "                 out_edge_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 **kw_args):\n",
    "\n",
    "        super().__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._out_node_feats = out_node_feats\n",
    "        self._out_edge_feats = out_edge_feats\n",
    "        \n",
    "        self.fc_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=bias)\n",
    "        self.fc_ni = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_nj = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        \n",
    "        # Attention parameter\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.manual_seed(42)\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.xavier_normal_(self.fc_node.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "        init.xavier_normal_(self.attn, gain=gain)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "        with graph.local_scope():\n",
    "            graph.edata['f'] = efeats\n",
    "            graph.ndata['h'] = nfeats\n",
    "            \n",
    "            f_ni = self.fc_ni(nfeats)\n",
    "            f_nj = self.fc_nj(nfeats)\n",
    "            f_fij = self.fc_fij(efeats)\n",
    "            graph.srcdata.update({'f_ni' : f_ni})\n",
    "            graph.dstdata.update({'f_nj' : f_nj})\n",
    "            \n",
    "            graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "            f_out = graph.edata.pop('f_tmp') + f_fij\n",
    "            \n",
    "            if self.bias is not None:\n",
    "                f_out += self.bias\n",
    "            f_out = nn.functional.leaky_relu(f_out)\n",
    "            f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "            \n",
    "            e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.edata['a'] = edge_softmax(graph, e)\n",
    "            graph.ndata['h_out'] = self.fc_node(nfeats).view(-1, self._num_heads, self._out_node_feats)\n",
    "            \n",
    "            graph.update_all(fn.u_mul_e('h_out', 'a', 'm'), fn.sum('m', 'h_out'))\n",
    "\n",
    "            h_out = graph.ndata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "            if get_attention:\n",
    "                return h_out, f_out, graph.edata.pop('a')\n",
    "            else:\n",
    "                return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d7adf37e",
   "metadata": {
    "code_folding": [
     0,
     11,
     29,
     31,
     50
    ]
   },
   "outputs": [],
   "source": [
    "def get_ohe(edge_types):\n",
    "    one_hot_encoding = []\n",
    "    for edge_type in edge_types:\n",
    "        if edge_type == 0:\n",
    "            one_hot_encoding.append([1., 0., 0.])\n",
    "        elif edge_type == 1:\n",
    "            one_hot_encoding.append([0., 1., 0.])\n",
    "        elif edge_type == 2:\n",
    "            one_hot_encoding.append([0., 0., 1.])\n",
    "    return torch.tensor(one_hot_encoding)\n",
    "\n",
    "def get_inferred_edgetypes_GAT(dialog, edge_types):\n",
    "    inferred_edge_types = []\n",
    "    inferred_edge_indices = []\n",
    "    for target_node in dialog.values():\n",
    "        if len(target_node) == 1:\n",
    "            inferred_edge_types.append(0)\n",
    "            inferred_edge_indices.append(0)\n",
    "        else:\n",
    "            edge_index = target_node[0][0]\n",
    "            highest_attention = target_node[0][1]\n",
    "            for src_node in target_node[1:]:\n",
    "                if highest_attention < src_node[1]:\n",
    "                    highest_attention = src_node[1]\n",
    "                    edge_index = src_node[0]\n",
    "            inferred_edge_indices.append(edge_index)\n",
    "            inferred_edge_types.append(edge_types[edge_index].tolist())\n",
    "    return inferred_edge_indices, inferred_edge_types\n",
    "\n",
    "def get_inferred_edgetypes_EGAT(edges_target_nodes, sample_edge_types, size_dialog, dialog_id):\n",
    "    inferred_edge_types = []\n",
    "    for target_idx in range(size_dialog):\n",
    "        num_edges = len(edges_target_nodes[target_idx])\n",
    "        if num_edges == 1:\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "        else:\n",
    "            highest_attn_score = max(edges_target_nodes[target_idx][0][1])\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            for sample_edge in range(1, num_edges):\n",
    "                cur_highest_attn_score = max(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                if cur_highest_attn_score > highest_attn_score:\n",
    "                    highest_attn_score = cur_highest_attn_score\n",
    "                    edgetype_idx = np.argmax(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                    edge_idx = edges_target_nodes[target_idx][sample_edge][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "    return inferred_edge_types\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7639e77",
   "metadata": {
    "code_folding": [
     0,
     15,
     25,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 0\n",
    "        while (val <= 3) and (i+val <= end_idx):\n",
    "            target_idx = i+val\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "            val = val+1\n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "    return adjacency_list_dict\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    for i in range(dialogs_len):\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "        for j in range(dialog_len):\n",
    "            src_node = dialog_nodes_pairs[j]\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k]\n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))\n",
    "    return whole_edge_type_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed53f3f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddb394",
   "metadata": {},
   "source": [
    "<h3> Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c424c",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8a8f87",
   "metadata": {
    "code_folding": [
     4,
     6
    ]
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "rangesTrain = []\n",
    "\n",
    "if not checkFile:\n",
    "    print(\"Run first the contextEncoder1 or 2 to generate this file\")\n",
    "else:\n",
    "    with open('data/dump/speaker_encoder_train.pkl', \"rb\") as file:\n",
    "        encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/adjListTrain.pkl\")\n",
    "adjacencyListTrain = []\n",
    "\n",
    "if key:\n",
    "    adjacencyListTrain = get_all_adjacency_list(rangesTrain)\n",
    "else:\n",
    "    with open('data/dump/adjListTrain', \"rb\") as file:\n",
    "        adjacencyListTrain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771b8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_CNNBiLSTM_train.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    emotions = pickle.load(file)\n",
    "    \n",
    "contextualEmbeddingsTrain = emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6698b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 1, 200])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualEmbeddingsTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6055971",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain)\n",
    "edgeTypesTrain = get_all_edge_type_list(edgeIndicesTrain, encodedSpeakersTrain)\n",
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18118d",
   "metadata": {},
   "source": [
    "<h4> Creating \"SAMPLE\" graph features based on various graph networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16a469bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rangesTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b197fe2",
   "metadata": {},
   "source": [
    "Start of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc943c7d",
   "metadata": {},
   "source": [
    "<h5>DGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9caf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.features = [torch.rand(14, 200)]\n",
    "        self.edge_index = [torch.randint(0, 14, (2, 69))]\n",
    "        self.edge_type = [torch.randint(0, 4, (69,))]\n",
    "        self.edge_index_lengths = [torch.tensor([69])]\n",
    "        self.umask = [torch.randint(0, 2, (1, 14))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.edge_index[idx], self.edge_type[idx], self.edge_index_lengths[idx], self.umask[idx])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = SampleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4874963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN Representation Shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "# D_m = 200\n",
    "# D_g = 100\n",
    "# D_p = 100\n",
    "# D_e = 100\n",
    "# D_h = 100\n",
    "# D_a = 100\n",
    "# graph_hidden_size = 64\n",
    "# n_speakers = 2\n",
    "# max_seq_len = 110\n",
    "# window_past = 0\n",
    "# window_future = 5\n",
    "# n_classes = 7\n",
    "# dropout_rec = 0.5\n",
    "# dropout = 0.5\n",
    "nodal_attention = True\n",
    "avec = False\n",
    "no_cuda = False\n",
    "\n",
    "features = torch.randn(14, 200)\n",
    "edge_index = [torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0]])]\n",
    "edge_type = [torch.tensor([1, 0, 2, 2, 2, 0, 1, 3, 2, 2, 0, 2, 2, 2])]\n",
    "seq_lengths  = torch.tensor([[14]])\n",
    "umask = torch.ones(1, 1, 14)\n",
    "\n",
    "nodal_attn = False\n",
    "avec = False\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphNetwork_RGCN(num_features=200, num_classes=7, num_relations=4, max_seq_len=14)\n",
    "gcn_representation = model(features, edge_index, edge_type, seq_lengths, umask, nodal_attn, avec)\n",
    "print(\"GCN Representation Shape:\", gcn_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3846a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d48a0",
   "metadata": {},
   "source": [
    "<h5>GAT w/o edge feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e6cdb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4  # This parameter is not used with GATConv but kept for compatibility\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_type = [torch.randint(0, num_relations, (20,))]  # Example edge types\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "928e2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862804c",
   "metadata": {},
   "source": [
    "<h5>GAT with edge feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "276cee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))\n",
    "edge_index = [torch.randint(0, 14, (2, 20))]\n",
    "edge_attr = [torch.randn((20, num_relations))]\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1869288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_attr = torch.randint(0, 2, (20, 1)).float()  # Example binary edge features\n",
    "# edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b632",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/o edgetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab557a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n",
      "Output shape with attention: torch.Size([14, 64])\n",
      "Attention weights shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2(num_features, num_classes, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Forward pass with attention weights\n",
    "out, (edge_index, attention_weights) = model(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "print(\"Output shape with attention:\", out.shape)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5781299",
   "metadata": {},
   "source": [
    "<h5>GATv2 with edge type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b5ae9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_attr = torch.randn((20, num_relations))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d91e3",
   "metadata": {},
   "source": [
    "<h5>RGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d536b5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  0,  4, 11,  2,  0,  7,  1,  2,  9,  6, 12,  2,  9,  0, 13,  9, 12,\n",
       "          4, 11],\n",
       "        [11,  7,  9,  8, 12,  8,  2, 12,  3,  6,  9,  3, 13,  5,  7, 12, 12,  3,\n",
       "          1, 12]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e05a3a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 3  # Example number of relations\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "edge_dim = 1  # Dimensionality of edge attributes\n",
    "no_cuda = False\n",
    "\n",
    "\n",
    "model = GraphNetwork_RGAT(num_features, num_classes, num_relations, hidden_size, num_heads, dropout, edge_dim, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_type=edge_type)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94a009",
   "metadata": {},
   "source": [
    "End of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317848a",
   "metadata": {},
   "source": [
    "<h4> Encode speaker to the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7bc233c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class GraphContextDataset(Dataset):\n",
    "#     def __init__(self, rangeSet, labels, features, edge_index, edge_type, \\\n",
    "#                  edge_index_lengths, umask, seq_lengths):\n",
    "#         self.rangeSet = rangeSet\n",
    "#         self.labels = [torch.tensor(label) for label in labels]\n",
    "#         self.features = [torch.tensor(feature) for feature in features]\n",
    "#         self.edge_index = [torch.tensor(edge) for edge in edge_index]\n",
    "#         self.edge_type = [torch.tensor(edge) for edge in edge_type]\n",
    "#         self.edge_index_lengths = [torch.tensor(length) for length in edge_index_lengths]\n",
    "#         self.umask = umask\n",
    "#         self.seq_lengths = seq_lengths\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.rangeSet)  # Use rangeSet for length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         startIdx, endIdx = self.rangeSet[idx]\n",
    "#         return (\n",
    "#             self.labels[startIdx: endIdx+1],\n",
    "#             self.features[idx],\n",
    "#             self.edge_index[idx],\n",
    "#             self.edge_type[idx],\n",
    "#             self.edge_index_lengths[idx],\n",
    "#             self.umask[idx],\n",
    "#             self.seq_lengths[idx]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbbdf11a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.features = [torch.tensor(feature) for feature in features]\n",
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_index = [torch.tensor(edge) for edge in edge_index]\n",
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_type = [torch.tensor(edge) for edge in edge_type]\n"
     ]
    }
   ],
   "source": [
    "file_path1 = 'embed/pre_h_prime_CNNBiLSTM_train.pkl'\n",
    "file_path2 = 'data/dump/train_labels.pkl'\n",
    "with open(file_path1, 'rb') as file1:\n",
    "     all_umask, \\\n",
    "     all_seq_lengths,\\\n",
    "     all_features, \\\n",
    "     all_edge_index, \\\n",
    "     all_edge_norm, \\\n",
    "     all_edge_type, \\\n",
    "     all_edge_index_lengths = pickle.load(file1)\n",
    "        \n",
    "with open(file_path2, 'rb') as file2:\n",
    "    trainLabels = pickle.load(file2)\n",
    "\n",
    "trainDataset = GraphContextDataset(rangesTrain, trainLabels,\n",
    "                                   all_features, all_edge_index,\n",
    "                                   all_edge_type,\n",
    "                                   all_edge_index_lengths,\n",
    "                                   all_umask, all_seq_lengths)\n",
    "dataLoader = DataLoader(trainDataset, batch_size=1, shuffle=False, num_workers=4)  # Use num_workers for parallel data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ad33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76fc8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RelationEncoding(file_path, dataLoader, model, config):\n",
    "    all_h_prime = []\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    j = 1\n",
    "    for _, features_in, edge_index_in, edge_type_in, _, umask, seq_lengths_in in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        if config == \"dgcn\":\n",
    "            avec, no_cuda, nodal_attn = False, False, False\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            seq_lengths = torch.tensor(seq_lengths_in).view(1, 1)\n",
    "#             print(\"feature: \", feature.shape)\n",
    "#             print(\"edge_index.shape: \", edge_index.shape)\n",
    "#             print(\"edge_index: \", [edge_index])\n",
    "#             print(\"edge_type.shape: \", edge_type.shape)\n",
    "#             print(\"edge_type: \", [edge_type])\n",
    "#             print(\"seq_len.shape : \", seq_lengths.shape)\n",
    "#             print(\"seq_len: \", seq_lengths)\n",
    "#             print(\"umask.shape: \", umask.shape)\n",
    "#             print(\"umask: \", umask)\n",
    "            graph_representation = model(feature, [edge_index], [edge_type], seq_lengths, umask, nodal_attn, avec)\n",
    "        elif config == \"GATv1_noAttn\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            graph_representation = model(feature, [edge_index])\n",
    "            \n",
    "        elif config == \"GATv1\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            num_edge_types = 8\n",
    "            edge_attr = torch.zeros((edge_type.size(0), num_edge_types))\n",
    "            edge_attr.scatter_(1, edge_type.view(-1, 1), 1)\n",
    "            graph_representation = model(feature, [edge_index], [edge_attr])\n",
    "            \n",
    "        elif config == \"GATv2_noAttn\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            graph_representation = model(feature, edge_index)\n",
    "        \n",
    "        elif config == \"GATv2\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            num_edge_types = 8\n",
    "            edge_attr = torch.zeros((edge_type.size(0), num_edge_types))\n",
    "            edge_attr.scatter_(1, edge_type.view(-1, 1), 1)\n",
    "            graph_representation = model(feature, edge_index, edge_attr)\n",
    "        \n",
    "        elif config == \"RGAT\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            graph_representation = model(feature, edge_index, edge_type)\n",
    "        \n",
    "        all_h_prime.append(graph_representation.cpu())\n",
    "        \n",
    "        i = i + 1\n",
    "        if i % 500 == 0 and config == \"RGAT\":\n",
    "            pt_file_path = file_path + str(j) + \".pkl\"\n",
    "            with open(pt_file_path, 'wb') as file:  # Corrected the file path\n",
    "                pickle.dump(all_h_prime, file)\n",
    "            all_h_prime = []\n",
    "            j = j + 1\n",
    "            \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train\", config)\n",
    "    \n",
    "    if config == \"RGAT\":\n",
    "        pt_file_path = file_path + str(j) + \".pkl\"\n",
    "        with open(pt_file_path, 'wb') as file:\n",
    "            pickle.dump(all_h_prime, file)\n",
    "    else:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(all_h_prime, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3073ff8",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# features = torch.randn(14, 200)\n",
    "# edge_index = [torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "#                             [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0]])]\n",
    "# edge_type = [torch.tensor([1, 0, 2, 2, 2, 0, 1, 3, 2, 2, 0, 2, 2, 2])]\n",
    "# seq_lengths  = torch.tensor([[14]])\n",
    "# umask = torch.ones(1, 1, 14)\n",
    "# print(\"features: \", features.shape)\n",
    "# print(\"edge_index.shape: \", edge_index[0].shape)\n",
    "# print(\"edge_index: \", edge_index)\n",
    "# print(\"edge_type.shape: \", edge_type[0].shape)\n",
    "# print(\"edge_type: \", edge_type)\n",
    "# print(\"seq_lengths.shape: \", seq_lengths.shape)\n",
    "# print(\"seq_lengths: \", seq_lengths)\n",
    "# print(\"umask.shape: \", umask.shape)\n",
    "# print(\"umask: \", umask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d64f3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# edge_type_mapping = {}\n",
    "\n",
    "# for j in range(2):\n",
    "#     for k in range(2):\n",
    "#         edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "#         edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)\n",
    "# edge_type_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648061a8",
   "metadata": {},
   "source": [
    "<h5>DGCN_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75c77723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:33<00:00, 64.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 33.457242012023926 seconds to encode train dgcn\n"
     ]
    }
   ],
   "source": [
    "file_path = 'embed/h_prime_CNNBiLSTM_DGCN_train.pkl'\n",
    "model = GraphNetwork_RGCN(num_features=200, num_classes=7, num_relations=8, max_seq_len=30)\n",
    "RelationEncoding(file_path, dataLoader, model, \"dgcn\")\n",
    "# gcn_representation = model(features, edge_index, edge_type, seq_lengths, umask, nodal_attn, avec)\n",
    "# print(\"GCN Representation Shape:\", gcn_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46527c",
   "metadata": {},
   "source": [
    "<h5>GAT w/o edge_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0bcdb",
   "metadata": {},
   "source": [
    "reviwing the types and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edge_type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa4149",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_attr = [torch.randn((20, num_relations))]  # Example edge features\n",
    "# print(\"x.shape: \", x.shape)\n",
    "# print(\"edge_index.shape: \", edge_index[0].shape)\n",
    "# print(\"edge_attr.shape: \", edge_attr[0].shape)\n",
    "# print(\"x: \", x)\n",
    "# print(\"edge_index: \", edge_index)\n",
    "# print(\"edge_attr: \", edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "758ba04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:32<00:00, 67.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 32.00769090652466 seconds to encode train GATv1_noAttn\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 4  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "file_path = 'embed/h_prime_CNNBiLSTM_GATv1_train.pkl'\n",
    "model = GraphNetwork_GAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "RelationEncoding(file_path, dataLoader, model, \"GATv1_noAttn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb39da0",
   "metadata": {},
   "source": [
    "<h5>GAT w/ edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01092d30",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_attr = [torch.randn((20, num_relations))]  # Example edge features\n",
    "# print(edge_index[0].shape)\n",
    "# print(edge_attr[0].shape)\n",
    "# print(edge_index)\n",
    "# print(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "195e2641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:28<00:00, 76.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 28.11700201034546 seconds to encode train GATv1\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 8  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "file_path = 'embed/h_prime_CNNBiLSTM_GATv1_edgeAttr_train.pkl'\n",
    "RelationEncoding(file_path, dataLoader, model, \"GATv1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06c4f7",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/o edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3525a0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "# print(x.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(edge_attr.shape)\n",
    "# print(x)\n",
    "# print(edge_index)\n",
    "# print(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a40b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:22<00:00, 94.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 22.787252187728882 seconds to encode train GATv2_noAttn\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "model = GraphNetwork_GATv2(num_features, num_classes, hidden_size, num_heads, dropout, no_cuda)\n",
    "file_path = 'embed/h_prime_CNNBiLSTM_GATv2_train.pkl'\n",
    "RelationEncoding(file_path, dataLoader, model, \"GATv2_noAttn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb8298",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/ edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea9a6dcd",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_attr = torch.randn((20, num_relations))  # Example edge features\n",
    "# print(x.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "245ee655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:30<00:00, 71.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 30.10883855819702 seconds to encode train GATv2\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 8\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "file_path = 'embed/h_prime_CNNBiLSTM_GATv2_edgeAttr_train.pkl'\n",
    "RelationEncoding(file_path, dataLoader, model, \"GATv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609069ea",
   "metadata": {},
   "source": [
    "<h5> RGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db77a2c4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "# # edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "# print(edge_type.shape)\n",
    "# print(edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b92fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [01:44<00:00, 20.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 104.07745361328125 seconds to encode train RGAT\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 8\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "edge_dim = 1  # Dimensionality of edge attributes\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_RGAT(num_features, num_classes, num_relations, hidden_size, num_heads, dropout, edge_dim, no_cuda)\n",
    "file_path = 'embed/h_prime_CNNBiLSTM_RGAT_train'\n",
    "RelationEncoding(file_path, dataLoader, model, \"RGAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36a4c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['embed/h_prime_CNNBiLSTM_RGAT_train1.pkl', 'embed/h_prime_CNNBiLSTM_RGAT_train2.pkl', 'embed/h_prime_CNNBiLSTM_RGAT_train3.pkl', 'embed/h_prime_CNNBiLSTM_RGAT_train4.pkl', 'embed/h_prime_CNNBiLSTM_RGAT_train5.pkl']\n",
    "# Initialize an empty list to store the contents of each file\n",
    "combined_data = []\n",
    "\n",
    "# Load data from each file and append it to the combined_data list\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "# Save the combined data to a new pickle file\n",
    "output_file_path = 'embed/h_prime_CNNBiLSTM_RGAT_train.pkl'\n",
    "with open(output_file_path, 'wb') as file:\n",
    "    pickle.dump(combined_data, file)\n",
    "\n",
    "# Delete the original files\n",
    "for file_path in file_paths:\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016df95",
   "metadata": {},
   "source": [
    "end of encoding train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da874fb",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# change D_m into\n",
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 100\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "seed_everything()\n",
    "model = DialogueGCN_MELDModel(\n",
    "                               D_m, D_g, D_p, D_e, D_h, D_a, graph_h,\n",
    "                               n_speakers=2,\n",
    "                               max_seq_len=110,\n",
    "                               window_past=0,\n",
    "                               window_future=5,\n",
    "                               n_classes=7,\n",
    "                               listener_state=False,\n",
    "                               context_attention='general',\n",
    "                               dropout=0.5,\n",
    "                               nodal_attention=False,\n",
    "                               no_cuda=False\n",
    "                               )\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e759176",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_features = 200\n",
    "num_classes = 7\n",
    "num_relations = 3  # Example number of relations\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork4WithRGAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_type=edge_type, edge_attr=edge_attr)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18936917",
   "metadata": {
    "code_folding": [
     0,
     69
    ]
   },
   "outputs": [],
   "source": [
    "# def train_or_eval_graph_model(model, loss_function, dataloader, epoch, cuda, optimizer=None, train=False):\n",
    "#     losses, preds, labels = [], [], []\n",
    "#     scores, vids = [], []\n",
    "    \n",
    "#     ei, et, en, el = torch.empty(0).type(torch.LongTensor), torch.empty(0).type(torch.LongTensor), torch.empty(0), []\n",
    "    \n",
    "#     assert not train or optimizer != None\n",
    "#     if train:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "        \n",
    "#     seed_everything()\n",
    "#     for data in dataloader:\n",
    "#         if train:\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#         labels, features, edge_index, edge_type, edge_index_lengths, umask, _ = data\n",
    "#         print(\"features: \", features[0].shape)\n",
    "#         print(\"edge_index: \", edge_index[0].shape)\n",
    "# #         print(edge_norm[0].shape)\n",
    "#         print(\"edge_type: \", edge_type[0].shape)\n",
    "#         print(\"edge_index_lengths: \", edge_index_lengths[0])\n",
    "#         print(\"umask: \", umask[0].shape)\n",
    "#         print(\"-------------------------------\")\n",
    "#         log_prob, e_i, e_n, e_t, e_l = model(features, edge_index,\n",
    "# #                                              edge_norm,\n",
    "#                                              edge_type, edge_index_lengths, umask)\n",
    "#         label = torch.cat([label[j][:lengths[j]] for j in range(len(label))])\n",
    "#         loss = loss_function(log_prob, label)\n",
    "\n",
    "#         ei = torch.cat([ei, e_i], dim=1)\n",
    "#         et = torch.cat([et, e_t])\n",
    "#         en = torch.cat([en, e_n])\n",
    "#         el += e_l\n",
    "\n",
    "#         preds.append(torch.argmax(log_prob, 1).cpu().numpy())\n",
    "#         labels.append(label.cpu().numpy())\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         if train:\n",
    "#             loss.backward()\n",
    "#             if args.tensorboard:\n",
    "#                 for param in model.named_parameters():\n",
    "#                     writer.add_histogram(param[0], param[1].grad, epoch)\n",
    "#             optimizer.step()\n",
    "            \n",
    "#     if preds != []:\n",
    "#         preds = np.concatenate(preds)\n",
    "#         labels = np.concatenate(labels)\n",
    "#     else:\n",
    "#         return float('nan'), float('nan'), [], [], float('nan'), [], [], [], [], []\n",
    "    \n",
    "#     ei = ei.data.cpu().numpy()\n",
    "#     et = et.data.cpu().numpy()\n",
    "#     en = en.data.cpu().numpy()\n",
    "#     el = np.array(el)\n",
    "#     labels = np.array(labels)\n",
    "#     preds = np.array(preds)\n",
    "\n",
    "#     avg_loss = round(np.sum(losses) / len(losses), 4)\n",
    "#     avg_accuracy = round(accuracy_score(labels, preds) * 100, 2)\n",
    "#     avg_fscore = round(f1_score(labels, preds, average='macro') * 100, 2)\n",
    "#     # Add precision and recall\n",
    "#     precision = round(precision_score(labels, preds, average='macro') * 100, 2)\n",
    "#     recall = round(recall_score(labels, preds, average='macro') * 100, 2)\n",
    "#     return avg_loss, avg_accuracy, labels, preds, avg_fscore, ei, et, en, el, precision, recall\n",
    "    \n",
    "    \n",
    "# for e in range(60):\n",
    "#     train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_graph_model(model, loss_function,\n",
    "#                                                                           dataloader, e,\n",
    "#                                                                           optimizer, True)\n",
    "#     print(\n",
    "#     'epoch: {}, train_loss: {}, train_acc: {}, train_fscore: {}, train_precision: {}, train_recall: {}, '. \\\n",
    "#     format(e + 1, train_loss, train_acc, train_fscore, train_precision, train_recall))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07310ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f0dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c871b0b4",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de3baa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "rangesTest = []\n",
    "\n",
    "if not checkFile:\n",
    "    print(\"Run first the contextEncoder2 to generate this file\")\n",
    "else:\n",
    "    with open('data/dump/speaker_encoder_test.pkl', \"rb\") as file:\n",
    "        encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/adjListTest.pkl\")\n",
    "adjacencyListTest = []\n",
    "\n",
    "if key:\n",
    "    adjacencyListTest = get_all_adjacency_list(rangesTest)\n",
    "else:\n",
    "    with open('data/dump/adjListTest', \"rb\") as file:\n",
    "        adjacencyListTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0b9fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    contextualEmbeddingsTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29deca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTest = get_all_adjacency_list(rangesTest)\n",
    "edgeTypesTest = get_all_edge_type_list(edgeIndicesTest, encodedSpeakersTest)\n",
    "edgeIndicesTest = get_all_adjacency_list(rangesTest, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e5796",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0634150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO repeat the one above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36c94f",
   "metadata": {},
   "source": [
    "<h3> Get GAT output from each set of data (DISCONTINUED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f024704",
   "metadata": {},
   "source": [
    "<h4> Instantiating the GAT (1st implementation) for 1 sample train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cacafa9a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# num_in_features = len(contextualEmbeddingsTrain[0][0])\n",
    "# num_out_features = len(contextualEmbeddingsTrain[0][0])\n",
    "# num_heads = 4\n",
    "# num_edge_types = 3\n",
    "# gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98a8775c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i = 0  # dialogue id\n",
    "# relationalEmbedding, attentionCoef = gat_layer((contextualEmbeddingsTrain[i], edgeIndicesTrain[i]), edgeTypesTrain[i])\n",
    "# print(\"h_prime shape: \", relationalEmbedding.shape, \"attention_coef shape: \", attentionCoef.shape)\n",
    "\n",
    "# targetNodes = edgeIndicesTrain[i][1].tolist()\n",
    "\n",
    "# sample = {}\n",
    "# sampleEdgetypes = []\n",
    "\n",
    "# for target_i in sorted(set(targetNodes)):\n",
    "#     sample[target_i] = []\n",
    "\n",
    "# for targetNode, idx in zip(targetNodes, range(len(targetNodes))):\n",
    "#     sample[targetNode].append([idx, relationalEmbedding[targetNode][idx].tolist()])\n",
    "\n",
    "# listEdgeIdxTrain, inferredEdgeTypes = get_inferred_edgetypes_GAT(sample, edgeTypesTrain[i])\n",
    "# sampleEdgetypes.append(inferredEdgeTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc9c28c3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/dump/label_decoder.pkl', 'rb')\n",
    "label_decoder = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "label_decoder = list(label_decoder.values())\n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6003c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/labels_train.pkl\")\n",
    "\n",
    "# if checkFile is False:\n",
    "#     print(\"Please run the contextEncoder1 notebook to save the label file\")\n",
    "# else:\n",
    "#     file = open('data/dump/labels_train.pkl', 'rb')\n",
    "#     y_train = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f789e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# y_train[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3e8e1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/labels_test.pkl\")\n",
    "\n",
    "# if checkFile is False:\n",
    "#     print(\"Please run the contextEncoder2 notebook to save the label file\")\n",
    "# else:\n",
    "#     file = open('data/dump/labels_test.pkl', 'rb')\n",
    "#     y_test = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223125",
   "metadata": {},
   "source": [
    "<h5>Unsupervised Visualizarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a41a39",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming h_prime contains the node embeddings\n",
    "# utt_size = 13\n",
    "# labels = torch.tensor(y_train[:utt_size + 1])\n",
    "\n",
    "# cherrypicked_nodes = []\n",
    "# for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#     cherrypicked_nodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "# cherrypicked_nodes = torch.tensor(cherrypicked_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a97231",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# h_prime_np = cherrypicked_nodes.detach().numpy()\n",
    "\n",
    "# # Perform dimensionality reduction using t-SNE\n",
    "# tsne = TSNE(n_components=3, perplexity=5, random_state=42)\n",
    "# h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# # Plot the node embeddings with different colors for each label\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#     indices = (labels == label).nonzero().squeeze()\n",
    "#     plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "# plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "# plt.xlabel('Dimension 1', color=\"white\")\n",
    "# plt.ylabel('Dimension 2', color=\"white\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8043a6a",
   "metadata": {},
   "source": [
    "<h4> Now get new representations of all train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588776d9",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "# # filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "# #            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "# #            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "\n",
    "# def get_GAT_representation(filePath, contextualEmbeddings, edgeIndices, edgeTypes):\n",
    "# #     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-GAT_train.pkl\") #replace it with key when deployed\n",
    "#     if key:\n",
    "#         print(\"Start of getting output of 1st GAT\")\n",
    "#         allInferredEdgetypes = []\n",
    "#         listAllEdgeIdx = []\n",
    "#         cherrypickedNodes = []\n",
    "#         for dialog, dialog_id in zip(contextualEmbeddings, range(len(contextualEmbeddings))):\n",
    "#             h_prime, attention_coef = gat_layer((dialog, edgeIndices[dialog_id]), edgeTypes[dialog_id])\n",
    "#             target_nodes = edgeIndices[dialog_id][1].tolist() # first idx represents dialogue id\n",
    "\n",
    "#             sample_edgetypes = {}\n",
    "#             for i in set(target_nodes):\n",
    "#                 sample_edgetypes[i] = []\n",
    "\n",
    "#             for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "#                 sample_edgetypes[target_node].append([edge_idx, h_prime[target_node][edge_idx].tolist()])\n",
    "\n",
    "#             list_edge_idx, inferred_edgetypes = get_inferred_edgetypes_GAT(sample_edgetypes,  edgeTypes[dialog_id])\n",
    "#             listAllEdgeIdx.append(list_edge_idx)\n",
    "#             allInferredEdgetypes.append(inferred_edgetypes)\n",
    "\n",
    "#             for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#                 cherrypickedNodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "\n",
    "#         cherrypickedNodes = torch.tensor(cherrypickedNodes)\n",
    "#         cherrypickedNodes.shape\n",
    "#         print(\"End of getting output of 1st GAT\")\n",
    "\n",
    "#         pickle.dump([cherrypickedNodes, allInferredEdgetypes],\n",
    "#                     open(filePath, 'wb'))\n",
    "\n",
    "#     else:\n",
    "#         file = open(filePath, 'rb')\n",
    "#         cherrypickedNodes, allInferredEdgetypes = pickle.load(file)\n",
    "#         file.close()\n",
    "\n",
    "#     return cherrypickedNodes, allInferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef42f2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # train data\n",
    "# cherrypickedNodesTrain, allInferredEdgetypesTrain = get_GAT_representation(\n",
    "#                                                     \"embed/h_prime_CNNBiLSTM-GAT_train.pkl\",\n",
    "#                                                     contextualEmbeddingsTrain,\n",
    "#                                                     edgeIndicesTrain,\n",
    "#                                                     edgeTypesTrain)\n",
    "# # only save the pickle data for test and validation\n",
    "# _, _ = get_GAT_representation(\"embed/h_prime_CNNBiLSTM-GAT_test.pkl\",\n",
    "#                         contextualEmbeddingsTest,\n",
    "#                         edgeIndicesTest,\n",
    "#                         edgeTypesTest)\n",
    "# # TODO add valid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de95f55",
   "metadata": {},
   "source": [
    "<h5> Visualize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e56aa1c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# labels = torch.tensor(trainLabels)\n",
    "# h_prime_np = cherrypickedNodesTrain.detach().numpy() (discontinued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5bd42",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# if runTSNE:\n",
    "#     # List of perplexity values to loop over\n",
    "#     perplexity_values = [30, 100]\n",
    "\n",
    "#     # Loop over each perplexity value\n",
    "#     for perplexity in perplexity_values:\n",
    "#         # Initialize t-SNE with the current perplexity value\n",
    "#         tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "#         # Fit and transform the data using t-SNE\n",
    "#         h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "#         # Plot the node embeddings with different colors for each label\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#             indices = (labels == label).nonzero().squeeze()\n",
    "#             plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "#         plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "#         plt.xlabel('Dimension 1', color=\"white\")\n",
    "#         plt.ylabel('Dimension 2', color=\"white\")\n",
    "#         plt.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caea68",
   "metadata": {},
   "source": [
    "<h4> Analyze the edgetypes of all train nodes in the context of a dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236b455",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming `all_inferred_edgetypes` and `y_train` are defined\n",
    "# df_eda = pd.DataFrame(\n",
    "#     {'edgetype': flatten_extend(allInferredEdgetypesTrain),\n",
    "#      'label': y_train,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7f357",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming `df_eda` and `CrosstabResult` are defined\n",
    "# CrosstabResult = pd.crosstab(index=df_eda['edgetype'], columns=df_eda['label'])\n",
    "\n",
    "# print(\"Crosstab Result:\")\n",
    "# print(CrosstabResult)\n",
    "# print()\n",
    "\n",
    "# # Performing Chi-squared test\n",
    "# ChiSqResult = chi2_contingency(CrosstabResult)\n",
    "\n",
    "# # P-Value is the Probability of H0 being True\n",
    "# # If P-Value > 0.05 then only we Accept the assumption(H0)\n",
    "# # H0: The variables are not correlated with each other.\n",
    "\n",
    "# print('The P-Value of the Chi-Squared Test is:', ChiSqResult[1])\n",
    "\n",
    "# if ChiSqResult[1] > 0.05:\n",
    "#     print(\"Variables are not correlated with each other\")\n",
    "# else:\n",
    "#     print(\"Two variables are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb0a7",
   "metadata": {},
   "source": [
    "<h3> Get EGAT output from each set of data (train, test, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "60061c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "egat = EGATConv(in_node_feats=len(contextualEmbeddingsTrain[0][0]),\n",
    "                    in_edge_feats=3,\n",
    "                    out_node_feats=64,\n",
    "                    out_edge_feats=3,\n",
    "                    num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc9167ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "def get_EGAT_representations(filePath, contextualEmbeddings, edgeIndices, edgeTypes, ranges):\n",
    "#     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-EGAT_train.pkl\")\n",
    "    if key:\n",
    "#         print(\"Start of getting output of 2nd GAT\")\n",
    "        inferredEdgetypes = []\n",
    "        allNodeFeats = []\n",
    "\n",
    "        # Iterate over each dialogue\n",
    "        for dialog_id in tqdm(range(len(edgeIndices)), desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "            startIdx, endIdx = ranges[dialog_id][0], ranges[dialog_id][1]\n",
    "            # Create a DGL graph\n",
    "            graph = dgl.graph((edgeIndices[dialog_id][0], edgeIndices[dialog_id][1]))\n",
    "\n",
    "            # Get one-hot encoded edge features\n",
    "            edge_feats = get_ohe(edgeTypes[dialog_id])\n",
    "\n",
    "            # Get outputs from the second GAT layer\n",
    "            egat_output = egat(graph, contextualEmbeddings[startIdx: endIdx+1], edge_feats)\n",
    "            new_node_feats, new_edge_feats = egat_output\n",
    "\n",
    "            # Compute mean edge features\n",
    "            mean_edge_feats = new_edge_feats.mean(dim=1)\n",
    "            allNodeFeats.append(new_node_feats.mean(dim=1).tolist())\n",
    "\n",
    "            # Prepare edge features for inference\n",
    "            target_nodes = edgeIndices[dialog_id][1].tolist()\n",
    "            sample_edgetypes = {}\n",
    "            for i in set(target_nodes):\n",
    "                sample_edgetypes[i] = []\n",
    "            for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "                sample_edgetypes[target_node].append([edge_idx, \n",
    "                                                      mean_edge_feats[edge_idx].tolist()])\n",
    "\n",
    "            # Infer edge types\n",
    "            sample_edgetypes = get_inferred_edgetypes_EGAT(sample_edgetypes, edgeTypes[dialog_id], \n",
    "                                                           len(contextualEmbeddings[startIdx: endIdx+1]), \n",
    "                                                           dialog_id)\n",
    "            inferredEdgetypes.append(sample_edgetypes)\n",
    "\n",
    "        # Flatten and convert node features to tensor\n",
    "        allNodeFeats = torch.tensor(flatten_extend(allNodeFeats))\n",
    "\n",
    "#         print(\"End of getting output of 2nd GAT\")\n",
    "\n",
    "        # Save the data to a pickle file\n",
    "        pickle.dump([allNodeFeats, inferredEdgetypes], open(filePath, 'wb'))\n",
    "    else:\n",
    "        # Load data from the existing pickle file\n",
    "        file = open(filePath, 'rb')\n",
    "        all_node_feats, inferredEdgetypes = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return allNodeFeats, inferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0afe0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 200])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_squeezedTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a62f0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edgeTypesTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "938645cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|███████████████████████████████████████████████████████| 2160/2160 [00:08<00:00, 241.01batch/s]\n",
      "Encoding Progress: 100%|█████████████████████████████████████████████████████████| 577/577 [00:02<00:00, 250.27batch/s]\n"
     ]
    }
   ],
   "source": [
    "tensor_squeezedTrain = contextualEmbeddingsTrain.squeeze(1)\n",
    "\n",
    "allNodeFeatsTrain, inferredEdgetypesTrain = get_EGAT_representations(\n",
    "                                        \"embed/h_prime_CNNBiLSTM-EGAT_train.pkl\",\n",
    "                                        tensor_squeezedTrain,\n",
    "                                        edgeIndicesTrain,\n",
    "                                        edgeTypesTrain,\n",
    "                                        rangesTrain\n",
    "                                )\n",
    "tensor_squeezedTest = contextualEmbeddingsTest.squeeze(1)\n",
    "_, _ = get_EGAT_representations(\n",
    "        \"embed/h_prime_CNNBiLSTM-EGAT_test.pkl\",\n",
    "        tensor_squeezedTest,\n",
    "        edgeIndicesTest,\n",
    "        edgeTypesTest, \n",
    "        rangesTest\n",
    ")\n",
    "#TODO do for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2d83d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda2 = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(inferredEdgetypesTrain),\n",
    "     'label': trainLabels,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf36e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab Result:\n",
      " label        0    1    2     3     4    5     6\n",
      "edgetype                                       \n",
      "0         1091  258  242  1644  4347  627  1136\n",
      "1          409  106   96   668  1613  249   354\n",
      "The P-Value of the ChiSq Test is: 0.03080562512893102\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from your data (df_eda2)\n",
    "# Assuming df_eda2 is already defined\n",
    "\n",
    "# Crosstabulation\n",
    "CrosstabResult2 = pd.crosstab(index=df_eda2['edgetype'], columns=df_eda2['label'])\n",
    "print(\"Crosstab Result:\\n\", CrosstabResult2)\n",
    "\n",
    "# Performing Chi-squared test\n",
    "ChiSqResult2 = chi2_contingency(CrosstabResult2)\n",
    "\n",
    "# Print the p-value of the Chi-squared test\n",
    "print('The P-Value of the ChiSq Test is:', ChiSqResult2[1])\n",
    "\n",
    "# Interpret the p-value\n",
    "if ChiSqResult2[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213c68a",
   "metadata": {},
   "source": [
    "Testing on 1 dialog data before scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "84641d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Node Features Shape: torch.Size([14, 4, 64])\n",
      "New Edge Features Shape: torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "dialog_id = 0\n",
    "\n",
    "# Create a DGL graph\n",
    "graph = dgl.graph((edgeIndicesTrain[dialog_id][0], edgeIndicesTrain[dialog_id][1]))\n",
    "\n",
    "# Obtain one-hot encoded edge features\n",
    "edge_feats = get_ohe(edgeTypesTrain[dialog_id])\n",
    "\n",
    "# Pass the graph, node representations, and edge features through the EGAT model\n",
    "tensor_squeezedTrain = contextualEmbeddingsTrain.squeeze(1)\n",
    "newNodeFeats, newEdgeFeats = egat(graph, tensor_squeezedTrain[0:14], edge_feats)\n",
    "\n",
    "# Print the shapes of the new node and edge features\n",
    "print(\"New Node Features Shape:\", newNodeFeats.shape)\n",
    "print(\"New Edge Features Shape:\", newEdgeFeats.shape)\n",
    "\n",
    "# Calculate the mean of node features along the second dimension (number of nodes)\n",
    "h_prime_mean = newNodeFeats.mean(dim=1)\n",
    "\n",
    "# Assuming you want to select only a subset of labels for visualization\n",
    "utt_size = 13\n",
    "labels = torch.tensor(trainLabels[:utt_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bbcf4d9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "h_prime_np = h_prime_mean.detach().numpy()\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# Plot the node embeddings with different colors for each label\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "    indices = (labels == label).nonzero().squeeze()\n",
    "    plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "plt.xlabel('Dimension 1', color=\"white\")\n",
    "plt.ylabel('Dimension 2', color=\"white\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9e75975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "851a250c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # Convert tensor to numpy array\n",
    "    h_prime_np = allNodeFeatsTrain.detach().numpy()\n",
    "    labels = torch.tensor(trainLabels)\n",
    "    \n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "        print(h_prime_tsne.shape)\n",
    "        \n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
