{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6e46ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os,re, time, pickle, collections, importlib, datetime, torch, nltk, pandas as pd, numpy as np, time\n",
    "from chardet import detect\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict, Counter\n",
    "from wordebd import WORDEBD\n",
    "from vocab import Vocab, Vectors\n",
    "from munch import Munch\n",
    "from cnnlstmseq import CNNLSTMseq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from model import batch_graphify, MaskedEdgeAttention, MaskedNLLLoss, LSTMModel\n",
    "from model import DATASET_PATH\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c1dda",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "- dataset_original\n",
    "- dataset_drop_noise\n",
    "- dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1098bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a273b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']\n",
    "\n",
    "def detect_misspelling(source):\n",
    "    pass\n",
    "\n",
    "def replace_spelling(source):\n",
    "    return re.sub(\"\", \"\", source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "08e24b0d",
   "metadata": {
    "code_folding": [
     15,
     31,
     34
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    '''\n",
    "    Preprocess text data\n",
    "    @param data: list of text examples\n",
    "    @return preprocessed_data: list of preprocessed text examples\n",
    "    '''\n",
    "    preprocessed_data = []\n",
    "    for example in data:\n",
    "        # Convert to lowercase\n",
    "#         example = example.lower()\n",
    "        # Remove punctuation\n",
    "        example = re.sub(r'[^\\w\\s]', '\\'', example)\n",
    "        preprocessed_data.append(example)\n",
    "    return preprocessed_data\n",
    "\n",
    "def load_pretrained_glove():\n",
    "    print(\"Loading GloVe...\")\n",
    "    glv_vector = {}\n",
    "    f = open('/embed/glove/glove.840B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float')\n",
    "        try:\n",
    "            glv_vector[word] = coefs\n",
    "        except ValueError:\n",
    "            continue\n",
    "    f.close()\n",
    "    start_time = time.time()\n",
    "    print(f\"Took {time.time() - start_time} seconds to load pretrained GloVe model.\")\n",
    "    return glv_vector\n",
    "\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "def _read_words(data, convmode=None):\n",
    "    '''    \n",
    "    Count the occurrences of all words\n",
    "    @param convmode: str, None for non conversational scope, 'naive' for classic or naive approach, 'conv' for conversation depth into account (one additional dim and nested values)\n",
    "    @param data: list of examples\n",
    "    @return words: list of words (with duplicates)\n",
    "    '''    \n",
    "    words = []\n",
    "    if convmode is None:\n",
    "        for example in data:\n",
    "            words += example.split()\n",
    "    return words\n",
    "\n",
    "def find_value_ranges(lst):\n",
    "    value_ranges = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            value_ranges.append((start_index, i - 1))\n",
    "            start_index = i\n",
    "\n",
    "    # Add the last range\n",
    "    value_ranges.append((start_index, len(lst) - 1))\n",
    "\n",
    "    return value_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c8fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e9ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0d2cf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12840, 12)\n",
      "(3400, 12)\n",
      "(1462, 12)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "X_train = pd.read_csv('data/' + dataset_path + '/train_sent_emo_dya.csv', encoding='shift_jis')\n",
    "X_test = pd.read_csv('data/' + dataset_path+ '/test_sent_emo_dya.csv', encoding='utf-8')\n",
    "X_dev = pd.read_csv('data/' + dataset_path + '/dev_sent_emo_dya.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first three rows\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1b24eca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5825e9de",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define features to drop\n",
    "drop_features = list(X_train.columns[6:]) \n",
    "\n",
    "# Create DataFrame for target labels\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "y_dev = pd.DataFrame()\n",
    "\n",
    "y_train[\"Emotion\"] = X_train[\"Emotion\"].copy()\n",
    "y_test[\"Emotion\"] = X_test[\"Emotion\"].copy()\n",
    "y_dev[\"Emotion\"] = X_dev[\"Emotion\"].copy()\n",
    "\n",
    "y_train[\"Dialogue_ID\"] = X_train[\"Dialogue_ID\"].copy()\n",
    "y_test[\"Dialogue_ID\"] = X_test[\"Dialogue_ID\"].copy()\n",
    "y_dev[\"Dialogue_ID\"] = X_dev[\"Dialogue_ID\"].copy()\n",
    "\n",
    "# Drop features from X_train DataFrame\n",
    "X_train = X_train.drop(drop_features, axis=1)\n",
    "X_test = X_test.drop(drop_features, axis=1)\n",
    "X_dev = X_dev.drop(drop_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d104ee",
   "metadata": {},
   "source": [
    "Before spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ad1731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[[\"Utterance\", \"Emotion\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "880afd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test[[\"Utterance\", \"Emotion\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b73b303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[\"Utterance\"] = X_train[\"Utterance\"].apply(lambda x: replace_spelling(x))\n",
    "# X_test[\"Utterance\"] = X_test[\"Utterance\"].apply(lambda x: replace_spelling(x))\n",
    "\n",
    "# X_train[\"Utterance\"] = preprocess_text(X_train[\"Utterance\"].tolist())\n",
    "# X_test[\"Utterance\"] = preprocess_text(X_test[\"Utterance\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "742805a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove instances based on index ranges\n",
    "def removeInstanceTurn1(X_set, Y_set, ranges):\n",
    "    indices_to_remove = []\n",
    "    for range_pair in ranges:\n",
    "        i = range_pair[0]\n",
    "        j = range_pair[1]\n",
    "        \n",
    "        if i == j:\n",
    "            indices_to_remove.append(i)\n",
    "#             print(X_set[\"Utterance\"].iloc[i])\n",
    "#             print(i, \" vs \", len(X_train))\n",
    "\n",
    "    X_set = X_set.drop(X_set.index[indices_to_remove])\n",
    "    Y_set = Y_set.drop(Y_set.index[indices_to_remove])\n",
    "    \n",
    "    # Reset index after dropping rows\n",
    "    X_set.reset_index(drop=True, inplace=True)\n",
    "    Y_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_set, Y_set\n",
    "\n",
    "\n",
    "X_train, y_train = removeInstanceTurn1(X_train, y_train, rangesTrain)\n",
    "X_test, y_test = removeInstanceTurn1(X_test, y_test, rangesTest)\n",
    "X_dev, y_dev = removeInstanceTurn1(X_dev, y_dev, rangesDev)\n",
    "\n",
    "# print(\"X_set_cleaned:\")\n",
    "# print(X_train_cleaned)\n",
    "# print(\"\\nY_set_cleaned:\")\n",
    "# print(Y_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "86036b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_encoder.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_decoder.pkl\")\n",
    "\n",
    "if not(checkFile1 and checkFile2):\n",
    "    labels = sorted(set(y_train.Emotion))\n",
    "    labelEncoder = {label: i for i, label in enumerate(labels)}\n",
    "    labelDecoder = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    pickle.dump(labelEncoder, open('data/dump/' + dataset_path + '/label_encoder.pkl', 'wb'))\n",
    "    pickle.dump(labelDecoder, open('data/dump/' + dataset_path + '/label_decoder.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('data/dump/' + dataset_path + '/label_encoder.pkl', 'rb')\n",
    "    file2 = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "    labelEncoder = pickle.load(file1)\n",
    "    labelDecoder = pickle.load(file2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6211e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the \"Emotion\" column in y_train\n",
    "if not isinstance(y_train[\"Emotion\"][0], np.int64):   \n",
    "    y_train[\"Emotion\"] = y_train[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "    y_test[\"Emotion\"] = y_test[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "    y_dev[\"Emotion\"] = y_dev[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "\n",
    "# Copy the encoded \"Emotion\" column from y_train to X_train\n",
    "X_train[\"Emotion\"] = y_train[\"Emotion\"].copy()\n",
    "X_test[\"Emotion\"] = y_test[\"Emotion\"].copy()\n",
    "X_dev[\"Emotion\"] = y_dev[\"Emotion\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0db6676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_train.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_test.pkl\")\n",
    "checkFile3 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_dev.pkl\")\n",
    "\n",
    "if not (checkFile1 and checkFile2 and checkFile3):\n",
    "    pickle.dump(X_train[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_train.pkl', 'wb'))\n",
    "    pickle.dump(X_test[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(X_dev[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_dev.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1bc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a1a4583",
   "metadata": {},
   "source": [
    "Creating an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19929c11",
   "metadata": {},
   "source": [
    "Testing on smaller data. Uncomment to see the size of updated representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "da518513",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of contextual embeddings: torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "model = transformers.BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Define your dialog data\n",
    "dialogs = [\n",
    "    \"How are you today?\",\n",
    "    \"I'm doing well, thank you!\",\n",
    "    \"That's good to hear.\",\n",
    "    \"Yes, it is.\",\n",
    "    \"Do you have any plans for the weekend?\",\n",
    "    \"Not really, just relaxing at home.\",\n",
    "    \"Sounds nice.\",\n",
    "    \"Indeed.\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the dialogs\n",
    "encoded_dialogs = [tokenizer.encode(dialog, add_special_tokens=True) for dialog in dialogs]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_length = max(len(dialog) for dialog in encoded_dialogs)\n",
    "padded_dialogs = [dialog + [tokenizer.pad_token_id] * (max_length - len(dialog)) for dialog in encoded_dialogs]\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = [[1] * len(dialog) + [0] * (max_length - len(dialog)) for dialog in encoded_dialogs]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_ids = torch.tensor(padded_dialogs)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# Obtain the BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "# Extract the contextual embeddings (CLS token)\n",
    "contextual_embeddings = outputs[0][:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "\n",
    "# Print the shape of the contextual embeddings\n",
    "print(\"Shape of contextual embeddings:\", contextual_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "79e3fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1578baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d7e17",
   "metadata": {},
   "source": [
    "This is just a duplicate of code above. Using this on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3718155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a89c08ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     also I was the point person on my company’s tr...\n",
       "1                      You must’ve had your hands full.\n",
       "2                               That I did. That I did.\n",
       "3         So let’s talk a little bit about your duties.\n",
       "4                                My duties?  All right.\n",
       "5     Now you’ll be heading a whole division, so you...\n",
       "6                                                I see.\n",
       "7     But there’ll be perhaps 30 people under you so...\n",
       "8                                         Good to know.\n",
       "9                                 We can go into detail\n",
       "10                               No don’t I beg of you!\n",
       "11    All right then, we’ll have a definite answer f...\n",
       "12                                             Really?!\n",
       "Name: Utterance, dtype: object"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"Utterance\"][rangesTrain[0][0]:rangesTrain[0][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77955986",
   "metadata": {},
   "source": [
    "Testing on small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "63595e55",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Load pre-trained BERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # List of text dialogs\n",
    "# dialogs = [\n",
    "#     [\"How are you today?\", \"I'm doing well, thank you!\"],\n",
    "#     [\"That's good to hear.\", \"Yes, it is.\", \"Do you have any plans for the weekend?\", \"Not really, just relaxing at home.\"],\n",
    "#     [\"Sounds nice.\", \"Indeed.\"]\n",
    "# ]\n",
    "\n",
    "# # List to store contextual embeddings for each utterance\n",
    "# contextual_embeddings = []\n",
    "\n",
    "# # Iterate through each dialog\n",
    "# for dialog in dialogs:\n",
    "#     # Tokenize and convert dialog to input IDs\n",
    "#     inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "#     # Get BERT model outputs\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#     # Extract contextual embeddings (CLS token represents the entire sequence)\n",
    "#     embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "\n",
    "#     # Store embeddings for each utterance in the dialog\n",
    "#     contextual_embeddings.append(embeddings)\n",
    "\n",
    "# # Print the list of contextual embeddings\n",
    "# print(\"List of Contextual Embeddings:\")\n",
    "# # for embedding in contextual_embeddings:\n",
    "# #     print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87280a72",
   "metadata": {},
   "source": [
    "#### Contexualized train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7abe85c5",
   "metadata": {
    "code_folding": [
     3,
     32
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Ranges: 2160it [00:00, 55304.10it/s]\n",
      "Processing Dialogs: 100%|██████████████████████████████████████████████████████████| 2160/2160 [07:25<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized train data - Elapsed time: 447.6954336166382 seconds\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_train.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair, iteration in tqdm(zip(rangesTrain, range(len(rangesTrain))), desc=\"Processing Ranges\"):\n",
    "        start_idx, end_idx = range_pair\n",
    "        dialog = list(X_train['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    # List to store contextual embeddings for each utterance\n",
    "    contextualEmbeddingsTrain = []\n",
    "\n",
    "    # Iterate through each dialog\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        # Tokenize and convert dialog to input IDs\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # Get BERT model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract contextual embeddings (CLS token represents the entire sequence)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "\n",
    "        # Store embeddings for each utterance in the dialog\n",
    "        contextualEmbeddingsTrain.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "            pickle.dump(contextualEmbeddingsTrain, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_train.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsTrain = pickle.load(file)\n",
    "        \n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized train data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1c3a4",
   "metadata": {},
   "source": [
    "<h4> Contexualize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8604a65f",
   "metadata": {
    "code_folding": [
     3,
     23
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 577/577 [00:00<00:00, 38644.53it/s]\n",
      "Processing Dialogs: 100%|████████████████████████████████████████████████████████████| 577/577 [02:02<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized test data - Elapsed time: 124.44786024093628 seconds\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_test.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair in tqdm(rangesTest):\n",
    "        start_idx, end_idx = range_pair            \n",
    "        dialog = list(X_test['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    contextualEmbeddingsTest = []\n",
    "\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "        contextualEmbeddingsTest.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(contextualEmbeddingsTest, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_test.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsTest = pickle.load(file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized test data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432631b1",
   "metadata": {},
   "source": [
    "<h4> Contexualize val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4bf80889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rangesDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "75f65f3f",
   "metadata": {
    "code_folding": [
     3,
     23
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:00<?, ?it/s]\n",
      "Processing Dialogs: 100%|████████████████████████████████████████████████████████████| 270/270 [00:51<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized test data - Elapsed time: 51.460155963897705 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_dev.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair in tqdm(rangesDev):\n",
    "        start_idx, end_idx = range_pair            \n",
    "        dialog = list(X_dev['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    contextualEmbeddingsDev = []\n",
    "\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "        contextualEmbeddingsDev.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(contextualEmbeddingsDev, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_dev.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsDev = pickle.load(file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized test data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b17e6",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "65ff6791",
   "metadata": {
    "code_folding": [
     4,
     19
    ]
   },
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTrain:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_train['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTrain.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTrain, rangesTrain], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\")\n",
    "    encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30584c74",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bd271d5c",
   "metadata": {
    "code_folding": [
     3,
     18
    ]
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTest:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_test['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTest.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTest, rangesTest], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_test.pkl', \"rb\")\n",
    "    encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b03ba",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "76fc8104",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\")\n",
    "encodedSpeakersDev = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesDev:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_dev['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersDev.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersDev, rangesDev], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_dev.pkl', \"rb\")\n",
    "    encodedSpeakersDev, rangesDev = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b1064",
   "metadata": {},
   "source": [
    "<h4>Getting data required for graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7851436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = int(contextualEmbeddingsTrain[0].shape[1]/2)\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "n_speakers=2\n",
    "max_seq_len=110\n",
    "window_past=0\n",
    "window_future=5\n",
    "# vocab_size=vocab_size\n",
    "n_classes=7\n",
    "listener_state=False\n",
    "context_attention='general'\n",
    "dropout=0.5\n",
    "nodal_attention=False\n",
    "no_cuda=True\n",
    "n_relations = 2 * n_speakers ** 2\n",
    "att_model = MaskedEdgeAttention(2 * D_e, max_seq_len, no_cuda)\n",
    "nodal_attention=True\n",
    "edge_type_mapping = {}\n",
    "for j in range(n_speakers):\n",
    "    for k in range(n_speakers):\n",
    "        edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "        edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d455d",
   "metadata": {},
   "source": [
    "Sample Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "83e33c37",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x_data = torch.tensor([\n",
    "#                         [[1, 2, 3, 0, 0],     # Utterance 1\n",
    "#                         [4, 5, 0, 0, 0]],    # Utterance 2\n",
    "#                        [[6, 7, 8, 9, 0],     # Utterance 3\n",
    "#                         [10, 0, 0, 0, 0]]])  # Utterance 4\n",
    "\n",
    "# umask_data = torch.tensor([[1, 1],        # Dialogue 1 has 2 utterances\n",
    "#                            [1, 0]])       # Dialogue 2 has 1 utterance\n",
    "# # features = cnn_feat_extractor(x_data, umask_data)\n",
    "# # emotions, hidden = lstm(features)\n",
    "\n",
    "# textf = x_data.squeeze(0)  # Remove batch dimension (1, utterance_size, embedding_size) -> (utterance_size, embedding_size)\n",
    "# umask = torch.FloatTensor([[1] * textf.size(0)])  # Adjust to (1, utterance_size)\n",
    "# lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b67ddfec",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print(textf)\n",
    "# print(umask)\n",
    "# print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "cae20761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodedSpeakersTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044a56a",
   "metadata": {},
   "source": [
    "June 5 possible mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "43df81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ranges in rangesDev:\n",
    "#     if ranges[1]-ranges[0] == 0:\n",
    "#         print(\"delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "37d0b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = 17\n",
    "# umask = torch.FloatTensor([[1] * 10 + [0] * (17 - 10)])\n",
    "# assert umask.size(1) == max_seq_len, f\"umask size is {umask.size(1)}, expected {max_seq_len}\"\n",
    "# lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "# print(umask)\n",
    "# print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "07b3fa4d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, contextEmbeddings, rangesSet, encodedSpeakersSet):\n",
    "#         print(contextEmbeddings[0])\n",
    "        self.contextEmbeddings = contextEmbeddings\n",
    "        self.rangesSet = rangesSet\n",
    "        self.encodedSpeakersSet = encodedSpeakersSet\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rangesSet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        startIdx, endIdx = self.rangesSet[idx]\n",
    "#         sequence = self.X_set[\"sequence\"][startIdx:endIdx+1].tolist()\n",
    "        convs = self.contextEmbeddings[idx]\n",
    "        qmask = self.encodedSpeakersSet[startIdx: endIdx+1]\n",
    "        return convs, qmask\n",
    "\n",
    "# Define the ContextEncoding function\n",
    "def ContextEncoding(file_path, dataset):\n",
    "    all_emotions, all_umask, all_seq_lengths = [], [], []\n",
    "    all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths = [], [], [], [], []\n",
    "    max_seq_len = 30\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    for convs, qmask in tqdm(dataloader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        convs = convs[0]\n",
    "        textf = convs.unsqueeze(1)\n",
    "\n",
    "        umask = torch.FloatTensor([[1] * len(textf) + [0] * (max_seq_len - len(textf))])\n",
    "        assert umask.size(1) == max_seq_len, f\"umask size is {umask.size(1)}, expected {max_seq_len}\"\n",
    "        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "        \n",
    "        features, edge_index, \\\n",
    "        edge_norm, edge_type, \\\n",
    "        edge_index_lengths = batch_graphify(textf, \n",
    "                                            qmask,\n",
    "                                            lengths,\n",
    "                                            window_past,\n",
    "                                            window_future,\n",
    "                                            edge_type_mapping,\n",
    "                                            att_model, \n",
    "                                            no_cuda)\n",
    "        all_umask.append(umask)\n",
    "        all_seq_lengths.append(lengths)\n",
    "        all_features.append(features)\n",
    "        all_edge_index.append(edge_index)\n",
    "        all_edge_norm.append(edge_norm)\n",
    "        all_edge_type.append(edge_type)\n",
    "        all_edge_index_lengths.append(edge_index_lengths)\n",
    "        \n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([   all_umask, \\\n",
    "                        all_seq_lengths,\n",
    "                        all_features, \\\n",
    "                        all_edge_index, \\\n",
    "                        all_edge_norm, \\\n",
    "                        all_edge_type, \\\n",
    "                        all_edge_index_lengths], file)\n",
    "    \n",
    "    return all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3c998a81",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|███████████████████████████████████████████████████████| 2160/2160 [00:06<00:00, 313.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8.484763145446777 seconds to encode train text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|█████████████████████████████████████████████████████████| 577/577 [00:01<00:00, 316.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 2.674950361251831 seconds to encode test text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|█████████████████████████████████████████████████████████| 270/270 [00:00<00:00, 331.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.9863080978393555 seconds to encode test text\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_path1 = 'embed/' + dataset_path + '/pre_h_prime_BERT_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/pre_h_prime_BERT_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/pre_h_prime_BERT_dev.pkl'\n",
    "\n",
    "# Check if files exist\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if not checkFile1:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    trainDataset = ContextDataset(contextualEmbeddingsTrain, rangesTrain, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_features, \\\n",
    "     all_edge_index, \\\n",
    "     all_edge_norm, \\\n",
    "     all_edge_type, \\\n",
    "     all_edge_index_lengths = ContextEncoding(file_path1, trainDataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train text\")\n",
    "\n",
    "if not checkFile2:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTest for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    testDataset = ContextDataset(contextualEmbeddingsTest, rangesTest, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    testContext, _, _, _, _ = ContextEncoding(file_path2, testDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "    \n",
    "if not checkFile3:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersDev for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    devDataset = ContextDataset(contextualEmbeddingsDev, rangesDev, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    devContext, _, _, _, _ = ContextEncoding(file_path3, devDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3:\n",
    "#     with open(file_path1[0], 'rb') as file1:\n",
    "#         trainContext = pickle.load(file1)\n",
    "    with open(file_path1, 'rb') as file1:\n",
    "        _, _, features, edge_index, \\\n",
    "        edge_norm, edge_type, edge_index_lengths = pickle.load(file1)     \n",
    "#     with open(file_path2[0], 'rb') as file2:\n",
    "#         testContext = pickle.load(file2)\n",
    "#     with open(file_path3[0], 'rb') as file3:\n",
    "#         devContext = pickle.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e620748f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 768])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualEmbeddingsTrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c3ac9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "print(len(contextualEmbeddingsTrain))\n",
    "print(len(contextualEmbeddingsTest))\n",
    "print(len(contextualEmbeddingsDev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a19cb",
   "metadata": {},
   "source": [
    "Unsupervised visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b5f1de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6144])\n"
     ]
    }
   ],
   "source": [
    "# Assuming contextual_embeddings is your list of contextual embeddings\n",
    "\n",
    "# Flatten the list of contextual embeddings into a single list\n",
    "flattened_embeddings = [emb for dialogue in contextual_embeddings for emb in dialogue]\n",
    "\n",
    "# Convert the flattened list into a single tensor\n",
    "tensor_data = torch.tensor(flattened_embeddings)\n",
    "\n",
    "# Check the shape of the tensor\n",
    "print(tensor_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "12995d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'anger',\n",
       " 1: 'disgust',\n",
       " 2: 'fear',\n",
       " 3: 'joy',\n",
       " 4: 'neutral',\n",
       " 5: 'sadness',\n",
       " 6: 'surprise'}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d362bd",
   "metadata": {},
   "source": [
    "Distribution of labels in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3d031c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 1500 train occurrences\n",
      "disgust: 364 train occurrences\n",
      "fear: 338 train occurrences\n",
      "joy: 2312 train occurrences\n",
      "neutral: 5960 train occurrences\n",
      "sadness: 876 train occurrences\n",
      "surprise: 1490 train occurrences\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts for each unique label\n",
    "uniqueLabelsTrain, labelCountsTrain = np.unique(list(X_train[\"Emotion\"]), return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(uniqueLabelsTrain, labelCountsTrain):\n",
    "    print(f\"{labelDecoder[label]}: {count} train occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7a401",
   "metadata": {},
   "source": [
    "Distribution of labels in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9627c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 516 test occurrences\n",
      "disgust: 99 test occurrences\n",
      "fear: 60 test occurrences\n",
      "joy: 495 test occurrences\n",
      "neutral: 1615 test occurrences\n",
      "sadness: 263 test occurrences\n",
      "surprise: 352 test occurrences\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts for each unique label\n",
    "uniqueLabelsTest, labelCountsTest = np.unique(list(X_test[\"Emotion\"]), return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(uniqueLabelsTest, labelCountsTest):\n",
    "    print(f\"{labelDecoder[label]}: {count} test occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ef1a4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this to true if you finish the context encoding, also manually change the file\n",
    "key = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e06edd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Utterance   Emotion  \\\n",
      "0  Why do all you’re coffee mugs have numbers on ...  surprise   \n",
      "1  Oh. That’s so Monica can keep track. That way ...     anger   \n",
      "2                                       Y'know what?   neutral   \n",
      "3                                              Okay.   neutral   \n",
      "4  Ross, didn't you say that there was an elevato...   neutral   \n",
      "\n",
      "  BERT_predicted_label EGAT_predicted_label  \n",
      "0              neutral              neutral  \n",
      "1              neutral              neutral  \n",
      "2              neutral              neutral  \n",
      "3              neutral              neutral  \n",
      "4              neutral              neutral  \n"
     ]
    }
   ],
   "source": [
    "def apply_label_decoder(df, columns, decoder):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].map(decoder)\n",
    "    return df\n",
    "\n",
    "def load_and_concatenate_predictions(dataset_path):\n",
    "    # Load predictions from pickle files\n",
    "    file_name = f\"data/dump/{dataset_path}/BERT_data_for_classifier/BERT_predictedTest.pkl\"\n",
    "    with open(file_name, 'rb') as file:\n",
    "        df_predictions_BERT = pickle.load(file)\n",
    "\n",
    "    file_name = f\"data/dump/{dataset_path}/BERT_data_for_classifier/egat_predictedTest.pkl\"\n",
    "    with open(file_name, 'rb') as file:\n",
    "        df_predictions_BERT_egat = pickle.load(file)\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    df_concatenated = pd.concat([\n",
    "        X_test[[\"Utterance\", \"Emotion\"]],\n",
    "        df_predictions_BERT[\"predicted_label\"].rename(\"BERT_predicted_label\"),\n",
    "        df_predictions_BERT_egat[\"predicted_label\"].rename(\"EGAT_predicted_label\")\n",
    "    ], axis=1)\n",
    "\n",
    "    # Apply label decoder to specified columns\n",
    "    df_concatenated = apply_label_decoder(df_concatenated, [\"Emotion\", \"BERT_predicted_label\", \"EGAT_predicted_label\"], labelDecoder)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    output_file = f\"data/dump/{dataset_path}/BERT_data_for_classifier/test_predictions.csv\"\n",
    "    df_concatenated.to_csv(output_file, index=False)\n",
    "\n",
    "    return df_concatenated\n",
    "\n",
    "# Example usage:\n",
    "if key:\n",
    "    concatenated_df = load_and_concatenate_predictions(dataset_path)\n",
    "    print(concatenated_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c07b8f",
   "metadata": {},
   "source": [
    "Visualize utterance embeddnig (u') with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "3d430fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.tensor(X_train[\"Emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f1043832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "aed63efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runTSNE = 1\n",
    "# if runTSNE:\n",
    "#     from sklearn.manifold import TSNE\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     # List of perplexity values to loop over\n",
    "#     perplexity_values = [50]\n",
    "\n",
    "#     # Loop over each perplexity value\n",
    "#     for perplexity in perplexity_values:\n",
    "#         # Initialize t-SNE with the current perplexity value\n",
    "#         tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "#         # Fit and transform the data using t-SNE\n",
    "#         h_prime_tsne = tsne.fit_transform(tensor_data.detach().numpy())\n",
    "\n",
    "#         # Plot the node embeddings with different colors for each label\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         for label, emotion in zip(range(len(label_encoder)), label_encoder):\n",
    "#             indices = (labels == label).nonzero().squeeze()\n",
    "#             plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "#         plt.title(f'Utterance Embeddings (Train) Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "#         plt.xlabel('Dimension 1', color=\"white\")\n",
    "#         plt.ylabel('Dimension 2', color=\"white\")\n",
    "#         plt.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e2170c",
   "metadata": {},
   "source": [
    " Visualize utterance embedding (u') with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "91e6dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(tensor_data.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(labels):\n",
    "#     indices = labels == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
