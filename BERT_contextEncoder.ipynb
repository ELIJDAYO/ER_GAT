{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e46ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re, time, pickle, collections, importlib, datetime, torch, nltk, pandas as pd, numpy as np, time\n",
    "from chardet import detect\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict, Counter\n",
    "from wordebd import WORDEBD\n",
    "from vocab import Vocab, Vectors\n",
    "from munch import Munch\n",
    "from cnnlstmseq import CNNLSTMseq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from model import batch_graphify, MaskedEdgeAttention, MaskedNLLLoss, \\\n",
    "LSTMModel, GRUModel, DialogRNNModel, DialogueGCNModel, DialogueGCN_DailyModel\n",
    "from model import DATASET_PATH\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f23de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c1dda",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "- dataset_original\n",
    "- dataset_drop_noise\n",
    "- dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1098bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a273b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e24b0d",
   "metadata": {
    "code_folding": [
     0,
     15,
     31,
     34
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_glove():\n",
    "    print(\"Loading GloVe...\")\n",
    "    glv_vector = {}\n",
    "    f = open('/embed/glove/glove.840B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float')\n",
    "        try:\n",
    "            glv_vector[word] = coefs\n",
    "        except ValueError:\n",
    "            continue\n",
    "    f.close()\n",
    "    start_time = time.time()\n",
    "    print(f\"Took {time.time() - start_time} seconds to load pretrained GloVe model.\")\n",
    "    return glv_vector\n",
    "\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "def _read_words(data, convmode=None):\n",
    "    '''    \n",
    "    Count the occurrences of all words\n",
    "    @param convmode: str, None for non conversational scope, 'naive' for classic or naive approach, 'conv' for conversation depth into account (one additional dim and nested values)\n",
    "    @param data: list of examples\n",
    "    @return words: list of words (with duplicates)\n",
    "    '''    \n",
    "    words = []\n",
    "    if convmode is None:\n",
    "        for example in data:\n",
    "            words += example.split()\n",
    "    return words\n",
    "\n",
    "def find_value_ranges(lst):\n",
    "    value_ranges = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            value_ranges.append((start_index, i - 1))\n",
    "            start_index = i\n",
    "\n",
    "    # Add the last range\n",
    "    value_ranges.append((start_index, len(lst) - 1))\n",
    "\n",
    "    return value_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6b155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Utterance', 'Speaker', 'Emotion', 'Sentiment',\n",
      "       'Dialogue_ID', 'Utterance_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data\\DatasetPreparation\\X_train.csv', encoding='shift_jis')\n",
    "\n",
    "# Print the column names\n",
    "print(df.columns)\n",
    "\n",
    "columns_to_use = df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2cf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12176, 6)\n",
      "(12176, 3)\n",
      "(3230, 6)\n",
      "(3230, 3)\n",
      "(1373, 6)\n",
      "(1373, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "X_train = pd.read_csv('data\\DatasetPreparation\\X_train.csv', encoding='shift_jis', usecols=columns_to_use)\n",
    "X_test = pd.read_csv('data\\DatasetPreparation\\X_test.csv', encoding='shift_jis', usecols=columns_to_use)\n",
    "X_dev = pd.read_csv('data\\DatasetPreparation\\X_dev.csv', encoding='shift_jis', usecols=columns_to_use)\n",
    "\n",
    "y_train = pd.read_csv('data\\DatasetPreparation\\y_train.csv', encoding='shift_jis')\n",
    "y_test = pd.read_csv('data\\DatasetPreparation\\y_test.csv', encoding='shift_jis')\n",
    "y_dev = pd.read_csv('data\\DatasetPreparation\\y_dev.csv', encoding='shift_jis')\n",
    "\n",
    "# Display the first three rows\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1731f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also point person company transition system</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>must hand full</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>let talk little bit duty</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duty right</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heading whole division lot duty</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>see</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perhaps people dump certain amount</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good know</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>go detail</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beg</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Utterance  Speaker  Emotion Sentiment  \\\n",
       "0  also point person company transition system        0        4   neutral   \n",
       "1                               must hand full        1        4   neutral   \n",
       "2                     let talk little bit duty        1        4   neutral   \n",
       "3                                   duty right        0        6  positive   \n",
       "4              heading whole division lot duty        1        4   neutral   \n",
       "5                                          see        0        4   neutral   \n",
       "6           perhaps people dump certain amount        1        4   neutral   \n",
       "7                                    good know        0        4   neutral   \n",
       "8                                    go detail        1        4   neutral   \n",
       "9                                          beg        0        2  negative   \n",
       "\n",
       "   Dialogue_ID  Utterance_ID  \n",
       "0            0             0  \n",
       "1            0             1  \n",
       "2            0             3  \n",
       "3            0             4  \n",
       "4            0             5  \n",
       "5            0             6  \n",
       "6            0             7  \n",
       "7            0             8  \n",
       "8            0             9  \n",
       "9            0            10  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86036b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_encoder.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_decoder.pkl\")\n",
    "\n",
    "if not(checkFile1 and checkFile2):\n",
    "    labels = sorted(set(y_train.Emotion))\n",
    "    labelEncoder = {label: i for i, label in enumerate(labels)}\n",
    "    labelDecoder = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    pickle.dump(labelEncoder, open('data/dump/' + dataset_path + '/.pkl', 'wb'))\n",
    "    pickle.dump(labelDecoder, open('data/dump/' + dataset_path + '/label_decoder.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('data/dump/' + dataset_path + '/label_encoder.pkl', 'rb')\n",
    "    file2 = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "    labelEncoder = pickle.load(file1)\n",
    "    labelDecoder = pickle.load(file2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d0af03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'joy': 3,\n",
       " 'neutral': 4,\n",
       " 'sadness': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db6676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_train.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_test.pkl\")\n",
    "checkFile3 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_dev.pkl\")\n",
    "\n",
    "if not (checkFile1 and checkFile2 and checkFile3):\n",
    "    pickle.dump(X_train[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_train.pkl', 'wb'))\n",
    "    pickle.dump(X_test[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(X_dev[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_dev.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a4583",
   "metadata": {},
   "source": [
    "Creating an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19929c11",
   "metadata": {},
   "source": [
    "Testing on smaller data. Uncomment to see the size of updated representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da518513",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of contextual embeddings: torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "model = transformers.BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Define your dialog data\n",
    "dialogs = [\n",
    "    \"How are you today?\",\n",
    "    \"I'm doing well, thank you!\",\n",
    "    \"That's good to hear.\",\n",
    "    \"Yes, it is.\",\n",
    "    \"Do you have any plans for the weekend?\",\n",
    "    \"Not really, just relaxing at home.\",\n",
    "    \"Sounds nice.\",\n",
    "    \"Indeed.\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the dialogs\n",
    "encoded_dialogs = [tokenizer.encode(dialog, add_special_tokens=True) for dialog in dialogs]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_length = max(len(dialog) for dialog in encoded_dialogs)\n",
    "padded_dialogs = [dialog + [tokenizer.pad_token_id] * (max_length - len(dialog)) for dialog in encoded_dialogs]\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = [[1] * len(dialog) + [0] * (max_length - len(dialog)) for dialog in encoded_dialogs]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_ids = torch.tensor(padded_dialogs)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# Obtain the BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "# Extract the contextual embeddings (CLS token)\n",
    "contextual_embeddings = outputs[0][:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "\n",
    "# Print the shape of the contextual embeddings\n",
    "print(\"Shape of contextual embeddings:\", contextual_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e3fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1578baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d7e17",
   "metadata": {},
   "source": [
    "This is just a duplicate of code above. Using this on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3718155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2158\n",
      "577\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87280a72",
   "metadata": {},
   "source": [
    "#### Contexualized train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7abe85c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized train data - Elapsed time: 0.18141508102416992 seconds\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_train.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair, iteration in tqdm(zip(rangesTrain, range(len(rangesTrain))), desc=\"Processing Ranges\"):\n",
    "        start_idx, end_idx = range_pair\n",
    "        dialog = list(X_train['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    # List to store contextual embeddings for each utterance\n",
    "    contextualEmbeddingsTrain = []\n",
    "\n",
    "    # Iterate through each dialog\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        # Tokenize and convert dialog to input IDs\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # Get BERT model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract contextual embeddings (CLS token represents the entire sequence)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "\n",
    "        # Store embeddings for each utterance in the dialog\n",
    "        contextualEmbeddingsTrain.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "            pickle.dump(contextualEmbeddingsTrain, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_train.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsTrain = pickle.load(file)\n",
    "        \n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized train data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1c3a4",
   "metadata": {},
   "source": [
    "<h4> Contexualize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8604a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized test data - Elapsed time: 0.04954648017883301 seconds\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_test.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair in tqdm(rangesTest):\n",
    "        start_idx, end_idx = range_pair            \n",
    "        dialog = list(X_test['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    contextualEmbeddingsTest = []\n",
    "\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "        contextualEmbeddingsTest.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(contextualEmbeddingsTest, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_test.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsTest = pickle.load(file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized test data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432631b1",
   "metadata": {},
   "source": [
    "<h4> Contexualize val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75f65f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized test data - Elapsed time: 0.027005672454833984 seconds\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_dev.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair in tqdm(rangesDev):\n",
    "        start_idx, end_idx = range_pair            \n",
    "        dialog = list(X_dev['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    contextualEmbeddingsDev = []\n",
    "\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "        contextualEmbeddingsDev.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(contextualEmbeddingsDev, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_dev.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsDev = pickle.load(file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized test data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b17e6",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65ff6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTrain:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_train['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTrain.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTrain, rangesTrain], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\")\n",
    "    encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30584c74",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd271d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTest:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_test['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTest.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTest, rangesTest], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_test.pkl', \"rb\")\n",
    "    encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b03ba",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76fc8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\")\n",
    "encodedSpeakersDev = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesDev:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_dev['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersDev.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersDev, rangesDev], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_dev.pkl', \"rb\")\n",
    "    encodedSpeakersDev, rangesDev = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b1064",
   "metadata": {},
   "source": [
    "<h4>Getting data required for graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7851436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 384\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "n_speakers=2\n",
    "max_seq_len=110\n",
    "window_past=0\n",
    "window_future=5\n",
    "# vocab_size=vocab_size\n",
    "n_classes=7\n",
    "listener_state=False\n",
    "context_attention='general'\n",
    "dropout=0.5\n",
    "nodal_attention=False\n",
    "no_cuda=True\n",
    "n_relations = 2 * n_speakers ** 2\n",
    "att_model = MaskedEdgeAttention(2 * D_e, max_seq_len, no_cuda)\n",
    "nodal_attention=True\n",
    "edge_type_mapping = {}\n",
    "for j in range(n_speakers):\n",
    "    for k in range(n_speakers):\n",
    "        edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "        edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec76371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768]) (418, 418)\n",
      "torch.Size([1, 768]) (419, 419)\n",
      "torch.Size([1, 768]) (422, 422)\n",
      "torch.Size([1, 768]) (549, 549)\n",
      "torch.Size([1, 768]) (1611, 1611)\n",
      "torch.Size([1, 768]) (1612, 1612)\n",
      "torch.Size([1, 768]) (2116, 2116)\n",
      "torch.Size([1, 768]) (2117, 2117)\n",
      "torch.Size([1, 768]) (2442, 2442)\n",
      "torch.Size([1, 768]) (2546, 2546)\n"
     ]
    }
   ],
   "source": [
    "for sample, ranges in zip(contextualEmbeddingsTest, rangesTest):\n",
    "    if ranges[1] - ranges[0] == 0:\n",
    "        print(sample.shape, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c998a81",
   "metadata": {
    "code_folding": [
     73,
     89,
     111
    ]
   },
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, contextEmbeddings, rangesSet, encodedSpeakersSet):\n",
    "        # print(contextEmbeddings[0])\n",
    "        self.contextEmbeddings = contextEmbeddings\n",
    "        self.rangesSet = rangesSet\n",
    "        self.encodedSpeakersSet = encodedSpeakersSet\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rangesSet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        startIdx, endIdx = self.rangesSet[idx]\n",
    "#         sequence = self.X_set[\"sequence\"][startIdx:endIdx+1].tolist()\n",
    "        convs = self.contextEmbeddings[idx]\n",
    "        qmask = self.encodedSpeakersSet[startIdx: endIdx+1]\n",
    "        return convs, qmask\n",
    "\n",
    "# Define the ContextEncoding function\n",
    "def ContextEncoding(file_path, dataset):\n",
    "    all_emotions, all_umask, all_seq_lengths = [], [], []\n",
    "    all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths = [], [], [], [], []\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    for convs, qmask in tqdm(dataloader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        convs = convs[0]\n",
    "        textf = convs.unsqueeze(1)\n",
    "\n",
    "        umask = torch.FloatTensor([[1] * len(textf)])  # Adjust to (1, utterance_size)\n",
    "#         print(umask)\n",
    "        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "#         print(\"qmask.shape: \", qmask.shape, lengths)\n",
    "#         U = cnn_feat_extractor(textf, umask)\n",
    "#         emotions, hidden = lstm(U)\n",
    "#         all_emotions.append(emotions)\n",
    "        \n",
    "        features, edge_index, \\\n",
    "        edge_norm, edge_type, \\\n",
    "        edge_index_lengths = batch_graphify(textf, \n",
    "                                            qmask,\n",
    "                                            lengths,\n",
    "                                            window_past,\n",
    "                                            window_future,\n",
    "                                            edge_type_mapping,\n",
    "                                            att_model, \n",
    "                                            no_cuda)\n",
    "        all_umask.append(umask)\n",
    "        all_seq_lengths.append(lengths)\n",
    "        all_features.append(features)\n",
    "        all_edge_index.append(edge_index)\n",
    "        all_edge_norm.append(edge_norm)\n",
    "        all_edge_type.append(edge_type)\n",
    "        all_edge_index_lengths.append(edge_index_lengths)\n",
    "        \n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([   all_umask, \\\n",
    "                        all_seq_lengths,\n",
    "                        all_features, \\\n",
    "                        all_edge_index, \\\n",
    "                        all_edge_norm, \\\n",
    "                        all_edge_type, \\\n",
    "                        all_edge_index_lengths], file)\n",
    "    \n",
    "    return all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths\n",
    "\n",
    "# File paths\n",
    "file_path1 = 'embed/' + dataset_path + '/pre_h_prime_BERT_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/pre_h_prime_BERT_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/pre_h_prime_BERT_dev.pkl'\n",
    "\n",
    "# Check if files exist\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if not checkFile1:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    trainDataset = ContextDataset(contextualEmbeddingsTrain, rangesTrain, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_features, \\\n",
    "     all_edge_index, \\\n",
    "     all_edge_norm, \\\n",
    "     all_edge_type, \\\n",
    "     all_edge_index_lengths = ContextEncoding(file_path1, trainDataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train text\")\n",
    "\n",
    "if not checkFile2:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTest for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    testDataset = ContextDataset(contextualEmbeddingsTest, rangesTest, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    testContext, _, _, _, _ = ContextEncoding(file_path2, testDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "    \n",
    "if not checkFile3:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersDev for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    devDataset = ContextDataset(contextualEmbeddingsDev, rangesDev, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    devContext, _, _, _, _ = ContextEncoding(file_path3, devDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3:\n",
    "#     with open(file_path1[0], 'rb') as file1:\n",
    "#         trainContext = pickle.load(file1)\n",
    "    with open(file_path1, 'rb') as file1:\n",
    "        _, _, features, edge_index, \\\n",
    "        edge_norm, edge_type, edge_index_lengths = pickle.load(file1)     \n",
    "#     with open(file_path2[0], 'rb') as file2:\n",
    "#         testContext = pickle.load(file2)\n",
    "#     with open(file_path3[0], 'rb') as file3:\n",
    "#         devContext = pickle.load(file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a19cb",
   "metadata": {},
   "source": [
    "Unsupervised visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f1de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6144])\n"
     ]
    }
   ],
   "source": [
    "# Assuming contextual_embeddings is your list of contextual embeddings\n",
    "\n",
    "# Flatten the list of contextual embeddings into a single list\n",
    "flattened_embeddings = [emb for dialogue in contextual_embeddings for emb in dialogue]\n",
    "\n",
    "# Convert the flattened list into a single tensor\n",
    "tensor_data = torch.tensor(flattened_embeddings)\n",
    "\n",
    "# Check the shape of the tensor\n",
    "print(tensor_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12995d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'anger',\n",
       " 1: 'disgust',\n",
       " 2: 'fear',\n",
       " 3: 'joy',\n",
       " 4: 'neutral',\n",
       " 5: 'sadness',\n",
       " 6: 'surprise'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d362bd",
   "metadata": {},
   "source": [
    "Distribution of labels in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d031c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 1445 train occurrences\n",
      "disgust: 356 train occurrences\n",
      "fear: 320 train occurrences\n",
      "joy: 2241 train occurrences\n",
      "neutral: 5658 train occurrences\n",
      "sadness: 853 train occurrences\n",
      "surprise: 1303 train occurrences\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts for each unique label\n",
    "uniqueLabelsTrain, labelCountsTrain = np.unique(list(X_train[\"Emotion\"]), return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(uniqueLabelsTrain, labelCountsTrain):\n",
    "    print(f\"{labelDecoder[label]}: {count} train occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7a401",
   "metadata": {},
   "source": [
    "Distribution of labels in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9627c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 500 test occurrences\n",
      "disgust: 96 test occurrences\n",
      "fear: 56 test occurrences\n",
      "joy: 476 test occurrences\n",
      "neutral: 1541 test occurrences\n",
      "sadness: 258 test occurrences\n",
      "surprise: 303 test occurrences\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts for each unique label\n",
    "uniqueLabelsTest, labelCountsTest = np.unique(list(X_test[\"Emotion\"]), return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(uniqueLabelsTest, labelCountsTest):\n",
    "    print(f\"{labelDecoder[label]}: {count} test occurrences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
