{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6e46ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os,re, time, pickle, collections, importlib, datetime, torch, nltk, pandas as pd, numpy as np, time\n",
    "from chardet import detect\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict, Counter\n",
    "from wordebd import WORDEBD\n",
    "from vocab import Vocab, Vectors\n",
    "from munch import Munch\n",
    "from cnnlstmseq import CNNLSTMseq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from model import batch_graphify, MaskedEdgeAttention, MaskedNLLLoss, LSTMModel\n",
    "from model import DATASET_PATH\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c1dda",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "- dataset_original\n",
    "- dataset_drop_noise\n",
    "- dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1098bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH\n",
    "key=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a273b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']\n",
    "\n",
    "def detect_misspelling(source):\n",
    "    pass\n",
    "\n",
    "def replace_spelling(source):\n",
    "    return re.sub(\"\", \"\", source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "08e24b0d",
   "metadata": {
    "code_folding": [
     15,
     31,
     34
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    '''\n",
    "    Preprocess text data\n",
    "    @param data: list of text examples\n",
    "    @return preprocessed_data: list of preprocessed text examples\n",
    "    '''\n",
    "    preprocessed_data = []\n",
    "    for example in data:\n",
    "        # Convert to lowercase\n",
    "#         example = example.lower()\n",
    "        # Remove punctuation\n",
    "        example = re.sub(r'[^\\w\\s]', '\\'', example)\n",
    "        preprocessed_data.append(example)\n",
    "    return preprocessed_data\n",
    "\n",
    "def load_pretrained_glove():\n",
    "    print(\"Loading GloVe...\")\n",
    "    glv_vector = {}\n",
    "    f = open('/embed/glove/glove.840B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float')\n",
    "        try:\n",
    "            glv_vector[word] = coefs\n",
    "        except ValueError:\n",
    "            continue\n",
    "    f.close()\n",
    "    start_time = time.time()\n",
    "    print(f\"Took {time.time() - start_time} seconds to load pretrained GloVe model.\")\n",
    "    return glv_vector\n",
    "\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "def _read_words(data, convmode=None):\n",
    "    '''    \n",
    "    Count the occurrences of all words\n",
    "    @param convmode: str, None for non conversational scope, 'naive' for classic or naive approach, 'conv' for conversation depth into account (one additional dim and nested values)\n",
    "    @param data: list of examples\n",
    "    @return words: list of words (with duplicates)\n",
    "    '''    \n",
    "    words = []\n",
    "    if convmode is None:\n",
    "        for example in data:\n",
    "            words += example.split()\n",
    "    return words\n",
    "\n",
    "def find_value_ranges(lst):\n",
    "    value_ranges = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            value_ranges.append((start_index, i - 1))\n",
    "            start_index = i\n",
    "\n",
    "    # Add the last range\n",
    "    value_ranges.append((start_index, len(lst) - 1))\n",
    "\n",
    "    return value_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0d2cf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12840, 12)\n",
      "(3400, 12)\n",
      "(1462, 12)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "\n",
    "if dataset_path == \"dataset_drop_noise\":\n",
    "    X_train = pd.read_csv('data/' + dataset_path + '/X_train.csv', encoding='shift_jis')\n",
    "    X_test = pd.read_csv('data/' + dataset_path+ '/X_test.csv', encoding='utf-8')\n",
    "    X_dev = pd.read_csv('data/' + dataset_path + '/X_dev.csv', encoding='utf-8')\n",
    "else:\n",
    "    X_train = pd.read_csv('data/' + dataset_path + '/train_sent_emo_dya.csv', encoding='shift_jis')\n",
    "    X_test = pd.read_csv('data/' + dataset_path+ '/test_sent_emo_dya.csv', encoding='utf-8')\n",
    "    X_dev = pd.read_csv('data/' + dataset_path + '/dev_sent_emo_dya.csv', encoding='utf-8')\n",
    "# Display the first three rows\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "07f1bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFreqTurns(X_set):\n",
    "    utterance_counts = X_set.groupby('Dialogue_ID')['Utterance_ID'].count()\n",
    "    freq_table = Counter(utterance_counts)\n",
    "    max_x = 15\n",
    "    freq_table_combined = {i: 0 for i in range(max_x + 1)}\n",
    "    outlier_count = 0\n",
    "\n",
    "    for key, value in freq_table.items():\n",
    "        if key <= max_x:\n",
    "            freq_table_combined[key] += value\n",
    "        else:\n",
    "            outlier_count += value\n",
    "\n",
    "    # Add outlier count\n",
    "    freq_table_combined[f'>{max_x}'] = outlier_count\n",
    "\n",
    "    # Step 3: Plot the frequency table\n",
    "    x = list(map(str, freq_table_combined.keys()))  # Convert keys to strings\n",
    "    y = list(freq_table_combined.values())\n",
    "\n",
    "    plt.bar(x, y)\n",
    "    plt.xlabel('Number of Utterances in a Dialogue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Frequency Distribution of Utterances per Dialogue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09300207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showFreqTurns(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "82a32ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5825e9de",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define features to drop\n",
    "drop_features = list(X_train.columns[6:]) \n",
    "\n",
    "# Create DataFrame for target labels\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "y_dev = pd.DataFrame()\n",
    "\n",
    "y_train[\"Emotion\"] = X_train[\"Emotion\"].copy()\n",
    "y_test[\"Emotion\"] = X_test[\"Emotion\"].copy()\n",
    "y_dev[\"Emotion\"] = X_dev[\"Emotion\"].copy()\n",
    "\n",
    "y_train[\"Dialogue_ID\"] = X_train[\"Dialogue_ID\"].copy()\n",
    "y_test[\"Dialogue_ID\"] = X_test[\"Dialogue_ID\"].copy()\n",
    "y_dev[\"Dialogue_ID\"] = X_dev[\"Dialogue_ID\"].copy()\n",
    "\n",
    "# Drop features from X_train DataFrame\n",
    "X_train = X_train.drop(drop_features, axis=1)\n",
    "X_test = X_test.drop(drop_features, axis=1)\n",
    "X_dev = X_dev.drop(drop_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d104ee",
   "metadata": {},
   "source": [
    "Before spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ad1731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[[\"Utterance\", \"Emotion\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "880afd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test[[\"Utterance\", \"Emotion\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b73b303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[\"Utterance\"] = X_train[\"Utterance\"].apply(lambda x: replace_spelling(x))\n",
    "# X_test[\"Utterance\"] = X_test[\"Utterance\"].apply(lambda x: replace_spelling(x))\n",
    "\n",
    "# X_train[\"Utterance\"] = preprocess_text(X_train[\"Utterance\"].tolist())\n",
    "# X_test[\"Utterance\"] = preprocess_text(X_test[\"Utterance\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8c7849ca",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def removeInstanceTurn1(X_set, Y_set, ranges):\n",
    "    mask = [True] * len(X_set)  # Initialize a mask with all True values\n",
    "    \n",
    "    for range_pair in ranges:\n",
    "        i = range_pair[0]\n",
    "        j = range_pair[1]\n",
    "        \n",
    "        if i == j:\n",
    "            mask[i] = False  # Mark the row for deletion\n",
    "            print(f\"Index to remove: {i}, Current X_set length: {len(X_set)}\")\n",
    "\n",
    "    # Convert mask to a numpy array for easy boolean indexing\n",
    "    mask = np.array(mask)\n",
    "    \n",
    "    # Use the mask to filter out rows to keep\n",
    "    X_set = X_set[mask]\n",
    "    Y_set = Y_set[mask]\n",
    "    \n",
    "    # Reset index after filtering rows\n",
    "    X_set.reset_index(drop=True, inplace=True)\n",
    "    Y_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_set, Y_set\n",
    "\n",
    "def removeInstanceTurnLessThan6(X_set, Y_set, ranges):\n",
    "    mask = [True] * len(X_set)  # Initialize a mask with all True values\n",
    "    \n",
    "    for range_pair in ranges:\n",
    "        i = range_pair[0]\n",
    "        j = range_pair[1]\n",
    "        \n",
    "        if (j - i + 1) < 6:  # Check if the dialogue has fewer than 6 turns\n",
    "            for k in range(i, j + 1):\n",
    "                mask[k] = False  # Mark the row for deletion\n",
    "\n",
    "    # Convert mask to a numpy array for easy boolean indexing\n",
    "    mask = np.array(mask)\n",
    "    \n",
    "    # Use the mask to filter out rows to keep\n",
    "    X_set = X_set[mask]\n",
    "    Y_set = Y_set[mask]\n",
    "    \n",
    "    # Reset index after filtering rows\n",
    "    X_set.reset_index(drop=True, inplace=True)\n",
    "    Y_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_set, Y_set\n",
    "\n",
    "def removeInstanceTurnMoreThan5(X_set, Y_set, ranges):\n",
    "    mask = [True] * len(X_set)  # Initialize a mask with all True values\n",
    "    \n",
    "    for range_pair in ranges:\n",
    "        i = range_pair[0]\n",
    "        j = range_pair[1]\n",
    "        \n",
    "        if (j - i + 1) > 5:  # Check if the dialogue has more than 5 turns\n",
    "            for k in range(i, j + 1):\n",
    "                mask[k] = False  # Mark the row for deletion\n",
    "\n",
    "    # Convert mask to a numpy array for easy boolean indexing\n",
    "    mask = np.array(mask)\n",
    "    \n",
    "    # Use the mask to filter out rows to keep\n",
    "    X_set = X_set[mask]\n",
    "    Y_set = Y_set[mask]\n",
    "    \n",
    "    # Reset index after filtering rows\n",
    "    X_set.reset_index(drop=True, inplace=True)\n",
    "    Y_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return X_set, Y_set\n",
    "\n",
    "if dataset_path == \"dataset_drop_noise\":\n",
    "    X_train, y_train = removeInstanceTurn1(X_train, y_train, rangesTrain)\n",
    "    X_test, y_test = removeInstanceTurn1(X_test, y_test, rangesTest)\n",
    "    X_dev, y_dev = removeInstanceTurn1(X_dev, y_dev, rangesDev)\n",
    "\n",
    "if dataset_path == \"dataset_drop_short_graphs\":\n",
    "    X_train, y_train = removeInstanceTurnLessThan6(X_train, y_train, rangesTrain)\n",
    "    X_test, y_test = removeInstanceTurnLessThan6(X_test, y_test, rangesTest)\n",
    "    X_dev, y_dev = removeInstanceTurnLessThan6(X_dev, y_dev, rangesDev)\n",
    "\n",
    "if dataset_path == \"dataset_drop_long_graphs\":\n",
    "    X_train, y_train = removeInstanceTurnMoreThan5(X_train, y_train, rangesTrain)\n",
    "    X_test, y_test = removeInstanceTurnMoreThan5(X_test, y_test, rangesTest)\n",
    "    X_dev, y_dev = removeInstanceTurnMoreThan5(X_dev, y_dev, rangesDev)\n",
    "\n",
    "# print(\"X_set_cleaned:\")\n",
    "# print(X_train_cleaned)\n",
    "# print(\"\\nY_set_cleaned:\")\n",
    "# print(Y_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8ef81f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAAsTAAALEwEAmpwYAAAm6klEQVR4nO3deZgcVb3/8feHJBASlhAzIlkgARFZBMGAqKBRXNhBL6A8cAVF48IiCpddxHvlJ/xkv17RsMgigojsKKtE9CpLErYAAhFCFpYMS9i3wPf+cc4UlaZ7pqdnenqS+byeZ56p9dS3q6vrW3Wq6pQiAjMzM4BlWh2AmZn1H04KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFq5uk+yRN6qWy9pB0fak/JL2/N8rO5b0kac3eKq/OZS4v6SpJz0v6fV8u23pG0i8l/bDOaadK+kazY2qVAZ8UJM2W9GreiXT8jW51XH1J0vi8U+74/E9JulrS58rTRcT6ETG1zrIGdzZdRFwQEZ/vhfCr/kgjYoWIeKQ3yu+GXYBVgfdExK6VIyWdI+knFcMWW195e/xsrfHWfaXf+IuSFkr6u6RvSyr2fxHx7Yj4r1bG2V8M+KSQ7ZB3Ih1/j5dHDqAf5IiIWAHYCLgBuEzS3r29kKV4fa4BPBQRi1odSIeleF2/i5Ja+7QdImJF0nd0HHAocFafBbckiYgB/QfMBj5bZXgA+wIPA4/mYdsDdwELgb8DG5am3xiYAbwI/A64CPhJHrc38Lcq5b8/dy8HnADMAZ4Cfgksn8dNAuYBBwELgCeAr5XKWR44EXgMeB74Wx52DbB/xTLvAb5Y5bOOz/EMrhh+cI5nmcp1BWwGTANeyNOclIfPyWW9lP8+lj///wInA88AP6lcJ3meA4BHgKeBn5WWewzwm2rxAscCbwGv5eX9vMr6XRk4D2jP6+moUtl753V2AvAc8CiwTSfby7rA1LwN3AfsmIf/GHgDeDPHsU+Vec/p2CZqfJbzgbeBV3MZh1Rbn3m+rwMP5JivA9boYts9FZibv6/pwJal6Y8BLs7r6MX8uSaWxo8DLs3r75mOddxZHIDy970gL/NeYIMa63Qq8FPg9jztFcDI0vjNSb+3hcDdwKSKeY8lbV+vdnznXf3GSdvv2x0xlb8bYBXg6vx5n8vdYyuW+Y3cvQxpe3osf9bzgJVL0341j3sG+CGL/4YW2x7Iv/VS/2jgDzmOR4ED+mSf2BcL6c9/1TaYPDxIR8sjSTvZjfOX/lFgELBXnnc5YNn8xX8fGEKqRniT+pPCycCVeVkrAlcBPy1tKIuA/8xlbwu8AqySx/9P3kjH5Lg+nmPaDbittLyN8oa5bJXPOp7qSWHNPHzdynUF/AP499y9ArB5rbLy518E7E/a+S1fuU7yPDfndbA68BDv/PCOoUZSyP1TO6atsX7PI+1oVszzPkTeaec43gS+mdffd4DHAVVZT0OAWcAR+Tv/DGknuk61OKvMfw6dJIVq22ON9blTjmPdvD6PAv5ea9vNw/YE3pOnPwh4Ehhaivs10rY1iLSDvjWPG0TaEZ8MDAeGAlt0FQfwBVLyGUFKEOsCq9VYL1OB+cAGeRl/6FiPpO36mRzbMsDncn9bad45wPo5hiHd+I3PAb5T+d3k9fRvwDDSNvN74PKKeDu2za/ndbAm6XdwKXB+HrceKZFvQdpeTiBta10mhfxZpwNH53nXJB0wfaHp+8RmL6C//+UN5iXSUcjCji8//7A+U5rudOC/KuZ9EPgU8EkqdiSkI5suk0L+wbwMrFUa9zHeOcKbRDoCKu8UFpCOnpbJ4zaq8rmGko5y1s79JwC/qLEOxlM9KQzNwz9RWlcdG/QtpKPjUV2VlT//nIrpFlsneZ6tS/3fBW7K3cfQYFIg7dTeANYrjfsWMLUUx6zSuGF53vdVWU9bknamy5SGXQgcUy3OKvOfQ+8khT9ROhPJ28ErvHOUvti2WyOW5zq2mxz3jaVx6wGvlrbF9spto6s4SAnzIfJ22kUsU4HjKpb/Rv7uDiXvZEvjrwP2Ks37n3X8xqslhVuBI2t9N6XpPgw8VxFvR1K4Cfhuadw6pB3/YNIO/cKKbesN6ksKH+Xdv5nDgV939ll748/XFJKdI2JE/tu5NHxuqXsN4KB8oWqhpIWk0+rR+W9+5G8ue6zOZbeRNpbppXKvzcM7PBOL11O/QjoqGUXacf+rstCIeI1UjbVnrmfdnVQ90R1j8v9nq4zbB/gA8E9Jd0javouy5nYxvnKax0jrtadGkY7wy9/HY7zz2SDt6AGIiFdy5wpVyhoNzI2ItzspqzOLcixlQ0jVGG+/e/Ka1gBOLW0vz5IOLspxLLa+JR0s6YF8Z9RCUpXaqNIkT5a6XwGG5usR44DHovp1kppxRMSfgZ+TzmQXSJoiaaVOPlPldz8kx7cGsGvF724LYLVan7UbxlBl25Y0TNKvJD0m6QXSAdAISYOqlDGad29bg0k3HIwux5a3rWfqjG0NYHTF5z4il9tUTgqdK+/k5wLHlpLHiIgYFhEXkur5x0hSafrVS90vk3b8AEh6X2nc06Sj/fVL5a4c6YJvV54mnfavVWP8ucAewFbAKxHxjzrKLPsi6azkwcoREfFwROwOvBc4HrhE0nAWX2eLzVLH8saVulcnnX1BxfoDyuuvq7KfJh25rVFR9vw64qn0ODCu4mJmd8qaQzryL5vA4omm8rNU+2xzgW9VbIvLR8Tfq80naUvS9YndSNWOI0jXn8rbay1zgdVrXLDuNI6IOC0iPkI68v8A8B+dLKfyu3+T9N3NJZ0plJcxPCKOq/ZZ6yVpU1JS+FuV0QeRjvg/GhErkWoCoPr6epx3b1uLSNfZngDGlpa5PKlqqkNn2/VcUm1B+XOvGBHb1vP5esJJoX5nAN+W9NF8l8NwSdtJWpFUv74IOEDSEElfIl3I6nA3sL6kD0saSjpdByDvDM4ATpb0XgBJYyR9oauA8rxnAydJGi1pkKSPSVouj/8H6Qj0RLpxliBpVUn7AT8CDq84Mu6YZk9JbXncwjz4bVJVw9ukOtDu+g9Jq0gaB3yPdKYD6eL+JyWtLmll0ml02VO1lhcRb5Euoh4raUVJawA/AH7TQHy3kY6iD8nf8yRgB9JNBfX4A7CdpM/n72o0qR6+PH/lZ6m2Pn8JHC5pfQBJK0t61y2wJSuSts92YLCko4HOjtrLbift3I7L2/xQSZ/oKg5Jm+bfyhDSzu81Oj8b2lPSepKGka6fXZK/u98AO0j6Ql5nQyVNkjS2k7JqkrRSPqu9iFTVd2+VyVYkHagtlDSS9Duo5ULg+5ImSFoB+H/A7/KZ1SU59o9LWpb0uy8nlruAbSWNzAeKB5bG3Q68KOlQpedfBknaICezpnJSqFNETCNdjPw5qT52Fqk+moh4A/hS7n8W+DLpglPHvA+RNvQbSXeEVB6dHJrLuzWfrt5IOlKpx8GkOzvuyMs+nsW/1/OAD1HfTnChpJdzedsCu0bE2TWm3Rq4T9JLpDtbvhIRr+ZT5GOB/82nvZvX+TkgXQyeTvqxXEO+ZTAibiAliHvy+Ksr5jsV2EXSc5JOq1Lu/qQd0yOkdf9bUjLtlvw97wBsQzqK/QXw1Yj4Z53z30eqxvsp6bv6BynR/Lg02U+Bo/K6O7ja+oyIy0jf80V5e5mZY6rlOlKV5EOk6o3XqLPKJe+YdyBdn5lDuhPuy3lcZ3GsRDrYeY537r75WSeLOp9Ux/4kqUr0gLyMuaQL2keQktpc0hlHd/ddV0l6Mc9/JHAS8LUa055CuhniadJ1h2s7KffsHPstpDuEXiNtbx3f9/6kBPQE6drlAuD1PO/5pAPG2cD1vHMQ1LHetyddz3g0x3ImqdqvqbR4Nbj1FknnkC4aHdXiOL4KTI6ILVoZh1ktkqaSjtrPbHUszZTPJBaSbv54tMXh1OQzhaVYPhX/LjCl1bGYDUSSdsgXroeT7gC8l3Rm0G85KSyl8jWJdlId9W9bHI7ZQLUT6WL048DapGrWfl094+ojMzMr+EzBzMwKS3RjWaNGjYrx48e3OgwzsyXK9OnTn46ItmrjluikMH78eKZNm9bqMMzMliiSara44OojMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKyzRTzSbNWL8Ydc0PO/s47brxUjM+h+fKZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOBbUs0a5FtbbWnkMwUzMyv4TMGWGD4yN2s+nymYmVnBScHMzApNSwqSzpa0QNLMKuMOkhSSRuV+STpN0ixJ90japFlxmZlZbc08UzgH2LpyoKRxwOeBOaXB2wBr57/JwOlNjMvMzGpoWlKIiFuAZ6uMOhk4BIjSsJ2A8yK5FRghabVmxWZmZtX16TUFSTsB8yPi7opRY4C5pf55eVi1MiZLmiZpWnt7e5MiNTMbmPosKUgaBhwBHN2TciJiSkRMjIiJbW1tvROcmZkBffucwlrABOBuSQBjgRmSNgPmA+NK047Nw8zMrA/12ZlCRNwbEe+NiPERMZ5URbRJRDwJXAl8Nd+FtDnwfEQ80VexmZlZ0sxbUi8E/gGsI2mepH06mfyPwCPALOAM4LvNisvMzGprWvVRROzexfjxpe4A9m1WLGZmVh8/0WxmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrNPMdzWdLWiBpZmnYzyT9U9I9ki6TNKI07nBJsyQ9KOkLzYrLzMxqa+aZwjnA1hXDbgA2iIgNgYeAwwEkrQd8BVg/z/MLSYOaGJuZmVXRtKQQEbcAz1YMuz4iFuXeW4GxuXsn4KKIeD0iHgVmAZs1KzYzM6uuldcUvg78KXePAeaWxs3Lw95F0mRJ0yRNa29vb3KIZmYDS0uSgqQjgUXABd2dNyKmRMTEiJjY1tbW+8GZmQ1gg/t6gZL2BrYHtoqIyIPnA+NKk43Nw8zMrA/16ZmCpK2BQ4AdI+KV0qgrga9IWk7SBGBt4Pa+jM3MzJp4piDpQmASMErSPOBHpLuNlgNukARwa0R8OyLuk3QxcD+pWmnfiHirWbGZmVl1TUsKEbF7lcFndTL9scCxzYrHzMy65ieazcys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZoVmvqP5bGB7YEFEbJCHjQR+B4wHZgO7RcRzSi9sPhXYFngF2DsiZjQrNrP+ZPxh1zQ87+zjtuvFSMyae6ZwDrB1xbDDgJsiYm3gptwPsA2wdv6bDJzexLjMzKyGpiWFiLgFeLZi8E7Aubn7XGDn0vDzIrkVGCFptWbFZmZm1fX1NYVVI+KJ3P0ksGruHgPMLU03Lw8zM7M+1LILzRERQHR3PkmTJU2TNK29vb0JkZmZDVx9nRSe6qgWyv8X5OHzgXGl6cbmYe8SEVMiYmJETGxra2tqsGZmA01fJ4Urgb1y917AFaXhX1WyOfB8qZrJzMz6SDNvSb0QmASMkjQP+BFwHHCxpH2Ax4Dd8uR/JN2OOot0S+rXmhWXmZnV1rSkEBG71xi1VZVpA9i3WbGYmVl96qo+kvShZgdiZmatV+81hV9Iul3SdyWt3NSIzMysZepKChGxJbAH6Q6h6ZJ+K+lzTY3MzMz6XN13H0XEw8BRwKHAp4DTJP1T0peaFZyZmfWteq8pbCjpZOAB4DPADhGxbu4+uYnxmZlZH6r37qP/Bs4EjoiIVzsGRsTjko5qSmRmZtbn6k0K2wGvRsRbAJKWAYZGxCsRcX7TojMzsz5V7zWFG4HlS/3D8jAzM1uK1JsUhkbESx09uXtYc0IyM7NWqTcpvCxpk44eSR8BXu1kejMzWwLVe03hQOD3kh4HBLwP+HKzgjIzs9aoKylExB2SPgiskwc9GBFvNi8sM7OlX398P3d3GsTbFBif59lEEhFxXlOiMjOzlqgrKUg6H1gLuAt4Kw8OwEnBzGwpUu+ZwkRgvdzEtZmZLaXqvftoJunispmZLcXqPVMYBdwv6Xbg9Y6BEbFjU6IyM7OWqDcpHNPMIMzMrH+o95bUv0haA1g7Im6UNAwY1NzQzMysr9XbdPY3gUuAX+VBY4DLG12opO9Luk/STEkXShoqaYKk2yTNkvQ7Scs2Wr6ZmTWm3gvN+wKfAF6A4oU7721kgZLGAAcAEyNiA9IZx1eA44GTI+L9wHPAPo2Ub2Zmjas3KbweEW909EgaTHpOoVGDgeVzOcOAJ0gv7Lkkjz8X2LkH5ZuZWQPqTQp/kXQEaUf+OeD3wFWNLDAi5gMnAHNIyeB5YDqwMCIW5cnmkaqo3kXSZEnTJE1rb29vJAQzM6uh3qRwGNAO3At8C/gj6X3N3SZpFWAnYAIwGhgObF3v/BExJSImRsTEtra2RkIwM7Ma6r376G3gjPzXU58FHo2IdgBJl5KuV4yQNDifLYwF5vfCsszMrBvqbfvoUapcQ4iINRtY5hxg83xb66vAVsA04GZgF+AiYC/gigbKNjOzHuhO20cdhgK7AiMbWWBE3CbpEmAGsAi4E5gCXANcJOknedhZjZRvZmaNq7f66JmKQadImg4c3chCI+JHwI8qBj8CbNZIeWZm1jvqrT7apNS7DOnMoTvvYjAzsyVAvTv2E0vdi4DZwG69Ho2ZmbVUvdVHn252IGZm1nr1Vh/9oLPxEXFS74RjZmat1J27jzYFrsz9OwC3Aw83IygzM2uNepPCWGCTiHgRQNIxwDURsWezAjMzs75XbzMXqwJvlPrfyMPMzGwpUu+ZwnnA7ZIuy/07k1oyNTOzpUi9dx8dK+lPwJZ50Nci4s7mhWVmZq1Qb/URpPcevBARpwLzJE1oUkxmZtYi9b6O80fAocDhedAQ4DfNCsrMzFqj3jOFLwI7Ai8DRMTjwIrNCsrMzFqj3gvNb0RESAoAScObGJOZNWj8Ydc0NN/s47br5UhsSVXvmcLFkn5FehHON4Eb6Z0X7piZWT/S5ZmCJAG/Az4IvACsAxwdETc0OTYzM+tjXSaFXG30x4j4EOBEYDYAuBpq4Kq3+miGpE2bGomZmbVcvReaPwrsKWk26Q4kkU4iNmxWYGZm1vc6TQqSVo+IOcAXenOhkkYAZwIbAAF8HXiQdO1iPPklPhHxXG8u18zMOtdV9dHlABHxGHBSRDxW/uvBck8Fro2IDwIbAQ8AhwE3RcTawE2538zM+lBXSUGl7jV7Y4GSVgY+CZwFEBFvRMRCYCfeaWTvXFKje2Zm1oe6SgpRo7snJgDtwK8l3SnpzPww3KoR8USe5kncNLeZWZ/rKilsJOkFSS8CG+buFyS9KOmFBpc5GNgEOD0iNiZduF6sqigighpJSNJkSdMkTWtvb28wBDMzq6bTpBARgyJipYhYMSIG5+6O/pUaXOY8YF5E3Jb7LyEliackrQaQ/y+oEdOUiJgYERPb2toaDMHMzKrpTtPZvSIingTmSlonD9oKuJ/0/ue98rC9gCv6OjYzs4Gu3ucUetv+wAWSlgUeAb5GSlAXS9oHeAzYrUWxmZkNWC1JChFxFzCxyqit+jgUMzMr6fPqIzMz67+cFMzMrOCkYGZmhVZdaDazAcBNcC95fKZgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVWpYUJA2SdKekq3P/BEm3SZol6XeSlm1VbGZmA1UrzxS+BzxQ6j8eODki3g88B+zTkqjMzAawliQFSWOB7YAzc7+AzwCX5EnOBXZuRWxmZgNZq84UTgEOAd7O/e8BFkbEotw/DxhTbUZJkyVNkzStvb296YGamQ0kfZ4UJG0PLIiI6Y3MHxFTImJiRExsa2vr5ejMzAa2wS1Y5ieAHSVtCwwFVgJOBUZIGpzPFsYC81sQm5nZgNbnSSEiDgcOB5A0CTg4IvaQ9HtgF+AiYC/gir6OzcyWbuMPu6bheWcft10vRtJ/9afnFA4FfiBpFukaw1ktjsfMbMBpRfVRISKmAlNz9yPAZq2Mx8z6p0aP8AfK0X1v6k9nCmZm1mJOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys0NJWUs3MlkRL83sZfKZgZmYFJwUzMys4KZiZWcFJwczMCn2eFCSNk3SzpPsl3Sfpe3n4SEk3SHo4/1+lr2MzMxvoWnGmsAg4KCLWAzYH9pW0HnAYcFNErA3clPvNzKwP9XlSiIgnImJG7n4ReAAYA+wEnJsnOxfYua9jMzMb6Fp6TUHSeGBj4DZg1Yh4Io96Eli1xjyTJU2TNK29vb1vAjUzGyBalhQkrQD8ATgwIl4oj4uIAKLafBExJSImRsTEtra2PojUzGzgaElSkDSElBAuiIhL8+CnJK2Wx68GLGhFbGZmA1kr7j4ScBbwQEScVBp1JbBX7t4LuKKvYzMzG+ha0fbRJ4B/B+6VdFcedgRwHHCxpH2Ax4DdWhCbmdmA1udJISL+BqjG6K36MhYzM1ucn2g2M7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys0O+SgqStJT0oaZakw1odj5nZQNKvkoKkQcD/ANsA6wG7S1qvtVGZmQ0c/SopAJsBsyLikYh4A7gI2KnFMZmZDRiKiFbHUJC0C7B1RHwj9/878NGI2K80zWRgcu5dB3iwSeGMAp52OS6nH5TlcgZmOb1dVtkaEdFWbcTgJiysqSJiCjCl2cuRNC0iJrocl9PqslzOwCynt8uqV3+rPpoPjCv1j83DzMysD/S3pHAHsLakCZKWBb4CXNnimMzMBox+VX0UEYsk7QdcBwwCzo6I+1oUTm9VUbmcgVlOb5blcgZmOb1dVl361YVmMzNrrf5WfWRmZi3kpGBmZgUnhQq91cyGpLMlLZA0swdljJN0s6T7Jd0n6Xs9KGuopNsl3Z3L+nEPyhok6U5JVzdaRi5ntqR7Jd0laVoPyhkh6RJJ/5T0gKSPNVDGOjmOjr8XJB3YYDzfz+t4pqQLJQ1tsJzv5TLu624s1bY/SSMl3SDp4fx/lQbL2TXH9Lakum6XrFHOz/J3do+kyySNaLCc/8pl3CXpekmjGymnNO4gSSFpVIPxHCNpfmlb2rarcqqUu1/eBy0Wh6RJkp4vlX10d8vuUkT4L/+RLm7/C1gTWBa4G1ivwbI+CWwCzOxBPKsBm+TuFYGHehCPgBVy9xDgNmDzBsv6AfBb4Ooeru/ZwKhe+N7OBb6Ru5cFRvTCdvAk6QGf7s47BngUWD73Xwzs3UA5GwAzgWGkG0JuBN7fk+0P+P/AYbn7MOD4BstZl/Tg6FRgYg/i+TwwOHcf34N4Vip1HwD8spFy8vBxpBtdHqtn26wRzzHAwV3Mt0oX4zcGxlf+RoBJPf3ddfXnM4XF9VozGxFxC/BsT4KJiCciYkbufhF4gLTTaaSsiIiXcu+Q/NftuwwkjQW2A85sJI7eJmll0g/zLICIeCMiFvaw2K2Af0XEYw3OPxhYXtJg0k798QbKWBe4LSJeiYhFwF+AL9U7c43tbydSAiX/37mRciLigYjoVksCNcq5Pn82gFtJzyU1Us4Lpd7h1LFdd/L7PBk4pJ4yuiinK9MkXSDpM5JUpdw7I2J2A+X2mJPC4sYAc0v982hwJ9zbJI0nHT3c1oMyBkm6C1gA3BARjZR1CulH83ajcZQEcL2k6UrNlzRiAtAO/DpXaZ0paXgP4/oKcGEjM0bEfOAEYA7wBPB8RFzfQFEzgS0lvUfSMGBbFn+wsxGrRsQTuftJYNUeltebvg78qdGZJR0raS6wB9BQlYqknYD5EXF3o3GU7JertM6uUU33AdI2th9wv6Qj6qn2yj6Wq4H/JGn9Xoh1MU4KSwBJKwB/AA6sOCrqloh4KyI+TDoi20zSBt2MY3tgQURMbzSGCltExCakVnH3lfTJBsoYTDp9Pz0iNgZeJlWNNETpockdgd83OP8qpCPyCcBoYLikPbtbTkQ8QKpSuR64FrgLeKuRmGqUHzRwptgMko4EFgEXNFpGRBwZEeNyGft1NX2VGIYBR9BgQqlwOrAW8GHSgcGJlRPk3+LVEfEl0pnumsAcSZt1UfYMUrXmRsB/A5f3QryLcVJYXL9rZkPSEFJCuCAiLu2NMnP1ys3A1t2c9RPAjpJmk6rWPiPpNz2IY37+vwC4jFR9113zgHmls55LSEmiUdsAMyLiqQbn/yzwaES0R8SbwKXAxxspKCLOioiPRMQngedI15R64ilJqwHk/wt6WF6PSdob2B7YIyeqnroA+LcG5luLlMjvztv3WGCGpPd1t6CIeCrv9N8GzqDGdi1pZUnfIrXasDbpbOmeLsp+oaMaOCL+CAyp54J4dzgpLK5fNbOR6xrPAh6IiJN6WFZbx90dkpYHPgf8sztlRMThETE2IsaT1s2fI6LbR8E5huGSVuzoJl107PadWhHxJDBX0jp50FbA/Y3ElO1Og1VH2Rxgc0nD8ve3FelaULdJem/+vzrpesJvexAXpG15r9y9F3BFD8vrEUlbk6oid4yIV3pQztql3p3o5nYNEBH3RsR7I2J83r7nkW7yeLKBeFYr9X6RKtt1PpiaQUpEX42IT0XEeRHxWhdlv6/jGkQ+q1gGeKa7MXaqmVexl8Q/Ut3tQ6S7kI7sQTkXkk4d3yRtYPs0UMYWpFP8e0jVB3cB2zYYz4bAnbmsmcDRPVxPk+jBXRCk0+W78999PVzXHwam5c92OV3c2dFJOcPzD2zlHq6bH5N2TDOB84HlGiznr6QEdzewVU+3P+A9wE3Aw6S7mUY2WM4Xc/frwFPAdQ2WM4t0Da9j267nrqFq5fwhr+t7gKuAMY2UUzF+NvXdfVQtnvOBe3M8VwKrVZlvR/KdVzXKPSCXt4h0o8KZefh++fdyN+ni/Md7sq1W+3MzF2ZmVnD1kZmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJYYDKrS+eWOo/WNIxvVT2OZJ26Y2yuljOrkqtot5cMXySKlpwLcck6cD8BGvHuCOaHWszSJoo6bQmlHuOpEdzUwoPSTovt3nVMf6P6qJFU6UWcHv1oSrrG04KA9frwJf62w83NyJXr32Ab0bEp7u5mANJDdV16HZSkDSou/P0toiYFhEHNKn4/4jUlMI6pOdb/pwf6CQito2eNzpo/ZSTwsC1iPT+1+9Xjqg80pf0Uv4/SdJfJF0h6RFJx0naQ+k9DfdKWqtUzGclTctHmtvn+QcptaF/R24s7Fulcv8q6UqqPI0safdc/kxJx+dhR5Me7jtL0s/q/dCSDiC1SXSz0rsqjiO1aHqXpAvyNHvmz3SXpF91JABJL0k6UdLdpEbJjs6fZaakKaUnTadKOj6X8ZCkLUuf/4Q8/T2S9s/DP5LX63RJ1+mdpigOUHqXxj2SLqryWYozIqU2/M/Oy34kf85qn//0/L3U9U6NSE4mNaC3TS6jOAuQdHmO+z7VaNRQ0g/yZ56p0nshJP1Q6d0lf1N678TBpfU3MXePUmp2oub2Y72st5+G89+S8Qe8BKxEenJzZeBg4Jg87hxgl/K0+f8kYCHpPQ/LkdqF+nEe9z3glNL815IOOtYmPZk5FJgMHJWnWY70FPKEXO7LwIQqcY4mNR3RRmr87s/AznncVKq050+Vp63Ln4l3t1H/Uql7XdJTsUNy/y9IzRBAerp8t9K0I0vd5wM7lOI6MXdvC9yYu79Dapup4x0CI0lNmP8daMvDvgycnbsfJz8NTZV3RJQ/J6kN/7/n9TqK9GT2kCrzjMz/B+U4N6wyzWLffx52CnBo5forlbc86ani95SnAT5Cerp3OLAC6WncjYFNSU8xDyW9K+Rh8jsIyt9rLmN27q66/bT6t7S0/XXnVN2WMhHxgqTzSI/Uv1rnbHdEbn5Z0r9IrXhC+uGXq3EujtQg2MOSHgE+SGrfaMPSWcjKpKTxBnB7RDxaZXmbAlMjoj0v8wJSq5KXd/bRujm8bCvSjuyOfOC/PO80HPcWqUmFDp+WdAipKmokaYd3VR7X0XjhdNLLUiA1lvfLyO8QiIhnlVqq3QC4IS9vEKnZBEjNJFwg6XLqaw3zmoh4HXhd0gJS09jzKqbZLR/RDyYl9/XoohG27F1t/mcHSPpi7h5H+j7LbfFsAVwWES8DSLoU2JJ0wHBFpLZ+XpN0FV2rtf1U226sQU4KdgqpYa5fl4YtIlctSlqG9DazDq+Xut8u9b/N4ttT5Q44SDuW/SPiuvIISZNIZwq95Rmgsg37kcDTdcwr4NyIOLzKuNci4i1IrzclnUVMjIi5Shfpy6/d7Fgvb9H570zAfRFR7RWi25ES4A7AkZI+FO+8lKaa8nfzruVKmkA6I9w0Ip6TdE5FzJ3ZmNRuUrm8SaRE97GIeEXS1G6U15li+6sor+r2Y73L1xQGuIh4lvTKyH1Kg2eTjpYhNdw1pIGid5W0TL7OsCbwIOk1h99Rag4cSR9Q1y/EuR34VK5bHkRqxfQvXczzMDBa0rp5OWsAG5GqKwBeJFVZdHizIybSjm8XvdNC6cg8f6WOndXTSu+7qOduqxuAbylfTJc0krRe2pTfKy1piKT1czIeFxE3A4eSjopXqGMZnVmJlHyfl7Qq+RpBZ5QcQDqruLZi9MrAczkhfBDYvEoRfwV2Vmo1djipMb2/Av8L7KD07vAVSM1nd5jNO9tfeb02sv1YN/lMwSC9BKT8YpIzgCvyBdVraewofg5ph74S8O2IeE3SmaSqlBn5omw7XbwSMiKekHQY6f0PIlWRdNrkc0S8rvRim1/nI/o3Se9wfj5PMgW4VtLjke5cmgLcI2lGROwh6SjSG+GWyfPuS3pnb3kZCyWdQapHf5LU7HpXziS9ceseSW8CZ0TEz3N1yGlKrxYdTDp7ewj4TR4m4LTo4R0/EXG3pDtJLbjOJe2Ya/mZpB+SqsZuBT4d6RW1ZdcC35b0ACm53VplmTPyGcntedCZEXEngNKNBfeQWlq9F+j4fk4ALs7VXNeUiuv29mPd51ZSzawlJK0QES8pPTNyCzA58jvJrXV8pmBmrTJF0nqkqrhznRD6B58pmJlZwReazcys4KRgZmYFJwUzMys4KZiZWcFJwczMCv8HI5vkXDwEtgQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "showFreqTurns(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "86036b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_encoder.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_decoder.pkl\")\n",
    "\n",
    "directory_path = \"data/dump/\" + dataset_path\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "if not(checkFile1 and checkFile2):\n",
    "    labels = sorted(set(y_train.Emotion))\n",
    "    labelEncoder = {label: i for i, label in enumerate(labels)}\n",
    "    labelDecoder = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    pickle.dump(labelEncoder, open('data/dump/' + dataset_path + '/label_encoder.pkl', 'wb'))\n",
    "    pickle.dump(labelDecoder, open('data/dump/' + dataset_path + '/label_decoder.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('data/dump/' + dataset_path + '/label_encoder.pkl', 'rb')\n",
    "    file2 = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "    labelEncoder = pickle.load(file1)\n",
    "    labelDecoder = pickle.load(file2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6211e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the \"Emotion\" column in y_train\n",
    "if not isinstance(y_train[\"Emotion\"][0], np.int64):   \n",
    "    y_train[\"Emotion\"] = y_train[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "    y_test[\"Emotion\"] = y_test[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "    y_dev[\"Emotion\"] = y_dev[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "\n",
    "# Copy the encoded \"Emotion\" column from y_train to X_train\n",
    "X_train[\"Emotion\"] = y_train[\"Emotion\"].copy()\n",
    "X_test[\"Emotion\"] = y_test[\"Emotion\"].copy()\n",
    "X_dev[\"Emotion\"] = y_dev[\"Emotion\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0db6676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_train.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_test.pkl\")\n",
    "checkFile3 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_dev.pkl\")\n",
    "\n",
    "if not (checkFile1 and checkFile2 and checkFile3):\n",
    "    pickle.dump(X_train[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_train.pkl', 'wb'))\n",
    "    pickle.dump(X_test[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(X_dev[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_dev.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc91e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a1a4583",
   "metadata": {},
   "source": [
    "Creating an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19929c11",
   "metadata": {},
   "source": [
    "Testing on smaller data. Uncomment to see the size of updated representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "da518513",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of contextual embeddings: torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "model = transformers.BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Define your dialog data\n",
    "dialogs = [\n",
    "    \"How are you today?\",\n",
    "    \"I'm doing well, thank you!\",\n",
    "    \"That's good to hear.\",\n",
    "    \"Yes, it is.\",\n",
    "    \"Do you have any plans for the weekend?\",\n",
    "    \"Not really, just relaxing at home.\",\n",
    "    \"Sounds nice.\",\n",
    "    \"Indeed.\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the dialogs\n",
    "encoded_dialogs = [tokenizer.encode(dialog, add_special_tokens=True) for dialog in dialogs]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_length = max(len(dialog) for dialog in encoded_dialogs)\n",
    "padded_dialogs = [dialog + [tokenizer.pad_token_id] * (max_length - len(dialog)) for dialog in encoded_dialogs]\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = [[1] * len(dialog) + [0] * (max_length - len(dialog)) for dialog in encoded_dialogs]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_ids = torch.tensor(padded_dialogs)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# Obtain the BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "# Extract the contextual embeddings (CLS token)\n",
    "contextual_embeddings = outputs[0][:, 0, :]  # Extract embeddings for the [CLS] token\n",
    "\n",
    "# Print the shape of the contextual embeddings\n",
    "print(\"Shape of contextual embeddings:\", contextual_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "79e3fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1578baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d7e17",
   "metadata": {},
   "source": [
    "This is just a duplicate of code above. Using this on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3718155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918\n",
      "243\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a89c08ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     also I was the point person on my company’s tr...\n",
       "1                      You must’ve had your hands full.\n",
       "2                               That I did. That I did.\n",
       "3         So let’s talk a little bit about your duties.\n",
       "4                                My duties?  All right.\n",
       "5     Now you’ll be heading a whole division, so you...\n",
       "6                                                I see.\n",
       "7     But there’ll be perhaps 30 people under you so...\n",
       "8                                         Good to know.\n",
       "9                                 We can go into detail\n",
       "10                               No don’t I beg of you!\n",
       "11    All right then, we’ll have a definite answer f...\n",
       "12                                             Really?!\n",
       "Name: Utterance, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"Utterance\"][rangesTrain[0][0]:rangesTrain[0][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77955986",
   "metadata": {},
   "source": [
    "Testing on small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "63595e55",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # List of text dialogs\n",
    "# dialogs = [\n",
    "#     [\"How are you today?\", \"I'm doing well, thank you!\"],\n",
    "#     [\"That's good to hear.\", \"Yes, it is.\", \"Do you have any plans for the weekend?\", \"Not really, just relaxing at home.\"],\n",
    "#     [\"Sounds nice.\", \"Indeed.\"]\n",
    "# ]\n",
    "\n",
    "# # List to store contextual embeddings for each utterance\n",
    "# contextual_embeddings = []\n",
    "\n",
    "# # Iterate through each dialog\n",
    "# for dialog in dialogs:\n",
    "#     # Tokenize and convert dialog to input IDs\n",
    "#     inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "#     # Get BERT model outputs\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#     # Extract contextual embeddings (CLS token represents the entire sequence)\n",
    "#     embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "\n",
    "#     # Store embeddings for each utterance in the dialog\n",
    "#     contextual_embeddings.append(embeddings)\n",
    "\n",
    "# # Print the list of contextual embeddings\n",
    "# print(\"List of Contextual Embeddings:\")\n",
    "# # for embedding in contextual_embeddings:\n",
    "# #     print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87280a72",
   "metadata": {},
   "source": [
    "#### Contexualized train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7abe85c5",
   "metadata": {
    "code_folding": [
     32
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Ranges: 918it [00:00, 61955.86it/s]\n",
      "Processing Dialogs: 100%|████████████████████████████████████████████████████████████| 918/918 [05:34<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized train data - Elapsed time: 336.98226499557495 seconds\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_train.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    directory_path = \"embed/\" + dataset_path\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    dialogs = []\n",
    "    for range_pair, iteration in tqdm(zip(rangesTrain, range(len(rangesTrain))), desc=\"Processing Ranges\"):\n",
    "        start_idx, end_idx = range_pair\n",
    "        dialog = list(X_train['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    # List to store contextual embeddings for each utterance\n",
    "    contextualEmbeddingsTrain = []\n",
    "\n",
    "    # Iterate through each dialog\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        # Tokenize and convert dialog to input IDs\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # Get BERT model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract contextual embeddings (CLS token represents the entire sequence)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "\n",
    "        # Store embeddings for each utterance in the dialog\n",
    "        contextualEmbeddingsTrain.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "            pickle.dump(contextualEmbeddingsTrain, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_train.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsTrain = pickle.load(file)\n",
    "        \n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized train data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1c3a4",
   "metadata": {},
   "source": [
    "<h4> Contexualize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8604a65f",
   "metadata": {
    "code_folding": [
     3,
     23
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 243/243 [00:00<?, ?it/s]\n",
      "Processing Dialogs: 100%|████████████████████████████████████████████████████████████| 243/243 [01:31<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized test data - Elapsed time: 93.1108500957489 seconds\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_test.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair in tqdm(rangesTest):\n",
    "        start_idx, end_idx = range_pair            \n",
    "        dialog = list(X_test['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    contextualEmbeddingsTest = []\n",
    "\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "        contextualEmbeddingsTest.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(contextualEmbeddingsTest, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_test.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsTest = pickle.load(file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized test data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432631b1",
   "metadata": {},
   "source": [
    "<h4> Contexualize val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4bf80889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rangesDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "75f65f3f",
   "metadata": {
    "code_folding": [
     3,
     23
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 44535.08it/s]\n",
      "Processing Dialogs: 100%|██████████████████████████████████████████████████████████████| 89/89 [00:33<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexualized test data - Elapsed time: 33.80242943763733 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"embed/\" + dataset_path + \"/u_prime_BERT_dev.pkl\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not checkFile:\n",
    "    dialogs = []\n",
    "    for range_pair in tqdm(rangesDev):\n",
    "        start_idx, end_idx = range_pair            \n",
    "        dialog = list(X_dev['Utterance'][start_idx:end_idx + 1])\n",
    "        dialogs.append(dialog)\n",
    "\n",
    "    contextualEmbeddingsDev = []\n",
    "\n",
    "    for dialog in tqdm(dialogs, desc=\"Processing Dialogs\"):\n",
    "        inputs = tokenizer(dialog, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].tolist()\n",
    "        contextualEmbeddingsDev.append(torch.tensor(embeddings))\n",
    "\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(contextualEmbeddingsDev, file)\n",
    "\n",
    "else:\n",
    "    file_path = f'embed/' + dataset_path + '/u_prime_BERT_dev.pkl'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contextualEmbeddingsDev = pickle.load(file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Contexualized test data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b17e6",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "65ff6791",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_7708\\1242819068.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTrain:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_train['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTrain.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTrain, rangesTrain], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\")\n",
    "    encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30584c74",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bd271d5c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_7708\\1561329555.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTest:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_test['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTest.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTest, rangesTest], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_test.pkl', \"rb\")\n",
    "    encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b03ba",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "76fc8104",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_7708\\2618188345.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n"
     ]
    }
   ],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\")\n",
    "encodedSpeakersDev = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesDev:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_dev['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersDev.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersDev, rangesDev], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_dev.pkl', \"rb\")\n",
    "    encodedSpeakersDev, rangesDev = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b1064",
   "metadata": {},
   "source": [
    "<h4>Getting data required for graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7851436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = int(contextualEmbeddingsTrain[0].shape[1]/2)\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "n_speakers=2\n",
    "max_seq_len=110\n",
    "window_past=0\n",
    "window_future=5\n",
    "# vocab_size=vocab_size\n",
    "n_classes=7\n",
    "listener_state=False\n",
    "context_attention='general'\n",
    "dropout=0.5\n",
    "nodal_attention=False\n",
    "no_cuda=True\n",
    "n_relations = 2 * n_speakers ** 2\n",
    "att_model = MaskedEdgeAttention(2 * D_e, max_seq_len, no_cuda)\n",
    "nodal_attention=True\n",
    "edge_type_mapping = {}\n",
    "for j in range(n_speakers):\n",
    "    for k in range(n_speakers):\n",
    "        edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "        edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d455d",
   "metadata": {},
   "source": [
    "Sample Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "83e33c37",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x_data = torch.tensor([\n",
    "#                         [[1, 2, 3, 0, 0],     # Utterance 1\n",
    "#                         [4, 5, 0, 0, 0]],    # Utterance 2\n",
    "#                        [[6, 7, 8, 9, 0],     # Utterance 3\n",
    "#                         [10, 0, 0, 0, 0]]])  # Utterance 4\n",
    "\n",
    "# umask_data = torch.tensor([[1, 1],        # Dialogue 1 has 2 utterances\n",
    "#                            [1, 0]])       # Dialogue 2 has 1 utterance\n",
    "# # features = cnn_feat_extractor(x_data, umask_data)\n",
    "# # emotions, hidden = lstm(features)\n",
    "\n",
    "# textf = x_data.squeeze(0)  # Remove batch dimension (1, utterance_size, embedding_size) -> (utterance_size, embedding_size)\n",
    "# umask = torch.FloatTensor([[1] * textf.size(0)])  # Adjust to (1, utterance_size)\n",
    "# lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b67ddfec",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print(textf)\n",
    "# print(umask)\n",
    "# print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cae20761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodedSpeakersTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044a56a",
   "metadata": {},
   "source": [
    "June 5 possible mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "43df81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ranges in rangesDev:\n",
    "#     if ranges[1]-ranges[0] == 0:\n",
    "#         print(\"delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "37d0b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = 17\n",
    "# umask = torch.FloatTensor([[1] * 10 + [0] * (17 - 10)])\n",
    "# assert umask.size(1) == max_seq_len, f\"umask size is {umask.size(1)}, expected {max_seq_len}\"\n",
    "# lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "# print(umask)\n",
    "# print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "07b3fa4d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, contextEmbeddings, rangesSet, encodedSpeakersSet):\n",
    "#         print(contextEmbeddings[0])\n",
    "        self.contextEmbeddings = contextEmbeddings\n",
    "        self.rangesSet = rangesSet\n",
    "        self.encodedSpeakersSet = encodedSpeakersSet\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rangesSet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        startIdx, endIdx = self.rangesSet[idx]\n",
    "#         sequence = self.X_set[\"sequence\"][startIdx:endIdx+1].tolist()\n",
    "        convs = self.contextEmbeddings[idx]\n",
    "        qmask = self.encodedSpeakersSet[startIdx: endIdx+1]\n",
    "        return convs, qmask\n",
    "\n",
    "# Define the ContextEncoding function\n",
    "def ContextEncoding(file_path, dataset):\n",
    "    all_emotions, all_umask, all_seq_lengths = [], [], []\n",
    "    all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths = [], [], [], [], []\n",
    "    max_seq_len = 30\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    for convs, qmask in tqdm(dataloader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        convs = convs[0]\n",
    "        textf = convs.unsqueeze(1)\n",
    "\n",
    "        umask = torch.FloatTensor([[1] * len(textf) + [0] * (max_seq_len - len(textf))])\n",
    "        assert umask.size(1) == max_seq_len, f\"umask size is {umask.size(1)}, expected {max_seq_len}\"\n",
    "        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "        \n",
    "        features, edge_index, \\\n",
    "        edge_norm, edge_type, \\\n",
    "        edge_index_lengths = batch_graphify(textf, \n",
    "                                            qmask,\n",
    "                                            lengths,\n",
    "                                            window_past,\n",
    "                                            window_future,\n",
    "                                            edge_type_mapping,\n",
    "                                            att_model, \n",
    "                                            no_cuda)\n",
    "        all_umask.append(umask)\n",
    "        all_seq_lengths.append(lengths)\n",
    "        all_features.append(features)\n",
    "        all_edge_index.append(edge_index)\n",
    "        all_edge_norm.append(edge_norm)\n",
    "        all_edge_type.append(edge_type)\n",
    "        all_edge_index_lengths.append(edge_index_lengths)\n",
    "        \n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([   all_umask, \\\n",
    "                        all_seq_lengths,\n",
    "                        all_features, \\\n",
    "                        all_edge_index, \\\n",
    "                        all_edge_norm, \\\n",
    "                        all_edge_type, \\\n",
    "                        all_edge_index_lengths], file)\n",
    "    \n",
    "    return all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3c998a81",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|█████████████████████████████████████████████████████████| 918/918 [00:05<00:00, 170.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 6.344594240188599 seconds to encode train text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|█████████████████████████████████████████████████████████| 243/243 [00:01<00:00, 173.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.6044166088104248 seconds to encode test text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|███████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 163.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.6120474338531494 seconds to encode test text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_path1 = 'embed/' + dataset_path + '/pre_h_prime_BERT_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/pre_h_prime_BERT_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/pre_h_prime_BERT_dev.pkl'\n",
    "\n",
    "# Check if files exist\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if not checkFile1:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    trainDataset = ContextDataset(contextualEmbeddingsTrain, rangesTrain, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_features, \\\n",
    "     all_edge_index, \\\n",
    "     all_edge_norm, \\\n",
    "     all_edge_type, \\\n",
    "     all_edge_index_lengths = ContextEncoding(file_path1, trainDataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train text\")\n",
    "\n",
    "if not checkFile2:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTest for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    testDataset = ContextDataset(contextualEmbeddingsTest, rangesTest, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    testContext, _, _, _, _ = ContextEncoding(file_path2, testDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "    \n",
    "if not checkFile3:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersDev for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    devDataset = ContextDataset(contextualEmbeddingsDev, rangesDev, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    devContext, _, _, _, _ = ContextEncoding(file_path3, devDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3:\n",
    "#     with open(file_path1[0], 'rb') as file1:\n",
    "#         trainContext = pickle.load(file1)\n",
    "    with open(file_path1, 'rb') as file1:\n",
    "        _, _, features, edge_index, \\\n",
    "        edge_norm, edge_type, edge_index_lengths = pickle.load(file1)     \n",
    "#     with open(file_path2[0], 'rb') as file2:\n",
    "#         testContext = pickle.load(file2)\n",
    "#     with open(file_path3[0], 'rb') as file3:\n",
    "#         devContext = pickle.load(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e620748f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 768])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextualEmbeddingsTrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c3ac9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918\n",
      "243\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "print(len(contextualEmbeddingsTrain))\n",
    "print(len(contextualEmbeddingsTest))\n",
    "print(len(contextualEmbeddingsDev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a19cb",
   "metadata": {},
   "source": [
    "Unsupervised visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b5f1de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6144])\n"
     ]
    }
   ],
   "source": [
    "# Assuming contextual_embeddings is your list of contextual embeddings\n",
    "\n",
    "# Flatten the list of contextual embeddings into a single list\n",
    "flattened_embeddings = [emb for dialogue in contextual_embeddings for emb in dialogue]\n",
    "\n",
    "# Convert the flattened list into a single tensor\n",
    "tensor_data = torch.tensor(flattened_embeddings)\n",
    "\n",
    "# Check the shape of the tensor\n",
    "print(tensor_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d362bd",
   "metadata": {},
   "source": [
    "Distribution of labels in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3d031c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 1123 train occurrences\n",
      "disgust: 251 train occurrences\n",
      "fear: 244 train occurrences\n",
      "joy: 1567 train occurrences\n",
      "neutral: 4150 train occurrences\n",
      "sadness: 654 train occurrences\n",
      "surprise: 1050 train occurrences\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts for each unique label\n",
    "uniqueLabelsTrain, labelCountsTrain = np.unique(list(X_train[\"Emotion\"]), return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(uniqueLabelsTrain, labelCountsTrain):\n",
    "    print(f\"{labelDecoder[label]}: {count} train occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7a401",
   "metadata": {},
   "source": [
    "Distribution of labels in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9627c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 400 test occurrences\n",
      "disgust: 63 test occurrences\n",
      "fear: 43 test occurrences\n",
      "joy: 321 test occurrences\n",
      "neutral: 1095 test occurrences\n",
      "sadness: 200 test occurrences\n",
      "surprise: 248 test occurrences\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts for each unique label\n",
    "uniqueLabelsTest, labelCountsTest = np.unique(list(X_test[\"Emotion\"]), return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(uniqueLabelsTest, labelCountsTest):\n",
    "    print(f\"{labelDecoder[label]}: {count} test occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b732e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this to true if you finish the context encoding, also manually change the file\n",
    "key = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2b1d4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_label_decoder(df, columns, decoder):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].map(decoder)\n",
    "    return df\n",
    "\n",
    "def load_and_concatenate_predictions(dataset_path):\n",
    "    # Define directory path\n",
    "    directory = f\"data/dump/{dataset_path}/BERT_data_for_classifier\"\n",
    "    \n",
    "    # Create directory if it does not exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Load predictions from pickle files\n",
    "    file_name = f\"{directory}/BERT_predictedTest.pkl\"\n",
    "    with open(file_name, 'rb') as file:\n",
    "        df_predictions_BERT = pickle.load(file)\n",
    "\n",
    "    file_name = f\"{directory}/egat_predictedTest.pkl\"\n",
    "    with open(file_name, 'rb') as file:\n",
    "        df_predictions_BERT_egat = pickle.load(file)\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    df_concatenated = pd.concat([\n",
    "        X_test[[\"Utterance\", \"Emotion\", \"Dialogue_ID\"]],\n",
    "        df_predictions_BERT[\"predicted_label\"].rename(\"BERT_predicted_label\"),\n",
    "        df_predictions_BERT_egat[\"predicted_label\"].rename(\"EGAT_predicted_label\")\n",
    "    ], axis=1)\n",
    "\n",
    "    # Apply label decoder to specified columns\n",
    "    df_concatenated = apply_label_decoder(df_concatenated, [\"Emotion\", \"BERT_predicted_label\", \"EGAT_predicted_label\"], labelDecoder)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    output_file = f\"{directory}/test_predictions.csv\"\n",
    "    df_concatenated.to_csv(output_file, index=False)\n",
    "\n",
    "    return df_concatenated\n",
    "\n",
    "# Example usage:\n",
    "if key:\n",
    "    concatenated_df = load_and_concatenate_predictions(dataset_path)\n",
    "    print(concatenated_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28de57b",
   "metadata": {},
   "source": [
    "#### VIsualize graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7ebc427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/' + dataset_path + '/train_sent_emo_dya.csv', encoding='shift_jis')\n",
    "X_test = pd.read_csv('data/' + dataset_path+ '/test_sent_emo_dya.csv', encoding='utf-8')\n",
    "X_dev = pd.read_csv('data/' + dataset_path + '/dev_sent_emo_dya.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "df616a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Old_Dialogue_ID</th>\n",
       "      <th>Old_Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>Ross, but me down for another box of the mint ...</td>\n",
       "      <td>Monica</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>686</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:13,649</td>\n",
       "      <td>00:12:20,071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>Ah, we’re out. I sold them all.</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>686</td>\n",
       "      <td>1</td>\n",
       "      <td>336</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:20,948</td>\n",
       "      <td>00:12:23,032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>What?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>686</td>\n",
       "      <td>2</td>\n",
       "      <td>336</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:23,534</td>\n",
       "      <td>00:12:25,076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>No.</td>\n",
       "      <td>Monica</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>686</td>\n",
       "      <td>3</td>\n",
       "      <td>336</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:29,915</td>\n",
       "      <td>00:12:32,959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>No, just, just, just a couple more boxes.</td>\n",
       "      <td>Monica</td>\n",
       "      <td>anger</td>\n",
       "      <td>negative</td>\n",
       "      <td>686</td>\n",
       "      <td>4</td>\n",
       "      <td>336</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:29,915</td>\n",
       "      <td>00:12:35,211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>It-it-it’s no big deal, all right, I’m-I’m cool.</td>\n",
       "      <td>Monica</td>\n",
       "      <td>fear</td>\n",
       "      <td>negative</td>\n",
       "      <td>686</td>\n",
       "      <td>5</td>\n",
       "      <td>336</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:29,915</td>\n",
       "      <td>00:12:39,591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>You gotta help me out with a couple more boxes!</td>\n",
       "      <td>Monica</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>686</td>\n",
       "      <td>6</td>\n",
       "      <td>336</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:33,502</td>\n",
       "      <td>00:12:44,137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>Mon, look at yourself. You have cookie on your...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>disgust</td>\n",
       "      <td>negative</td>\n",
       "      <td>686</td>\n",
       "      <td>7</td>\n",
       "      <td>336</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:45,222</td>\n",
       "      <td>00:12:47,724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>Oh God!</td>\n",
       "      <td>Monica</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>686</td>\n",
       "      <td>8</td>\n",
       "      <td>336</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:48,184</td>\n",
       "      <td>00:12:49,225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>Okay, the other night I was leaving the museum...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>686</td>\n",
       "      <td>9</td>\n",
       "      <td>336</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:12:59,987</td>\n",
       "      <td>00:13:09,788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>That’s when it occurred to me, the key to my s...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>686</td>\n",
       "      <td>10</td>\n",
       "      <td>336</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:13:09,955</td>\n",
       "      <td>00:13:20,006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>I am selling cookies by the case.</td>\n",
       "      <td>Ross</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>686</td>\n",
       "      <td>11</td>\n",
       "      <td>336</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:13:21,967</td>\n",
       "      <td>00:13:25,136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>They call me: 'Cookie Dude!'</td>\n",
       "      <td>Ross</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>686</td>\n",
       "      <td>12</td>\n",
       "      <td>336</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:13:25,304</td>\n",
       "      <td>00:13:28,973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Utterance Speaker   Emotion  \\\n",
       "4017  Ross, but me down for another box of the mint ...  Monica   neutral   \n",
       "4018                    Ah, we’re out. I sold them all.    Ross   neutral   \n",
       "4019                                              What?  Monica  surprise   \n",
       "4020                                                No.  Monica   sadness   \n",
       "4021          No, just, just, just a couple more boxes.  Monica     anger   \n",
       "4022   It-it-it’s no big deal, all right, I’m-I’m cool.  Monica      fear   \n",
       "4023    You gotta help me out with a couple more boxes!  Monica   sadness   \n",
       "4024  Mon, look at yourself. You have cookie on your...    Ross   disgust   \n",
       "4025                                            Oh God!  Monica   sadness   \n",
       "4026  Okay, the other night I was leaving the museum...    Ross       joy   \n",
       "4027  That’s when it occurred to me, the key to my s...    Ross       joy   \n",
       "4028                  I am selling cookies by the case.    Ross       joy   \n",
       "4029                       They call me: 'Cookie Dude!'    Ross       joy   \n",
       "\n",
       "     Sentiment  Dialogue_ID  Utterance_ID  Old_Dialogue_ID  Old_Utterance_ID  \\\n",
       "4017   neutral          686             0              336                 2   \n",
       "4018   neutral          686             1              336                 3   \n",
       "4019  positive          686             2              336                 4   \n",
       "4020  negative          686             3              336                 6   \n",
       "4021  negative          686             4              336                 7   \n",
       "4022  negative          686             5              336                 8   \n",
       "4023  negative          686             6              336                 9   \n",
       "4024  negative          686             7              336                10   \n",
       "4025  negative          686             8              336                11   \n",
       "4026  positive          686             9              336                15   \n",
       "4027  positive          686            10              336                16   \n",
       "4028  positive          686            11              336                17   \n",
       "4029  positive          686            12              336                18   \n",
       "\n",
       "      Season  Episode     StartTime       EndTime  \n",
       "4017       3       10  00:12:13,649  00:12:20,071  \n",
       "4018       3       10  00:12:20,948  00:12:23,032  \n",
       "4019       3       10  00:12:23,534  00:12:25,076  \n",
       "4020       3       10  00:12:29,915  00:12:32,959  \n",
       "4021       3       10  00:12:29,915  00:12:35,211  \n",
       "4022       3       10  00:12:29,915  00:12:39,591  \n",
       "4023       3       10  00:12:33,502  00:12:44,137  \n",
       "4024       3       10  00:12:45,222  00:12:47,724  \n",
       "4025       3       10  00:12:48,184  00:12:49,225  \n",
       "4026       3       10  00:12:59,987  00:13:09,788  \n",
       "4027       3       10  00:13:09,955  00:13:20,006  \n",
       "4028       3       10  00:13:21,967  00:13:25,136  \n",
       "4029       3       10  00:13:25,304  00:13:28,973  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[4017:4030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1d97b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_df['Emotion'].map(labelDecoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80265319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n",
      "{0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "print(labelDecoder)\n",
    "print(labelDecoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fc531d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2944104896.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [170]\u001b[1;36m\u001b[0m\n\u001b[1;33m    if dataset_path == \"dataset_original\"\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def visualize_combined_dialogues(dialogues_df):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges for each dialogue\n",
    "    for dialogue_id in dialogues_df['Dialogue_ID'].unique():\n",
    "        df = dialogues_df[dialogue_id == dialogues_df['Dialogue_ID']]\n",
    "        \n",
    "        # Add nodes with emotion as an attribute\n",
    "        for idx, row in df.iterrows():\n",
    "            G.add_node((dialogue_id, row['Utterance_ID']), emotion=row['Emotion'])\n",
    "        \n",
    "        # Add edges (for simplicity, connect consecutive utterances)\n",
    "        for i in range(len(df) - 1):\n",
    "            G.add_edge((dialogue_id, df['Utterance_ID'].iloc[i]), (dialogue_id, df['Utterance_ID'].iloc[i + 1]))\n",
    "    \n",
    "    pos = nx.spring_layout(G, seed=42)  # Position nodes using the spring layout\n",
    "    emotions = nx.get_node_attributes(G, 'emotion')\n",
    "    node_colors = [emotions[node] for node in G.nodes()]\n",
    "    \n",
    "    # Use a specific color map for clear differentiation\n",
    "    cmap = plt.get_cmap('tab10', len(labelDecoder2))\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    nx.draw(G, pos, node_color=node_colors, cmap=cmap, \n",
    "            node_size=300, \n",
    "#             with_labels=True,\n",
    "            alpha=0.9)\n",
    "    \n",
    "    # Create a legend using labelDecoder2\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                          markerfacecolor=cmap(i / len(labelDecoder2)), markersize=10) \n",
    "               for i in range(len(labelDecoder2))]\n",
    "    plt.legend(handles, [labelDecoder2[i] for i in range(len(labelDecoder2))], title='Emotions', loc='best')\n",
    "    \n",
    "    plt.title('Combined Dialogue Graphs')\n",
    "    plt.show()\n",
    "\n",
    "if dataset_path == \"dataset_original\":\n",
    "    sampled_dialogues = X_train['Dialogue_ID'].unique()\n",
    "    MAX_G = 60\n",
    "    if len(sampled_dialogues) > MAX_G:\n",
    "        sampled_dialogues = random.sample(list(sampled_dialogues), MAX_G)\n",
    "\n",
    "    sampled_df = X_train[X_train['Dialogue_ID'].isin(sampled_dialogues)]\n",
    "\n",
    "    # Encoding emotions using custom labelEncoder2\n",
    "    sampled_df['Emotion'] = sampled_df['Emotion'].map(labelEncoder2)\n",
    "    # Visualize the combined dialogues\n",
    "    visualize_combined_dialogues(sampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c07b8f",
   "metadata": {},
   "source": [
    "Visualize utterance embeddnig (u') with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d430fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.tensor(X_train[\"Emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1043832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed63efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runTSNE = 1\n",
    "# if runTSNE:\n",
    "#     from sklearn.manifold import TSNE\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     # List of perplexity values to loop over\n",
    "#     perplexity_values = [50]\n",
    "\n",
    "#     # Loop over each perplexity value\n",
    "#     for perplexity in perplexity_values:\n",
    "#         # Initialize t-SNE with the current perplexity value\n",
    "#         tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "#         # Fit and transform the data using t-SNE\n",
    "#         h_prime_tsne = tsne.fit_transform(tensor_data.detach().numpy())\n",
    "\n",
    "#         # Plot the node embeddings with different colors for each label\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         for label, emotion in zip(range(len(label_encoder)), label_encoder):\n",
    "#             indices = (labels == label).nonzero().squeeze()\n",
    "#             plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "#         plt.title(f'Utterance Embeddings (Train) Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "#         plt.xlabel('Dimension 1', color=\"white\")\n",
    "#         plt.ylabel('Dimension 2', color=\"white\")\n",
    "#         plt.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e2170c",
   "metadata": {},
   "source": [
    " Visualize utterance embedding (u') with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(tensor_data.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(labels):\n",
    "#     indices = labels == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
