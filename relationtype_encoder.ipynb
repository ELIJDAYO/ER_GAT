{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3cd42",
   "metadata": {},
   "source": [
    "Put libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a88b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553916bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, pickle, sys,torch.nn.init as init, dgl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "# from GAT import GAT\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a0066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Path: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\\utils\\constans.py\n",
      "Project Directory: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\n"
     ]
    }
   ],
   "source": [
    "script_path = os.path.abspath(\"utils\\\\constans.py\")  # Replace __file__ with the path to your script if in a notebook\n",
    "\n",
    "# Determine the project directory by moving up two levels (adjust as needed)\n",
    "project_directory = os.path.dirname(os.path.dirname(script_path))\n",
    "\n",
    "print(\"Script Path:\", script_path)\n",
    "print(\"Project Directory:\", project_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc1152",
   "metadata": {},
   "source": [
    "Code related to GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecf3858",
   "metadata": {
    "code_folding": [
     1,
     6,
     17,
     41
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class DialogueGraphDataLoader(DataLoader):\n",
    "    def __init__(self, node_features_list, edge_index_list, batch_size=1, shuffle=False):\n",
    "        graph_dataset = DialogueGraphDataset(node_features_list, edge_index_list)\n",
    "        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=dialogue_graph_collate_fn)\n",
    "\n",
    "class DialogueGraphDataset(Dataset):\n",
    "    def __init__(self, node_features_list, edge_index_list):\n",
    "        self.node_features_list = node_features_list\n",
    "        self.edge_index_list = edge_index_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.edge_index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.node_features_list[idx], self.edge_index_list[idx]\n",
    "\n",
    "def dialogue_graph_collate_fn(batch):\n",
    "    node_features_list, edge_index_list = zip(*batch)\n",
    "    \n",
    "    node_features_list_combined = []\n",
    "    num_nodes_seen = 0\n",
    "\n",
    "    for node_features, edge_index in zip(node_features_list, edge_index_list):\n",
    "        # Assuming node_features is a tuple (text_embeddings, speakers_list)\n",
    "        text_embeddings, speakers_list = node_features\n",
    "        combined_features = (text_embeddings, speakers_list)\n",
    "\n",
    "        node_features_list_combined.append(combined_features)\n",
    "\n",
    "        # Translate the range of edge_index\n",
    "        edge_index_list.append(edge_index + num_nodes_seen)\n",
    "        num_nodes_seen += len(text_embeddings)\n",
    "\n",
    "    # Merge the dialogue graphs into a single graph with multiple connected components\n",
    "    node_features_combined = [torch.cat(features, 1) for features in zip(*node_features_list_combined)]\n",
    "    edge_index = torch.cat(edge_index_list, 1)\n",
    "\n",
    "    return node_features_combined, edge_index\n",
    "\n",
    "# get one hot encoding to normalize values\n",
    "def get_ohe(edge_types):\n",
    "    # Number of classes\n",
    "    num_classes = torch.max(edge_types) + 1\n",
    "\n",
    "    # Convert to one-hot encoding\n",
    "    one_hot_encoding = F.one_hot(edge_types, num_classes).float()\n",
    "\n",
    "    return one_hot_encoding\n",
    "\n",
    "# print(edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75f4cb1c",
   "metadata": {
    "code_folding": [
     52
    ]
   },
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        torch.manual_seed(42)\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "        \n",
    "        # Edge type embeddings\n",
    "        torch.manual_seed(42)  # Set your desired seed value\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "        \n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "        print(\"node_features.shape: \",node_features.shape, \" edge_indices: \", edge_indices.shape)\n",
    "        print(\"edge_type.shape: \",  edge_type.shape)\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        print(\"h_linear.shape after linear_proj of node_features: \",h_linear.shape)\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "        print(\"h_linear.shape after view: \",h_linear.shape)\n",
    "        # Transpose dimensions of h_linear to match edge_type_embedding's shape\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "        print(\"h_linear.shape after permuting dimension: \",h_linear.shape)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "        print(\"edge_type_embedding.shape after transpose: \",edge_type_embedding.shape)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "        print(\"attention_scores..shape after matmul h_linear and edge_type_emb: \",attention_scores.shape)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "         # Weighted sum of neighbor node representations\n",
    "        print(\"attention_coefficients.shape after softmax: \",attention_coefficients.shape)\n",
    "#       the one below is for edges\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).mean(dim=2)\n",
    "#         the one below is for attention heads\n",
    "#         updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).sum(dim=1)\n",
    "        print(\"updated_representation.shape after matmul of trasposed attn_coef and h_linear and sum at dim=1: \",updated_representation.shape)\n",
    "\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores\n",
    "\n",
    "class EGATConv(nn.Module):\n",
    "    r\"\"\"\n",
    "    \n",
    "    Description\n",
    "    -----------\n",
    "    Apply Graph Attention Layer over input graph. EGAT is an extension\n",
    "    of regular `Graph Attention Network <https://arxiv.org/pdf/1710.10903.pdf>`__ \n",
    "    handling edge features, detailed description is available in\n",
    "    `Rossmann-Toolbox <https://pubmed.ncbi.nlm.nih.gov/34571541/>`__ (see supplementary data).\n",
    "     The difference appears in the method how unnormalized attention scores :math:`e_{ij}`\n",
    "     are obtain:\n",
    "        \n",
    "    .. math::\n",
    "        e_{ij} &= \\vec{F} (f_{ij}^{\\prime})\n",
    "\n",
    "        f_{ij}^{\\prim} &= \\mathrm{LeakyReLU}\\left(A [ h_{i} \\| f_{ij} \\| h_{j}]\\right)\n",
    "\n",
    "    where :math:`f_{ij}^{\\prim}` are edge features, :math:`\\mathrm{A}` is weight matrix and \n",
    "    :math: `\\vec{F}` is weight vector. After that resulting node features \n",
    "    :math:`h_{i}^{\\prim}` are updated in the same way as in regular GAT. \n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_node_feats : int\n",
    "        Input node feature size :math:`h_{i}`.\n",
    "    in_edge_feats : int\n",
    "        Input edge feature size :math:`f_{ij}`.\n",
    "    out_node_feats : int\n",
    "        Output nodes feature size.\n",
    "    out_edge_feats : int\n",
    "        Output edge feature size.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    bias : bool, optional\n",
    "        If True, learns a bias term. Defaults: ``True``.\n",
    "        \n",
    "    Examples\n",
    "    ----------\n",
    "    >>> import dgl\n",
    "    >>> import torch as th\n",
    "    >>> from dgl.nn import EGATConv\n",
    "    >>> \n",
    "    >>> num_nodes, num_edges = 8, 30\n",
    "    >>>#define connections\n",
    "    >>> u, v = th.randint(num_nodes, num_edges), th.randint(num_nodes, num_edges) \n",
    "    >>> graph = dgl.graph((u,v))    \n",
    "\n",
    "    >>> node_feats = th.rand((num_nodes, 20)) \n",
    "    >>> edge_feats = th.rand((num_edges, 12))\n",
    "    >>> egat = EGATConv(in_node_feats=20,\n",
    "                          in_edge_feats=12,\n",
    "                          out_node_feats=15,\n",
    "                          out_edge_feats=10,\n",
    "                          num_heads=3)\n",
    "    >>> #forward pass                    \n",
    "    >>> new_node_feats, new_edge_feats = egat(graph, node_feats, edge_feats)\n",
    "    >>> new_node_feats.shape, new_edge_feats.shape\n",
    "    ((8, 3, 12), (30, 3, 10))\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_node_feats,\n",
    "                 in_edge_feats,\n",
    "                 out_node_feats,\n",
    "                 out_edge_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 **kw_args):\n",
    "        \n",
    "        super().__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._out_node_feats = out_node_feats\n",
    "        self._out_edge_feats = out_edge_feats\n",
    "        \n",
    "        self.fc_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=bias)\n",
    "        self.fc_ni = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_nj = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        \n",
    "        # Attention parameter\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reinitialize learnable parameters.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(42)  # You can use any integer value as the seed\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.xavier_normal_(self.fc_node.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "        init.xavier_normal_(self.attn, gain=gain)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "        r\"\"\"\n",
    "        Compute new node and edge features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : DGLGraph\n",
    "            The graph.\n",
    "        nfeats : torch.Tensor\n",
    "            The input node feature of shape :math:`(*, D_{in})`\n",
    "            where:\n",
    "                :math:`D_{in}` is size of input node feature,\n",
    "                :math:`*` is the number of nodes.\n",
    "        efeats: torch.Tensor\n",
    "             The input edge feature of shape :math:`(*, F_{in})`\n",
    "             where:\n",
    "                 :math:`F_{in}` is size of input node feauture,\n",
    "                 :math:`*` is the number of edges.\n",
    "        get_attention : bool, optional\n",
    "                Whether to return the attention values. Default to False.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pair of torch.Tensor\n",
    "            node output features followed by edge output features\n",
    "            The node output feature of shape :math:`(*, H, D_{out})` \n",
    "            The edge output feature of shape :math:`(*, H, F_{out})`\n",
    "            where:\n",
    "                :math:`H` is the number of heads,\n",
    "                :math:`D_{out}` is size of output node feature,\n",
    "                :math:`F_{out}` is size of output edge feature.            \n",
    "        \"\"\"\n",
    "        \n",
    "        with graph.local_scope():\n",
    "            # TODO allow node src and dst feats\n",
    "            graph.edata['f'] = efeats\n",
    "            graph.ndata['h'] = nfeats\n",
    "            # calc edge attention\n",
    "            # same trick way as in dgl.nn.pytorch.GATConv, but also includes edge feats\n",
    "            # https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py#L297\n",
    "            f_ni = self.fc_ni(nfeats)\n",
    "            f_nj = self.fc_nj(nfeats)\n",
    "            f_fij = self.fc_fij(efeats)\n",
    "            graph.srcdata.update({'f_ni' : f_ni})\n",
    "            graph.dstdata.update({'f_nj' : f_nj})\n",
    "            #graph.edata.update({'f_fij' : f_fij})\n",
    "            # add ni, nj factors\n",
    "            graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "            # add fij to node factor\n",
    "            f_out = graph.edata.pop('f_tmp') + f_fij \n",
    "            if self.bias is not None:\n",
    "                f_out+= self.bias\n",
    "            f_out = nn.functional.leaky_relu(f_out)\n",
    "            f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "            # compute attention factor\n",
    "            e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.edata['a'] = edge_softmax(graph, e)\n",
    "            graph.ndata['h_out'] = self.fc_node(nfeats).view(-1, self._num_heads, self._out_node_feats)\n",
    "            # calc weighted sum \n",
    "            graph.update_all(fn.u_mul_e('h_out', 'a', 'm'),\n",
    "                            fn.sum('m', 'h_out'))\n",
    "\n",
    "            h_out = graph.ndata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "            if get_attention:\n",
    "                return h_out, f_out, graph.edata.pop('a')\n",
    "            else:\n",
    "                return h_out, f_out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e3ed3",
   "metadata": {},
   "source": [
    "<h3>Methods definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53e5417",
   "metadata": {
    "code_folding": [
     24,
     58
    ]
   },
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    # Initialize an empty list to store pairs\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "#     node_pairs_dict = {}\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 0\n",
    "        while (val <= 3)  and (i+val <= end_idx):\n",
    "            target_idx = i+val\n",
    "#                 print(target_idx)\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "#                 node_pairs_dict[i] = target_idx\n",
    "            val = val+1\n",
    "    \n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "\n",
    "    # Iterate through pairs of nodes\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "\n",
    "#         # Add source node to target node's neighbors\n",
    "#         if target_node not in adjacency_list_dict:\n",
    "#             adjacency_list_dict[target_node] = [source_node]\n",
    "#         else:\n",
    "#             adjacency_list_dict[target_node].append(source_node)\n",
    "\n",
    "        # Add target node to source node's neighbors\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "\n",
    "    return adjacency_list_dict\n",
    "# print(ranges[:1])\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        \n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    \n",
    "    for i in range(dialogs_len): #2140 dialogs\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "#         print(i, \" th dialogue\")\n",
    "#         print(i, dialog_speakers)\n",
    "        for j in range(dialog_len): #num utterances\n",
    "            src_node = dialog_nodes_pairs[j] # j = key = src node\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k] # k = value = targ node\n",
    "                # edge_types = torch.tensor([0, 1, 2]) \n",
    "                # 0: cur-self, 1: past-self, 2: past-other/past-inter\n",
    "                                \n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "#                     print(\"This is 0 \", node_i_idx, node_j_idx)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "#                         print(\"This is 1 \", node_i_idx, node_j_idx)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "#                         print(\"This is 2 \", node_i_idx, node_j_idx)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))  \n",
    "                    \n",
    "    return whole_edge_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a199d8f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print(edge_indices[0][0][3])\n",
    "# len(edge_indices[0].keys())\n",
    "# list(encoded_speaker_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afbacda",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# assume this is working\n",
    "# edge_indices = get_all_adjacency_list(ranges)\n",
    "# edge_types = get_all_edge_type_list(edge_indices, encoded_speaker_list)\n",
    "# edge_indices = get_all_adjacency_list(ranges, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f2ae80",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print((edge_types[:10]))\n",
    "# edge_indices[:10]\n",
    "# (updated_representations[0].shape)\n",
    "# edge_indices[0]\n",
    "# edge_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20cb34a8",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder.pkl\")\n",
    "encoded_speaker_list = []\n",
    "if checkFile is False:\n",
    "    print(\"Run first the prototype_context_encoder to generate this file\")\n",
    "else:\n",
    "    file = open('data/dump/speaker_encoder.pkl', \"rb\")\n",
    "    encoded_speaker_list, ranges = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6bc790",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# need update\n",
    "# checkFile = os.path.isfile(\"data/dump/all_adjacency_list.pkl\")\n",
    "# adjacency_list = []\n",
    "# if checkFile is False:\n",
    "#     adjacency_list = get_all_adjacency_list(ranges)\n",
    "# else:\n",
    "#     file = open('data/dump/all_adjacency_list.pkl', \"rb\")\n",
    "#     adjacency_list = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d5fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacency_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8072d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f02158e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "file_path = 'embed/updated_representation_list.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    updated_representations = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "293f6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(updated_representations[0].shape)\n",
    "# print(updated_representations[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48400c3a",
   "metadata": {},
   "source": [
    "<h3> Making Progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea3af5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = get_all_adjacency_list(ranges)\n",
    "sample = edge_indices[0]\n",
    "edge_types = get_all_edge_type_list(edge_indices, encoded_speaker_list)\n",
    "edge_indices = get_all_adjacency_list(ranges, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50517bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f42f4561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 300])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_representations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "658dd100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1,\n",
       "        0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 0,\n",
       "        1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed5fed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_features.shape:  torch.Size([14, 300])  edge_indices:  torch.Size([2, 50])\n",
      "edge_type.shape:  torch.Size([50])\n",
      "h_linear.shape after linear_proj of node_features:  torch.Size([14, 1200])\n",
      "h_linear.shape after view:  torch.Size([14, 4, 300])\n",
      "h_linear.shape after permuting dimension:  torch.Size([14, 300, 4])\n",
      "edge_type_embedding.shape after transpose:  torch.Size([4, 50])\n",
      "attention_scores..shape after matmul h_linear and edge_type_emb:  torch.Size([14, 300, 50])\n",
      "attention_coefficients.shape after softmax:  torch.Size([14, 300, 50])\n",
      "updated_representation.shape after matmul of trasposed attn_coef and h_linear and sum at dim=1:  torch.Size([14, 50])\n",
      "dialogue_representation[0] shape: torch.Size([14, 300])\n",
      "Attention coef shape: torch.Size([14, 300, 50])\n",
      "h_prime shape: torch.Size([14, 50])\n"
     ]
    }
   ],
   "source": [
    "num_in_features = 300\n",
    "num_out_features = 300\n",
    "num_heads = 4\n",
    "num_edge_types = 3\n",
    "\n",
    "gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)\n",
    "\n",
    "i = 0\n",
    "h_prime, attention_coef = gat_layer((updated_representations[i], edge_indices[i]), edge_types[i])\n",
    "print(f\"dialogue_representation[{i}] shape:\", updated_representations[i].shape)\n",
    "print(\"Attention coef shape:\", attention_coef.shape)\n",
    "print(\"h_prime shape:\", h_prime.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a86b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 50])\n",
      "tensor([[-2.6777e-02, -3.8597e-02, -1.1790e-02, -3.8597e-02, -2.6777e-02,\n",
      "         -3.8597e-02, -1.1790e-02, -3.8597e-02, -2.6777e-02, -3.8597e-02,\n",
      "         -1.1790e-02, -3.8597e-02, -2.6777e-02, -3.8597e-02, -1.1790e-02,\n",
      "         -3.8597e-02, -2.6777e-02, -3.8597e-02, -1.1790e-02, -3.8597e-02,\n",
      "         -2.6777e-02, -3.8597e-02, -1.1790e-02, -3.8597e-02, -2.6777e-02,\n",
      "         -3.8597e-02, -1.1790e-02, -3.8597e-02, -2.6777e-02, -3.8597e-02,\n",
      "         -1.1790e-02, -3.8597e-02, -2.6777e-02, -3.8597e-02, -1.1790e-02,\n",
      "         -3.8597e-02, -2.6777e-02, -3.8597e-02, -1.1790e-02, -3.8597e-02,\n",
      "         -2.6777e-02, -3.8597e-02, -1.1790e-02, -3.8597e-02, -2.6777e-02,\n",
      "         -3.8597e-02, -1.1790e-02, -2.6777e-02, -3.8597e-02, -2.6777e-02],\n",
      "        [-1.3126e-02, -1.8014e-02, -7.0997e-03, -1.8014e-02, -1.3126e-02,\n",
      "         -1.8014e-02, -7.0997e-03, -1.8014e-02, -1.3126e-02, -1.8014e-02,\n",
      "         -7.0997e-03, -1.8014e-02, -1.3126e-02, -1.8014e-02, -7.0997e-03,\n",
      "         -1.8014e-02, -1.3126e-02, -1.8014e-02, -7.0997e-03, -1.8014e-02,\n",
      "         -1.3126e-02, -1.8014e-02, -7.0997e-03, -1.8014e-02, -1.3126e-02,\n",
      "         -1.8014e-02, -7.0997e-03, -1.8014e-02, -1.3126e-02, -1.8014e-02,\n",
      "         -7.0997e-03, -1.8014e-02, -1.3126e-02, -1.8014e-02, -7.0997e-03,\n",
      "         -1.8014e-02, -1.3126e-02, -1.8014e-02, -7.0997e-03, -1.8014e-02,\n",
      "         -1.3126e-02, -1.8014e-02, -7.0997e-03, -1.8014e-02, -1.3126e-02,\n",
      "         -1.8014e-02, -7.0997e-03, -1.3126e-02, -1.8014e-02, -1.3126e-02],\n",
      "        [-5.0447e-03, -8.4947e-03, -7.8676e-04, -8.4947e-03, -5.0447e-03,\n",
      "         -8.4947e-03, -7.8676e-04, -8.4947e-03, -5.0447e-03, -8.4947e-03,\n",
      "         -7.8676e-04, -8.4947e-03, -5.0447e-03, -8.4947e-03, -7.8676e-04,\n",
      "         -8.4947e-03, -5.0447e-03, -8.4947e-03, -7.8676e-04, -8.4947e-03,\n",
      "         -5.0447e-03, -8.4947e-03, -7.8676e-04, -8.4947e-03, -5.0447e-03,\n",
      "         -8.4947e-03, -7.8676e-04, -8.4947e-03, -5.0447e-03, -8.4947e-03,\n",
      "         -7.8676e-04, -8.4947e-03, -5.0447e-03, -8.4947e-03, -7.8676e-04,\n",
      "         -8.4947e-03, -5.0447e-03, -8.4947e-03, -7.8676e-04, -8.4947e-03,\n",
      "         -5.0447e-03, -8.4947e-03, -7.8676e-04, -8.4947e-03, -5.0447e-03,\n",
      "         -8.4947e-03, -7.8676e-04, -5.0447e-03, -8.4947e-03, -5.0447e-03],\n",
      "        [-1.5194e-03, -4.8519e-03,  2.4076e-03, -4.8519e-03, -1.5194e-03,\n",
      "         -4.8519e-03,  2.4076e-03, -4.8519e-03, -1.5194e-03, -4.8519e-03,\n",
      "          2.4076e-03, -4.8519e-03, -1.5194e-03, -4.8519e-03,  2.4076e-03,\n",
      "         -4.8519e-03, -1.5194e-03, -4.8519e-03,  2.4076e-03, -4.8519e-03,\n",
      "         -1.5194e-03, -4.8519e-03,  2.4076e-03, -4.8519e-03, -1.5194e-03,\n",
      "         -4.8519e-03,  2.4076e-03, -4.8519e-03, -1.5194e-03, -4.8519e-03,\n",
      "          2.4076e-03, -4.8519e-03, -1.5194e-03, -4.8519e-03,  2.4076e-03,\n",
      "         -4.8519e-03, -1.5194e-03, -4.8519e-03,  2.4076e-03, -4.8519e-03,\n",
      "         -1.5194e-03, -4.8519e-03,  2.4076e-03, -4.8519e-03, -1.5194e-03,\n",
      "         -4.8519e-03,  2.4076e-03, -1.5194e-03, -4.8519e-03, -1.5194e-03],\n",
      "        [ 1.7807e-04, -3.0753e-03,  3.9339e-03, -3.0753e-03,  1.7807e-04,\n",
      "         -3.0753e-03,  3.9339e-03, -3.0753e-03,  1.7807e-04, -3.0753e-03,\n",
      "          3.9339e-03, -3.0753e-03,  1.7807e-04, -3.0753e-03,  3.9339e-03,\n",
      "         -3.0753e-03,  1.7807e-04, -3.0753e-03,  3.9339e-03, -3.0753e-03,\n",
      "          1.7807e-04, -3.0753e-03,  3.9339e-03, -3.0753e-03,  1.7807e-04,\n",
      "         -3.0753e-03,  3.9339e-03, -3.0753e-03,  1.7807e-04, -3.0753e-03,\n",
      "          3.9339e-03, -3.0753e-03,  1.7807e-04, -3.0753e-03,  3.9339e-03,\n",
      "         -3.0753e-03,  1.7807e-04, -3.0753e-03,  3.9339e-03, -3.0753e-03,\n",
      "          1.7807e-04, -3.0753e-03,  3.9339e-03, -3.0753e-03,  1.7807e-04,\n",
      "         -3.0753e-03,  3.9339e-03,  1.7807e-04, -3.0753e-03,  1.7807e-04],\n",
      "        [ 1.5173e-03, -2.0053e-03,  5.3491e-03, -2.0053e-03,  1.5173e-03,\n",
      "         -2.0053e-03,  5.3491e-03, -2.0053e-03,  1.5173e-03, -2.0053e-03,\n",
      "          5.3491e-03, -2.0053e-03,  1.5173e-03, -2.0053e-03,  5.3491e-03,\n",
      "         -2.0053e-03,  1.5173e-03, -2.0053e-03,  5.3491e-03, -2.0053e-03,\n",
      "          1.5173e-03, -2.0053e-03,  5.3491e-03, -2.0053e-03,  1.5173e-03,\n",
      "         -2.0053e-03,  5.3491e-03, -2.0053e-03,  1.5173e-03, -2.0053e-03,\n",
      "          5.3491e-03, -2.0053e-03,  1.5173e-03, -2.0053e-03,  5.3491e-03,\n",
      "         -2.0053e-03,  1.5173e-03, -2.0053e-03,  5.3491e-03, -2.0053e-03,\n",
      "          1.5173e-03, -2.0053e-03,  5.3491e-03, -2.0053e-03,  1.5173e-03,\n",
      "         -2.0053e-03,  5.3491e-03,  1.5173e-03, -2.0053e-03,  1.5173e-03],\n",
      "        [ 4.1534e-03,  8.4429e-04,  7.7527e-03,  8.4429e-04,  4.1534e-03,\n",
      "          8.4429e-04,  7.7527e-03,  8.4429e-04,  4.1534e-03,  8.4429e-04,\n",
      "          7.7527e-03,  8.4429e-04,  4.1534e-03,  8.4429e-04,  7.7527e-03,\n",
      "          8.4429e-04,  4.1534e-03,  8.4429e-04,  7.7527e-03,  8.4429e-04,\n",
      "          4.1534e-03,  8.4429e-04,  7.7527e-03,  8.4429e-04,  4.1534e-03,\n",
      "          8.4429e-04,  7.7527e-03,  8.4429e-04,  4.1534e-03,  8.4429e-04,\n",
      "          7.7527e-03,  8.4429e-04,  4.1534e-03,  8.4429e-04,  7.7527e-03,\n",
      "          8.4429e-04,  4.1534e-03,  8.4429e-04,  7.7527e-03,  8.4429e-04,\n",
      "          4.1534e-03,  8.4429e-04,  7.7527e-03,  8.4429e-04,  4.1534e-03,\n",
      "          8.4429e-04,  7.7527e-03,  4.1534e-03,  8.4429e-04,  4.1534e-03],\n",
      "        [ 3.5506e-03, -8.4344e-05,  7.4204e-03, -8.4344e-05,  3.5506e-03,\n",
      "         -8.4344e-05,  7.4204e-03, -8.4344e-05,  3.5506e-03, -8.4344e-05,\n",
      "          7.4204e-03, -8.4344e-05,  3.5506e-03, -8.4344e-05,  7.4204e-03,\n",
      "         -8.4344e-05,  3.5506e-03, -8.4344e-05,  7.4204e-03, -8.4344e-05,\n",
      "          3.5506e-03, -8.4344e-05,  7.4204e-03, -8.4344e-05,  3.5506e-03,\n",
      "         -8.4344e-05,  7.4204e-03, -8.4344e-05,  3.5506e-03, -8.4344e-05,\n",
      "          7.4204e-03, -8.4344e-05,  3.5506e-03, -8.4344e-05,  7.4204e-03,\n",
      "         -8.4344e-05,  3.5506e-03, -8.4344e-05,  7.4204e-03, -8.4344e-05,\n",
      "          3.5506e-03, -8.4344e-05,  7.4204e-03, -8.4344e-05,  3.5506e-03,\n",
      "         -8.4344e-05,  7.4204e-03,  3.5506e-03, -8.4344e-05,  3.5506e-03],\n",
      "        [ 4.7946e-03,  1.5317e-03,  8.4869e-03,  1.5317e-03,  4.7946e-03,\n",
      "          1.5317e-03,  8.4869e-03,  1.5317e-03,  4.7946e-03,  1.5317e-03,\n",
      "          8.4869e-03,  1.5317e-03,  4.7946e-03,  1.5317e-03,  8.4869e-03,\n",
      "          1.5317e-03,  4.7946e-03,  1.5317e-03,  8.4869e-03,  1.5317e-03,\n",
      "          4.7946e-03,  1.5317e-03,  8.4869e-03,  1.5317e-03,  4.7946e-03,\n",
      "          1.5317e-03,  8.4869e-03,  1.5317e-03,  4.7946e-03,  1.5317e-03,\n",
      "          8.4869e-03,  1.5317e-03,  4.7946e-03,  1.5317e-03,  8.4869e-03,\n",
      "          1.5317e-03,  4.7946e-03,  1.5317e-03,  8.4869e-03,  1.5317e-03,\n",
      "          4.7946e-03,  1.5317e-03,  8.4869e-03,  1.5317e-03,  4.7946e-03,\n",
      "          1.5317e-03,  8.4869e-03,  4.7946e-03,  1.5317e-03,  4.7946e-03],\n",
      "        [ 4.7926e-03,  1.4544e-03,  8.8686e-03,  1.4544e-03,  4.7926e-03,\n",
      "          1.4544e-03,  8.8686e-03,  1.4544e-03,  4.7926e-03,  1.4544e-03,\n",
      "          8.8686e-03,  1.4544e-03,  4.7926e-03,  1.4544e-03,  8.8686e-03,\n",
      "          1.4544e-03,  4.7926e-03,  1.4544e-03,  8.8686e-03,  1.4544e-03,\n",
      "          4.7926e-03,  1.4544e-03,  8.8686e-03,  1.4544e-03,  4.7926e-03,\n",
      "          1.4544e-03,  8.8686e-03,  1.4544e-03,  4.7926e-03,  1.4544e-03,\n",
      "          8.8686e-03,  1.4544e-03,  4.7926e-03,  1.4544e-03,  8.8686e-03,\n",
      "          1.4544e-03,  4.7926e-03,  1.4544e-03,  8.8686e-03,  1.4544e-03,\n",
      "          4.7926e-03,  1.4544e-03,  8.8686e-03,  1.4544e-03,  4.7926e-03,\n",
      "          1.4544e-03,  8.8686e-03,  4.7926e-03,  1.4544e-03,  4.7926e-03],\n",
      "        [ 6.3430e-03,  2.6809e-03,  1.0978e-02,  2.6809e-03,  6.3430e-03,\n",
      "          2.6809e-03,  1.0978e-02,  2.6809e-03,  6.3430e-03,  2.6809e-03,\n",
      "          1.0978e-02,  2.6809e-03,  6.3430e-03,  2.6809e-03,  1.0978e-02,\n",
      "          2.6809e-03,  6.3430e-03,  2.6809e-03,  1.0978e-02,  2.6809e-03,\n",
      "          6.3430e-03,  2.6809e-03,  1.0978e-02,  2.6809e-03,  6.3430e-03,\n",
      "          2.6809e-03,  1.0978e-02,  2.6809e-03,  6.3430e-03,  2.6809e-03,\n",
      "          1.0978e-02,  2.6809e-03,  6.3430e-03,  2.6809e-03,  1.0978e-02,\n",
      "          2.6809e-03,  6.3430e-03,  2.6809e-03,  1.0978e-02,  2.6809e-03,\n",
      "          6.3430e-03,  2.6809e-03,  1.0978e-02,  2.6809e-03,  6.3430e-03,\n",
      "          2.6809e-03,  1.0978e-02,  6.3430e-03,  2.6809e-03,  6.3430e-03],\n",
      "        [ 9.5548e-03,  4.9678e-03,  1.5923e-02,  4.9678e-03,  9.5548e-03,\n",
      "          4.9678e-03,  1.5923e-02,  4.9678e-03,  9.5548e-03,  4.9678e-03,\n",
      "          1.5923e-02,  4.9678e-03,  9.5548e-03,  4.9678e-03,  1.5923e-02,\n",
      "          4.9678e-03,  9.5548e-03,  4.9678e-03,  1.5923e-02,  4.9678e-03,\n",
      "          9.5548e-03,  4.9678e-03,  1.5923e-02,  4.9678e-03,  9.5548e-03,\n",
      "          4.9678e-03,  1.5923e-02,  4.9678e-03,  9.5548e-03,  4.9678e-03,\n",
      "          1.5923e-02,  4.9678e-03,  9.5548e-03,  4.9678e-03,  1.5923e-02,\n",
      "          4.9678e-03,  9.5548e-03,  4.9678e-03,  1.5923e-02,  4.9678e-03,\n",
      "          9.5548e-03,  4.9678e-03,  1.5923e-02,  4.9678e-03,  9.5548e-03,\n",
      "          4.9678e-03,  1.5923e-02,  9.5548e-03,  4.9678e-03,  9.5548e-03],\n",
      "        [ 1.4665e-02,  8.7029e-03,  2.5289e-02,  8.7029e-03,  1.4665e-02,\n",
      "          8.7029e-03,  2.5289e-02,  8.7029e-03,  1.4665e-02,  8.7029e-03,\n",
      "          2.5289e-02,  8.7029e-03,  1.4665e-02,  8.7029e-03,  2.5289e-02,\n",
      "          8.7029e-03,  1.4665e-02,  8.7029e-03,  2.5289e-02,  8.7029e-03,\n",
      "          1.4665e-02,  8.7029e-03,  2.5289e-02,  8.7029e-03,  1.4665e-02,\n",
      "          8.7029e-03,  2.5289e-02,  8.7029e-03,  1.4665e-02,  8.7029e-03,\n",
      "          2.5289e-02,  8.7029e-03,  1.4665e-02,  8.7029e-03,  2.5289e-02,\n",
      "          8.7029e-03,  1.4665e-02,  8.7029e-03,  2.5289e-02,  8.7029e-03,\n",
      "          1.4665e-02,  8.7029e-03,  2.5289e-02,  8.7029e-03,  1.4665e-02,\n",
      "          8.7029e-03,  2.5289e-02,  1.4665e-02,  8.7029e-03,  1.4665e-02],\n",
      "        [ 2.1143e-02,  1.2698e-02,  4.1767e-02,  1.2698e-02,  2.1143e-02,\n",
      "          1.2698e-02,  4.1767e-02,  1.2698e-02,  2.1143e-02,  1.2698e-02,\n",
      "          4.1767e-02,  1.2698e-02,  2.1143e-02,  1.2698e-02,  4.1767e-02,\n",
      "          1.2698e-02,  2.1143e-02,  1.2698e-02,  4.1767e-02,  1.2698e-02,\n",
      "          2.1143e-02,  1.2698e-02,  4.1767e-02,  1.2698e-02,  2.1143e-02,\n",
      "          1.2698e-02,  4.1767e-02,  1.2698e-02,  2.1143e-02,  1.2698e-02,\n",
      "          4.1767e-02,  1.2698e-02,  2.1143e-02,  1.2698e-02,  4.1767e-02,\n",
      "          1.2698e-02,  2.1143e-02,  1.2698e-02,  4.1767e-02,  1.2698e-02,\n",
      "          2.1143e-02,  1.2698e-02,  4.1767e-02,  1.2698e-02,  2.1143e-02,\n",
      "          1.2698e-02,  4.1767e-02,  2.1143e-02,  1.2698e-02,  2.1143e-02]],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(h_prime.shape)\n",
    "print(h_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06f007ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 1, 2, 3, 4, 2, 3, 4, 5, 3, 4, 5, 6, 4, 5, 6, 7, 5, 6, 7, 8, 6, 7, 8, 9, 7, 8, 9, 10, 8, 9, 10, 11, 9, 10, 11, 12, 10, 11, 12, 13, 11, 12, 13, 12, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "# source node\n",
    "# index represent the edge\n",
    "target_nodes = edge_indices[0][1].tolist()\n",
    "print(edge_indices[0][1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf6a8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {}\n",
    "for i in set(target_nodes):\n",
    "    sample[i] = []\n",
    "\n",
    "for target_node, idx in zip(target_nodes, range(len(target_nodes))):\n",
    "    sample[target_node].append([idx, h_prime[target_node][idx].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c51f7cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [[0, -0.026776738464832306]],\n",
       " 1: [[1, -0.018013518303632736], [4, -0.013125997968018055]],\n",
       " 2: [[2, -0.0007867645472288132],\n",
       "  [5, -0.008494682610034943],\n",
       "  [8, -0.005044706631451845]],\n",
       " 3: [[3, -0.004851858131587505],\n",
       "  [6, 0.002407621592283249],\n",
       "  [9, -0.004851858131587505],\n",
       "  [12, -0.0015193652361631393]],\n",
       " 4: [[7, -0.003075276967138052],\n",
       "  [10, 0.0039338660426437855],\n",
       "  [13, -0.003075276967138052],\n",
       "  [16, 0.00017807260155677795]],\n",
       " 5: [[11, -0.0020052697509527206],\n",
       "  [14, 0.005349091254174709],\n",
       "  [17, -0.0020052697509527206],\n",
       "  [20, 0.0015172697603702545]],\n",
       " 6: [[15, 0.000844285823404789],\n",
       "  [18, 0.007752683945000172],\n",
       "  [21, 0.000844285823404789],\n",
       "  [24, 0.0041534025222063065]],\n",
       " 7: [[19, -8.43442976474762e-05],\n",
       "  [22, 0.007420366629958153],\n",
       "  [25, -8.43442976474762e-05],\n",
       "  [28, 0.0035506240092217922]],\n",
       " 8: [[23, 0.0015317141078412533],\n",
       "  [26, 0.008486921899020672],\n",
       "  [29, 0.0015317141078412533],\n",
       "  [32, 0.004794551990926266]],\n",
       " 9: [[27, 0.001454437617212534],\n",
       "  [30, 0.008868573233485222],\n",
       "  [33, 0.001454437617212534],\n",
       "  [36, 0.004792647436261177]],\n",
       " 10: [[31, 0.00268090283498168],\n",
       "  [34, 0.010978265665471554],\n",
       "  [37, 0.00268090283498168],\n",
       "  [40, 0.006343000568449497]],\n",
       " 11: [[35, 0.004967778921127319],\n",
       "  [38, 0.015923060476779938],\n",
       "  [41, 0.004967778921127319],\n",
       "  [44, 0.00955477636307478]],\n",
       " 12: [[39, 0.008702944032847881],\n",
       "  [42, 0.025288773700594902],\n",
       "  [45, 0.008702944032847881],\n",
       "  [47, 0.014665446244180202]],\n",
       " 13: [[43, 0.01269840169698],\n",
       "  [46, 0.041766852140426636],\n",
       "  [48, 0.01269840169698],\n",
       "  [49, 0.02114330232143402]]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d617248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  4,  4,\n",
       "          4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,\n",
       "          9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 12, 12, 13],\n",
       "        [ 0,  1,  2,  3,  1,  2,  3,  4,  2,  3,  4,  5,  3,  4,  5,  6,  4,  5,\n",
       "          6,  7,  5,  6,  7,  8,  6,  7,  8,  9,  7,  8,  9, 10,  8,  9, 10, 11,\n",
       "          9, 10, 11, 12, 10, 11, 12, 13, 11, 12, 13, 12, 13, 13]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd9ad0",
   "metadata": {},
   "source": [
    "The GATLayerWithExpEdgeType class does not work!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d8a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_in_features = 300\n",
    "# num_out_features = 300\n",
    "# num_heads = 4\n",
    "# num_edge_types = 3\n",
    "\n",
    "# gat_exp_layer = GATLayerWithExpEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)\n",
    "\n",
    "# # torch.manual_seed(42)\n",
    "# i = 0\n",
    "# attention_scores, attention_coefficients = gat_exp_layer((updated_representations[i], edge_indices[i]), edge_types[i])\n",
    "# print(f\"dialogue_representation[{i}] shape:\", updated_representations[i].shape)\n",
    "# print(\"Attention coef shape:\", attention_coefficients.shape)\n",
    "# print(\"Attention score shape:\", attention_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2ba5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(attention_coef[:10].shape)\n",
    "# print(attention_score[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f139c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_coef[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bc8245a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Frequent Edges for Node 0: [25, 39, 23, 5, 45]\n",
      "Top 5 Frequent Edges for Node 1: [43, 3, 35, 17, 33]\n",
      "Top 5 Frequent Edges for Node 2: [3, 17, 15, 13, 11]\n",
      "Top 5 Frequent Edges for Node 3: [11, 25, 27, 33, 13]\n",
      "Top 5 Frequent Edges for Node 4: [1, 3, 13, 33, 41]\n",
      "Top 5 Frequent Edges for Node 5: [11, 45, 3, 19, 1]\n",
      "Top 5 Frequent Edges for Node 6: [31, 39, 1, 27, 11]\n",
      "Top 5 Frequent Edges for Node 7: [45, 5, 21, 43, 48]\n",
      "Top 5 Frequent Edges for Node 8: [37, 13, 15, 11, 48]\n",
      "Top 5 Frequent Edges for Node 9: [46, 38, 34, 30, 2]\n",
      "Top 5 Frequent Edges for Node 10: [14, 18, 42, 22, 46]\n",
      "Top 5 Frequent Edges for Node 11: [34, 46, 14, 26, 2]\n",
      "Top 5 Frequent Edges for Node 12: [26, 2, 46, 14, 42]\n",
      "Top 5 Frequent Edges for Node 13: [46, 22, 34, 10, 26]\n"
     ]
    }
   ],
   "source": [
    "# Assuming input_tensor is your tensor of shape (14, 150, 50)\n",
    "# input_tensor = torch.rand((14, 150, 50))\n",
    "input_tensor = attention_coef\n",
    "# Set the value of k for top-k\n",
    "k = 5\n",
    "\n",
    "# Initialize a list to store top-k frequent edges for each node\n",
    "top_k_frequent_edges_per_node = []\n",
    "\n",
    "# Loop over each node\n",
    "for node_index in range(input_tensor.shape[0]):\n",
    "    # Extract the edges for the current node\n",
    "    node_edges = input_tensor[node_index]\n",
    "\n",
    "    # Flatten the tensor to have shape (150, 50)\n",
    "    flat_tensor = node_edges.view(-1)\n",
    "\n",
    "    # Find the indices of the top-k influential edges for the current node\n",
    "    top_k_indices = torch.argsort(flat_tensor, descending=True)[:k]\n",
    "\n",
    "    # Ensure the top-k indices are within the correct range (0-50)\n",
    "    top_k_indices = top_k_indices % 50\n",
    "\n",
    "    # Flatten the top-k indices and convert them to a list\n",
    "    top_k_flat_list = top_k_indices.view(-1).tolist()\n",
    "\n",
    "    # Count the occurrences of each edge index\n",
    "    edge_counts = Counter(top_k_flat_list)\n",
    "\n",
    "    # Find the top-k most frequent edges for the current node\n",
    "    top_k_frequent_edges = [edge for edge, count in edge_counts.most_common(k)]\n",
    "\n",
    "    # Append the result to the list\n",
    "    top_k_frequent_edges_per_node.append(top_k_frequent_edges)\n",
    "\n",
    "# Print the results\n",
    "for node_index, edges in enumerate(top_k_frequent_edges_per_node):\n",
    "    print(\"Top {} Frequent Edges for Node {}: {}\".format(k, node_index, edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d365dc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:  tensor([3, 5, 5, 1, 7, 3, 4, 0, 3, 1, 5, 4, 3, 0, 0, 2, 2, 6, 1, 7, 3, 3, 7, 6,\n",
      "        5, 5, 6, 5, 2, 3])\n",
      "v:  tensor([6, 3, 7, 0, 2, 4, 2, 6, 4, 0, 6, 1, 3, 0, 3, 5, 1, 1, 0, 1, 4, 1, 3, 3,\n",
      "        6, 3, 6, 3, 4, 7])\n",
      "Graph(num_nodes=8, num_edges=30,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "node_feats.shape:  torch.Size([8, 20])\n",
      "edge_feats.shape:  torch.Size([30, 12])\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 8\n",
    "num_edges = 30\n",
    "#define connections\n",
    "u = torch.randint(num_nodes, (num_edges,))\n",
    "print(\"u: \", u)\n",
    "v = torch.randint(num_nodes, (num_edges,)) \n",
    "print(\"v: \", v)\n",
    "graph = dgl.graph((u,v))    \n",
    "print(graph)\n",
    "node_feats = torch.rand((num_nodes, 20)) \n",
    "print(\"node_feats.shape: \", node_feats.shape)\n",
    "edge_feats = torch.rand((num_edges, 12))\n",
    "print(\"edge_feats.shape: \", edge_feats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24e188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fabbe5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use as regular torch/dgl layer work similar as GATConv from dgl library\n",
    "# egat = EGATConv(in_node_feats=num_node_feats,\n",
    "#                 in_edge_feats=num_edge_feats,\n",
    "#                 out_node_feats=10,\n",
    "#                 out_edge_feats=10,\n",
    "#                 num_heads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3799c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_node_feats, new_edge_feats = egat(graph, node_feats, edge_feats)\n",
    "#new_node_feats.shape = (*, num_heads, out_node_feats)\n",
    "#new_eode_feats.shape = (*, num_heads, out_edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1355e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a76b43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "graph = dgl.graph((edge_indices[i][0],edge_indices[i][1]))    \n",
    "edge_feats = get_ohe(edge_types[i])\n",
    "egat = EGATConv(in_node_feats=300,\n",
    "                in_edge_feats=3,\n",
    "                out_node_feats=300,\n",
    "                out_edge_feats=3,\n",
    "                num_heads=4)\n",
    "new_node_feats, new_edge_feats = egat(graph, updated_representations[i], edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c99dfa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 4, 3]) torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(new_edge_feats.shape, new_edge_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2656832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2043e-03,  1.3946e-01,  1.3355e-01],\n",
       "         [-1.1724e-02, -1.4797e-02, -4.4118e-04],\n",
       "         [-8.2909e-03, -3.0248e-03, -3.6196e-03],\n",
       "         [ 7.0107e-02, -4.0372e-03, -2.1249e-03]],\n",
       "\n",
       "        [[ 3.1490e-01,  5.6027e-01,  3.6082e-01],\n",
       "         [-4.0730e-03, -6.6862e-03, -7.2717e-04],\n",
       "         [ 7.3918e-01, -2.0522e-03, -4.5301e-03],\n",
       "         [ 9.0749e-01, -4.2760e-03, -3.3131e-03]],\n",
       "\n",
       "        [[ 3.8424e-01, -2.7835e-03,  9.7298e-02],\n",
       "         [ 5.8915e-01, -7.6024e-03,  2.3478e-01],\n",
       "         [ 4.3970e-01,  1.5603e-01, -1.1243e-03],\n",
       "         [ 7.3272e-01,  3.9763e-01, -4.7669e-03]],\n",
       "\n",
       "        [[ 3.9363e-01,  5.2008e-01,  2.9877e-01],\n",
       "         [-4.6900e-03, -5.4016e-03, -5.5663e-04],\n",
       "         [ 8.6497e-01, -2.4066e-03, -4.9672e-03],\n",
       "         [ 8.2114e-01, -1.8289e-03, -5.7593e-03]],\n",
       "\n",
       "        [[-2.7848e-03, -4.0790e-05,  5.7089e-02],\n",
       "         [-1.3146e-02, -1.3630e-02, -3.3268e-03],\n",
       "         [-7.6839e-03, -2.8498e-03, -6.6084e-04],\n",
       "         [-3.3714e-05, -2.7149e-04, -1.5495e-03]],\n",
       "\n",
       "        [[ 2.2539e-01,  4.8329e-01,  3.7881e-01],\n",
       "         [-5.6747e-03, -6.0238e-03, -4.0860e-03],\n",
       "         [ 6.7816e-01, -1.2607e-03, -3.1134e-03],\n",
       "         [ 9.3315e-01, -1.8508e-03, -1.1152e-03]],\n",
       "\n",
       "        [[ 2.6410e-01, -3.5403e-03,  1.6379e-01],\n",
       "         [ 4.2674e-01, -7.3926e-03, -7.1066e-04],\n",
       "         [ 3.1516e-01,  2.6464e-01,  3.8388e-02],\n",
       "         [ 7.9051e-01,  5.5909e-01, -1.8968e-03]],\n",
       "\n",
       "        [[ 2.7658e-01,  4.7804e-01,  3.7056e-01],\n",
       "         [-6.0700e-03, -5.2893e-03, -3.6635e-03],\n",
       "         [ 7.2552e-01, -1.3931e-03, -3.4355e-03],\n",
       "         [ 8.9061e-01, -8.9766e-04, -2.4774e-03]],\n",
       "\n",
       "        [[-2.8891e-03, -6.8161e-04,  3.5481e-02],\n",
       "         [-1.4245e-02, -1.2275e-02, -4.5252e-03],\n",
       "         [-6.9193e-03, -2.9321e-03, -8.7232e-04],\n",
       "         [-8.1813e-04,  1.1726e-01, -1.4460e-03]],\n",
       "\n",
       "        [[ 1.8433e-01,  4.2050e-01,  4.0571e-01],\n",
       "         [-6.7958e-03, -5.1213e-03, -4.9841e-03],\n",
       "         [ 6.9111e-01, -1.0485e-03, -3.2334e-03],\n",
       "         [ 8.8684e-01, -1.2173e-03, -3.3952e-04]],\n",
       "\n",
       "        [[ 2.2612e-01, -3.8319e-03,  1.9598e-01],\n",
       "         [ 3.3905e-01, -6.5876e-03, -1.6571e-03],\n",
       "         [ 3.1321e-01,  2.7861e-01,  2.8741e-02],\n",
       "         [ 7.5588e-01,  5.5410e-01, -7.0933e-04]],\n",
       "\n",
       "        [[ 2.1740e-01,  4.3162e-01,  3.9475e-01],\n",
       "         [-7.0936e-03, -4.7551e-03, -4.6195e-03],\n",
       "         [ 7.0715e-01, -1.1144e-03, -3.5519e-03],\n",
       "         [ 8.5982e-01, -8.9212e-04, -1.1100e-03]],\n",
       "\n",
       "        [[-2.8849e-03, -1.1849e-03,  4.4803e-02],\n",
       "         [-1.5067e-02, -1.1364e-02, -4.8902e-03],\n",
       "         [-6.7902e-03, -3.0258e-03, -1.1042e-03],\n",
       "         [-1.4454e-03,  2.0228e-01, -1.3828e-03]],\n",
       "\n",
       "        [[ 1.8784e-01,  4.0381e-01,  4.2033e-01],\n",
       "         [-7.3739e-03, -4.3079e-03, -5.3974e-03],\n",
       "         [ 6.8911e-01, -1.2147e-03, -3.4417e-03],\n",
       "         [ 8.3580e-01, -1.0504e-03,  1.3545e-02]],\n",
       "\n",
       "        [[ 2.0844e-01, -4.1714e-03,  2.0260e-01],\n",
       "         [ 2.6657e-01, -6.0450e-03, -2.0800e-03],\n",
       "         [ 2.9479e-01,  2.7588e-01,  5.9081e-03],\n",
       "         [ 7.0867e-01,  5.7634e-01, -5.4513e-05]],\n",
       "\n",
       "        [[ 2.1417e-01,  4.2671e-01,  4.1751e-01],\n",
       "         [-7.7283e-03, -4.2373e-03, -5.1706e-03],\n",
       "         [ 6.9472e-01, -1.4408e-03, -3.7393e-03],\n",
       "         [ 8.3561e-01, -1.0956e-03, -2.4579e-04]],\n",
       "\n",
       "        [[-2.7252e-03, -1.0856e-03,  3.5086e-02],\n",
       "         [-1.5593e-02, -1.0884e-02, -4.9056e-03],\n",
       "         [-6.6958e-03, -3.1879e-03, -1.1873e-03],\n",
       "         [-1.9442e-03,  2.1047e-01, -1.4038e-03]],\n",
       "\n",
       "        [[ 1.8261e-01,  3.9648e-01,  4.0261e-01],\n",
       "         [-8.0459e-03, -4.0992e-03, -5.4224e-03],\n",
       "         [ 6.8213e-01, -1.2378e-03, -3.5449e-03],\n",
       "         [ 7.8975e-01, -9.1308e-04,  2.9437e-02]],\n",
       "\n",
       "        [[ 2.1766e-01, -3.9543e-03,  2.0102e-01],\n",
       "         [ 2.0839e-01, -5.8611e-03, -2.2332e-03],\n",
       "         [ 2.9380e-01,  2.4367e-01, -3.1112e-06],\n",
       "         [ 6.8563e-01,  5.4748e-01,  3.1383e-02]],\n",
       "\n",
       "        [[ 2.0362e-01,  4.0171e-01,  4.1669e-01],\n",
       "         [-8.1266e-03, -4.1675e-03, -5.3086e-03],\n",
       "         [ 6.9765e-01, -1.2937e-03, -3.6815e-03],\n",
       "         [ 7.8543e-01, -6.9799e-04,  3.3510e-02]],\n",
       "\n",
       "        [[-2.6227e-03, -1.1222e-03,  1.3967e-02],\n",
       "         [-1.6015e-02, -1.0617e-02, -4.9425e-03],\n",
       "         [-6.7369e-03, -3.0672e-03, -1.4307e-03],\n",
       "         [-2.3806e-03,  2.3952e-01, -1.4435e-03]],\n",
       "\n",
       "        [[ 2.0731e-01,  4.2186e-01,  3.9763e-01],\n",
       "         [-8.3781e-03, -3.8566e-03, -5.5875e-03],\n",
       "         [ 6.8401e-01, -1.4163e-03, -3.7472e-03],\n",
       "         [ 7.6911e-01, -1.0485e-03,  4.6409e-02]],\n",
       "\n",
       "        [[ 2.2259e-01, -4.1677e-03,  1.9680e-01],\n",
       "         [ 1.9352e-01, -5.7326e-03, -2.3830e-03],\n",
       "         [ 2.9959e-01,  2.7275e-01, -8.5471e-05],\n",
       "         [ 6.3785e-01,  6.0256e-01,  6.9609e-02]],\n",
       "\n",
       "        [[ 2.3868e-01,  4.1328e-01,  4.1558e-01],\n",
       "         [-8.4501e-03, -3.8346e-03, -5.2820e-03],\n",
       "         [ 7.1093e-01, -1.3322e-03, -3.7656e-03],\n",
       "         [ 7.6236e-01, -8.3659e-04,  6.2476e-02]],\n",
       "\n",
       "        [[-2.3171e-03, -7.1311e-04, -7.4980e-05],\n",
       "         [-1.6360e-02, -1.0761e-02, -4.8952e-03],\n",
       "         [-6.6266e-03, -3.5469e-03, -1.1946e-03],\n",
       "         [-2.4351e-03,  2.1347e-01, -1.5237e-03]],\n",
       "\n",
       "        [[ 2.1810e-01,  4.1606e-01,  3.7693e-01],\n",
       "         [-8.5394e-03, -4.1147e-03, -5.5249e-03],\n",
       "         [ 6.9896e-01, -1.4267e-03, -3.3912e-03],\n",
       "         [ 7.3654e-01, -6.2273e-04,  5.9636e-02]],\n",
       "\n",
       "        [[ 2.6351e-01, -3.8966e-03,  1.7921e-01],\n",
       "         [ 1.5990e-01, -5.7863e-03, -2.1441e-03],\n",
       "         [ 3.2203e-01,  2.3877e-01,  2.6886e-02],\n",
       "         [ 6.2998e-01,  5.7619e-01,  7.3576e-02]],\n",
       "\n",
       "        [[ 2.4589e-01,  4.1330e-01,  4.0771e-01],\n",
       "         [-8.9052e-03, -4.1250e-03, -5.1438e-03],\n",
       "         [ 7.3435e-01, -9.8382e-04, -3.5533e-03],\n",
       "         [ 7.2522e-01, -6.8405e-04,  9.8644e-02]],\n",
       "\n",
       "        [[-2.4283e-03, -9.0931e-04,  2.9483e-02],\n",
       "         [-1.6277e-02, -1.0771e-02, -4.9480e-03],\n",
       "         [-6.5030e-03, -3.3131e-03, -1.3246e-03],\n",
       "         [-2.6595e-03,  2.6340e-01, -1.4494e-03]],\n",
       "\n",
       "        [[ 2.3711e-01,  4.2934e-01,  4.1701e-01],\n",
       "         [-8.6314e-03, -3.9205e-03, -5.4014e-03],\n",
       "         [ 7.1881e-01, -1.5221e-03, -3.5229e-03],\n",
       "         [ 7.3879e-01, -8.1288e-04,  5.7813e-02]],\n",
       "\n",
       "        [[ 2.4882e-01, -4.0346e-03,  2.2902e-01],\n",
       "         [ 1.3882e-01, -5.8288e-03, -2.1212e-03],\n",
       "         [ 3.4286e-01,  2.9804e-01, -4.8800e-06],\n",
       "         [ 6.0297e-01,  5.9880e-01,  1.0395e-01]],\n",
       "\n",
       "        [[ 2.8474e-01,  4.0526e-01,  4.7101e-01],\n",
       "         [-9.1719e-03, -4.4124e-03, -4.7239e-03],\n",
       "         [ 7.5784e-01, -6.9824e-04, -4.1530e-03],\n",
       "         [ 6.9862e-01, -5.9907e-04,  1.6073e-01]],\n",
       "\n",
       "        [[-2.1378e-03, -4.6556e-04,  5.6301e-02],\n",
       "         [-1.6380e-02, -1.0381e-02, -4.5777e-03],\n",
       "         [-6.2726e-03, -3.6408e-03, -1.1503e-03],\n",
       "         [-2.5587e-03,  2.3209e-01, -1.7061e-03]],\n",
       "\n",
       "        [[ 2.3245e-01,  4.4663e-01,  4.5356e-01],\n",
       "         [-8.8537e-03, -3.7672e-03, -5.1318e-03],\n",
       "         [ 7.4283e-01, -1.1617e-03, -3.4906e-03],\n",
       "         [ 7.1960e-01, -7.0975e-04,  6.4346e-02]],\n",
       "\n",
       "        [[ 2.9770e-01, -3.8041e-03,  2.7906e-01],\n",
       "         [ 1.1100e-01, -5.9204e-03, -1.4547e-03],\n",
       "         [ 3.6955e-01,  3.0337e-01, -2.9864e-04],\n",
       "         [ 5.8419e-01,  5.9501e-01,  1.4220e-01]],\n",
       "\n",
       "        [[ 3.5383e-01,  4.2178e-01,  5.2855e-01],\n",
       "         [-9.3301e-03, -4.5569e-03, -3.9026e-03],\n",
       "         [ 8.2229e-01, -2.7998e-04, -4.2974e-03],\n",
       "         [ 6.3590e-01, -5.3121e-04,  2.5623e-01]],\n",
       "\n",
       "        [[-2.4004e-03, -3.4412e-04,  1.2000e-01],\n",
       "         [-1.6520e-02, -1.0473e-02, -4.6859e-03],\n",
       "         [-6.0836e-03, -3.0710e-03, -1.2463e-03],\n",
       "         [-2.6791e-03,  2.4671e-01, -1.3911e-03]],\n",
       "\n",
       "        [[ 2.5973e-01,  4.6454e-01,  5.3075e-01],\n",
       "         [-9.0488e-03, -4.1046e-03, -4.8430e-03],\n",
       "         [ 7.6438e-01, -8.9906e-04, -3.9126e-03],\n",
       "         [ 7.0798e-01, -7.0464e-04,  1.2755e-01]],\n",
       "\n",
       "        [[ 3.4519e-01, -3.6904e-03,  3.6374e-01],\n",
       "         [ 1.0348e-01, -6.3107e-03, -1.0111e-03],\n",
       "         [ 4.2886e-01,  3.6613e-01, -5.7125e-04],\n",
       "         [ 5.2862e-01,  6.0610e-01,  2.6266e-01]],\n",
       "\n",
       "        [[ 4.9067e-01,  4.1290e-01,  6.3328e-01],\n",
       "         [-9.1821e-03, -5.1360e-03, -2.9236e-03],\n",
       "         [ 8.6319e-01, -2.2229e-04, -4.9755e-03],\n",
       "         [ 5.2791e-01, -1.1206e-03,  5.0221e-01]],\n",
       "\n",
       "        [[-1.9526e-03, -1.5636e-04,  1.8641e-01],\n",
       "         [-1.6645e-02, -1.0724e-02, -4.3477e-03],\n",
       "         [-5.9181e-03, -2.9671e-03, -1.7718e-03],\n",
       "         [-2.5763e-03,  2.3962e-01, -1.0539e-03]],\n",
       "\n",
       "        [[ 3.2472e-01,  4.7678e-01,  6.0466e-01],\n",
       "         [-9.0546e-03, -4.4080e-03, -4.3500e-03],\n",
       "         [ 8.1870e-01, -4.3016e-04, -4.2888e-03],\n",
       "         [ 6.7432e-01, -6.6967e-04,  2.1853e-01]],\n",
       "\n",
       "        [[ 4.9953e-01, -3.7704e-03,  4.5771e-01],\n",
       "         [ 1.2522e-01, -6.8028e-03,  1.7261e-03],\n",
       "         [ 4.6476e-01,  3.5603e-01, -1.3529e-03],\n",
       "         [ 4.4253e-01,  5.3956e-01,  4.7916e-01]],\n",
       "\n",
       "        [[ 7.6161e-01,  2.6287e-01,  7.5893e-01],\n",
       "         [-8.6060e-03, -5.5904e-03, -2.9490e-03],\n",
       "         [ 7.5399e-01, -2.6824e-03, -5.4006e-03],\n",
       "         [ 5.9150e-01, -6.8282e-04,  7.3098e-01]],\n",
       "\n",
       "        [[-1.0640e-03, -2.2988e-04,  2.9893e-01],\n",
       "         [-1.6999e-02, -1.0763e-02, -3.9093e-03],\n",
       "         [-5.5112e-03, -2.3048e-03, -2.1193e-03],\n",
       "         [-2.9126e-03,  2.4924e-01, -2.2155e-04]],\n",
       "\n",
       "        [[ 5.0293e-01,  4.4918e-01,  7.3723e-01],\n",
       "         [-9.1850e-03, -4.6357e-03, -3.3762e-03],\n",
       "         [ 8.4098e-01, -3.3784e-04, -5.0418e-03],\n",
       "         [ 5.8826e-01, -1.2739e-03,  4.2728e-01]],\n",
       "\n",
       "        [[ 7.9433e-01, -5.4666e-03,  6.2196e-01],\n",
       "         [ 1.4805e-01, -6.9928e-03, -6.2608e-05],\n",
       "         [ 3.4195e-01,  1.2935e-01, -1.7493e-03],\n",
       "         [ 5.0616e-01,  5.8946e-01,  7.0018e-01]],\n",
       "\n",
       "        [[ 1.0787e-01, -6.4320e-04,  3.5846e-01],\n",
       "         [-1.6894e-02, -1.0325e-02, -2.4598e-03],\n",
       "         [-5.7052e-03, -2.1799e-03, -2.3359e-03],\n",
       "         [-4.4100e-03,  1.8045e-01,  1.4401e-01]],\n",
       "\n",
       "        [[ 8.3381e-01,  2.6582e-01,  8.2845e-01],\n",
       "         [-8.7213e-03, -4.1599e-03, -2.9805e-03],\n",
       "         [ 6.7649e-01, -2.5720e-03, -4.9017e-03],\n",
       "         [ 5.8820e-01, -8.5855e-04,  6.0571e-01]],\n",
       "\n",
       "        [[ 4.0549e-01, -2.9130e-03,  5.6585e-01],\n",
       "         [-1.6016e-02, -9.0963e-03, -2.4456e-03],\n",
       "         [-6.1162e-03, -2.4000e-03, -3.0088e-03],\n",
       "         [-3.0151e-03,  2.5250e-01,  2.9874e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_edge_feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "119px",
    "width": "235px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
