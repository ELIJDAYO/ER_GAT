{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3cd42",
   "metadata": {},
   "source": [
    "Put libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a88b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "553916bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch, os, pickle, sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# from GAT import GAT\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38a0066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Path: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\\utils\\constans.py\n",
      "Project Directory: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\n"
     ]
    }
   ],
   "source": [
    "script_path = os.path.abspath(\"utils\\\\constans.py\")  # Replace __file__ with the path to your script if in a notebook\n",
    "\n",
    "# Determine the project directory by moving up two levels (adjust as needed)\n",
    "project_directory = os.path.dirname(os.path.dirname(script_path))\n",
    "\n",
    "print(\"Script Path:\", script_path)\n",
    "print(\"Project Directory:\", project_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc1152",
   "metadata": {},
   "source": [
    "Code related to GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ecf3858",
   "metadata": {
    "code_folding": [
     1,
     6,
     17
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class DialogueGraphDataLoader(DataLoader):\n",
    "    def __init__(self, node_features_list, edge_index_list, batch_size=1, shuffle=False):\n",
    "        graph_dataset = DialogueGraphDataset(node_features_list, edge_index_list)\n",
    "        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=dialogue_graph_collate_fn)\n",
    "\n",
    "class DialogueGraphDataset(Dataset):\n",
    "    def __init__(self, node_features_list, edge_index_list):\n",
    "        self.node_features_list = node_features_list\n",
    "        self.edge_index_list = edge_index_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.edge_index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.node_features_list[idx], self.edge_index_list[idx]\n",
    "\n",
    "def dialogue_graph_collate_fn(batch):\n",
    "    node_features_list, edge_index_list = zip(*batch)\n",
    "    \n",
    "    node_features_list_combined = []\n",
    "    num_nodes_seen = 0\n",
    "\n",
    "    for node_features, edge_index in zip(node_features_list, edge_index_list):\n",
    "        # Assuming node_features is a tuple (text_embeddings, speakers_list)\n",
    "        text_embeddings, speakers_list = node_features\n",
    "        combined_features = (text_embeddings, speakers_list)\n",
    "\n",
    "        node_features_list_combined.append(combined_features)\n",
    "\n",
    "        # Translate the range of edge_index\n",
    "        edge_index_list.append(edge_index + num_nodes_seen)\n",
    "        num_nodes_seen += len(text_embeddings)\n",
    "\n",
    "    # Merge the dialogue graphs into a single graph with multiple connected components\n",
    "    node_features_combined = [torch.cat(features, 1) for features in zip(*node_features_list_combined)]\n",
    "    edge_index = torch.cat(edge_index_list, 1)\n",
    "\n",
    "    return node_features_combined, edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75f4cb1c",
   "metadata": {
    "code_folding": [
     35
    ]
   },
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "\n",
    "        # Edge type embeddings\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "\n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "\n",
    "        # Transpose dimensions of h_linear to match edge_type_embedding's shape\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "         # Weighted sum of neighbor node representations\n",
    "        print(\"attention_coefficients.unsqueeze(-1).shape: \",attention_coefficients.shape)\n",
    "        print(\"h_linear.permute(0, 2, 1).squeeze(-1).shape: \",h_linear.shape)\n",
    "\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).sum(dim=1)\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e3ed3",
   "metadata": {},
   "source": [
    "<h3>Methods definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53e5417",
   "metadata": {
    "code_folding": [
     0,
     20,
     42,
     58
    ]
   },
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    # Initialize an empty list to store pairs\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "#     node_pairs_dict = {}\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 3\n",
    "        while(val >= 0):\n",
    "            target_idx = i-val\n",
    "#                 print(target_idx)\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "#                 node_pairs_dict[i] = target_idx\n",
    "            val = val-1\n",
    "    \n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "\n",
    "    # Iterate through pairs of nodes\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "\n",
    "#         # Add source node to target node's neighbors\n",
    "#         if target_node not in adjacency_list_dict:\n",
    "#             adjacency_list_dict[target_node] = [source_node]\n",
    "#         else:\n",
    "#             adjacency_list_dict[target_node].append(source_node)\n",
    "\n",
    "        # Add target node to source node's neighbors\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "\n",
    "    return adjacency_list_dict\n",
    "# print(ranges[:1])\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        \n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    \n",
    "    for i in range(dialogs_len): #2140 dialogs\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "#         print(i, \" th dialogue\")\n",
    "#         print(i, dialog_speakers)\n",
    "        for j in range(dialog_len): #num utterances\n",
    "            src_node = dialog_nodes_pairs[j] # j = key = src node\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k] # k = value = targ node\n",
    "                # edge_types = torch.tensor([0, 1, 2]) \n",
    "                # 0: cur-self, 1: past-self, 2: past-other/past-inter\n",
    "                                \n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "#                     print(\"This is 0 \", node_i_idx, node_j_idx)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "#                         print(\"This is 1 \", node_i_idx, node_j_idx)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "#                         print(\"This is 2 \", node_i_idx, node_j_idx)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))  \n",
    "                    \n",
    "    return whole_edge_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a199d8f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print(edge_indices[0][0][3])\n",
    "# len(edge_indices[0].keys())\n",
    "# list(encoded_speaker_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0afbacda",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# assume this is working\n",
    "# edge_indices = get_all_adjacency_list(ranges)\n",
    "# edge_types = get_all_edge_type_list(edge_indices, encoded_speaker_list)\n",
    "# edge_indices = get_all_adjacency_list(ranges, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97f2ae80",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print((edge_types[:10]))\n",
    "# edge_indices[:10]\n",
    "# (updated_representations[0].shape)\n",
    "# edge_indices[0]\n",
    "# edge_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20cb34a8",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder.pkl\")\n",
    "encoded_speaker_list = []\n",
    "if checkFile is False:\n",
    "    print(\"Run first the prototype_context_encoder to generate this file\")\n",
    "else:\n",
    "    file = open('data/dump/speaker_encoder.pkl', \"rb\")\n",
    "    encoded_speaker_list, ranges = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb6bc790",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# need update\n",
    "# checkFile = os.path.isfile(\"data/dump/all_adjacency_list.pkl\")\n",
    "# adjacency_list = []\n",
    "# if checkFile is False:\n",
    "#     adjacency_list = get_all_adjacency_list(ranges)\n",
    "# else:\n",
    "#     file = open('data/dump/all_adjacency_list.pkl', \"rb\")\n",
    "#     adjacency_list = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64d5fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacency_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8072d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f02158e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "file_path = 'embed/updated_representation_list.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    updated_representations = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "293f6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(updated_representations[0].shape)\n",
    "# print(updated_representations[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48400c3a",
   "metadata": {},
   "source": [
    "<h3> Making Progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea3af5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = get_all_adjacency_list(ranges)\n",
    "edge_types = get_all_edge_type_list(edge_indices, encoded_speaker_list)\n",
    "edge_indices = get_all_adjacency_list(ranges, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50517bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2,\n",
       "        1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2,\n",
       "        1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f42f4561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 300])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_representations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "658dd100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 75])\n"
     ]
    }
   ],
   "source": [
    "print(updated_representations[0].view(-1, 75).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ed5fed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_coefficients.unsqueeze(-1).shape:  torch.Size([14, 150, 50])\n",
      "h_linear.permute(0, 2, 1).squeeze(-1).shape:  torch.Size([14, 150, 4])\n",
      "dialogue_representation[0] shape: torch.Size([14, 300])\n",
      "Attention coef shape: torch.Size([14, 150, 50])\n",
      "h_prime shape: torch.Size([14, 4])\n"
     ]
    }
   ],
   "source": [
    "num_in_features = 300\n",
    "num_out_features = 150\n",
    "num_heads = 4\n",
    "num_edge_types = 3\n",
    "\n",
    "gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "i = 0\n",
    "h_prime, attention_coef = gat_layer((updated_representations[i], edge_indices[i]), edge_types[i])\n",
    "print(f\"dialogue_representation[{i}] shape:\", updated_representations[i].shape)\n",
    "print(\"Attention coef shape:\", attention_coef.shape)\n",
    "print(\"h_prime shape:\", h_prime.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2f139c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_coef[7,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1bc8245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Frequent Edges for Node 0: [27, 43, 23, 39, 11]\n",
      "Top 5 Frequent Edges for Node 1: [19, 15, 39, 23, 11]\n",
      "Top 5 Frequent Edges for Node 2: [11, 27, 35, 31, 19]\n",
      "Top 5 Frequent Edges for Node 3: [47, 39, 11, 31, 23]\n",
      "Top 5 Frequent Edges for Node 4: [31, 15, 47, 39, 7]\n",
      "Top 5 Frequent Edges for Node 5: [27, 47, 35, 7, 15]\n",
      "Top 5 Frequent Edges for Node 6: [19, 11, 7, 39, 3]\n",
      "Top 5 Frequent Edges for Node 7: [47, 39, 35, 19, 3]\n",
      "Top 5 Frequent Edges for Node 8: [35, 31, 23, 3, 47]\n",
      "Top 5 Frequent Edges for Node 9: [47, 39, 19, 43, 15]\n",
      "Top 5 Frequent Edges for Node 10: [19, 23, 15, 11, 7]\n",
      "Top 5 Frequent Edges for Node 11: [39, 35, 43, 47, 23]\n",
      "Top 5 Frequent Edges for Node 12: [15, 11, 39, 35, 31]\n",
      "Top 5 Frequent Edges for Node 13: [47, 39, 43, 11, 3]\n"
     ]
    }
   ],
   "source": [
    "# Assuming input_tensor is your tensor of shape (14, 150, 50)\n",
    "# input_tensor = torch.rand((14, 150, 50))\n",
    "input_tensor = attention_coef\n",
    "# Set the value of k for top-k\n",
    "k = 5\n",
    "\n",
    "# Initialize a list to store top-k frequent edges for each node\n",
    "top_k_frequent_edges_per_node = []\n",
    "\n",
    "# Loop over each node\n",
    "for node_index in range(input_tensor.shape[0]):\n",
    "    # Extract the edges for the current node\n",
    "    node_edges = input_tensor[node_index]\n",
    "\n",
    "    # Flatten the tensor to have shape (150, 50)\n",
    "    flat_tensor = node_edges.view(-1)\n",
    "\n",
    "    # Find the indices of the top-k influential edges for the current node\n",
    "    top_k_indices = torch.argsort(flat_tensor, descending=True)[:k]\n",
    "\n",
    "    # Ensure the top-k indices are within the correct range (0-50)\n",
    "    top_k_indices = top_k_indices % 50\n",
    "\n",
    "    # Flatten the top-k indices and convert them to a list\n",
    "    top_k_flat_list = top_k_indices.view(-1).tolist()\n",
    "\n",
    "    # Count the occurrences of each edge index\n",
    "    edge_counts = Counter(top_k_flat_list)\n",
    "\n",
    "    # Find the top-k most frequent edges for the current node\n",
    "    top_k_frequent_edges = [edge for edge, count in edge_counts.most_common(k)]\n",
    "\n",
    "    # Append the result to the list\n",
    "    top_k_frequent_edges_per_node.append(top_k_frequent_edges)\n",
    "\n",
    "# Print the results\n",
    "for node_index, edges in enumerate(top_k_frequent_edges_per_node):\n",
    "    print(\"Top {} Frequent Edges for Node {}: {}\".format(k, node_index, edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af652dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f0a56700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1823, 0.1155, 0.1823, 0.1688, 0.1688, 0.1823],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "86c0e326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2185, -0.2383,  0.2185,  0.1411,  0.1411,  0.2185],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38cc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# num_of_layers = 2\n",
    "# num_heads_per_layer = [4, 2]\n",
    "# num_features_per_layer = [300, 150, 64]\n",
    "# num_edge_types = 4  # Change this according to your specific edge types\n",
    "\n",
    "# gat_model = GATWithEdgeType(num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types)\n",
    "\n",
    "# # Assuming you have input data 'node_features', 'edge_indices', and 'edge_types'\n",
    "# output, attention_scores = gat_model(updated_representations, edge_indices, edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80221e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_reps = updated_representations[:3]\n",
    "# sample_edge_idx_list = ranges[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86db236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of DialogueGraphDataset and DialogueGraphDataLoader\n",
    "# dataset = DialogueGraphDataset(node_features_list= [updated_representations, encoded_speaker_list],\n",
    "# #                                node_labels_list, \n",
    "#                                edge_index_list = adjacency_list,\n",
    "#                               )\n",
    "# dataloader = DialogueGraphDataLoader(node_features_list = dataset, \n",
    "#                                      edge_index_list = adjacency_list, \n",
    "#                                      batch_size=2, \n",
    "#                                      shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ee705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize your GAT model\n",
    "# gat_model = GAT(\n",
    "#     num_of_layers=3,\n",
    "#     num_heads_per_layer=[4, 4, 6],\n",
    "#     num_features_per_layer=[len(dataset.node_features_list), 64, 64, dataset.num_classes],\n",
    "#     add_skip_connection=True,\n",
    "#     bias=True,\n",
    "#     dropout=0.6,\n",
    "# #     layer_type=\"your_layer_type\",  default 3\n",
    "#     log_attention_weights=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e99ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST_VAL_ACC = 0\n",
    "# BEST_VAL_LOSS = 0\n",
    "# PATIENCE_CNT = 0\n",
    "\n",
    "# BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\n",
    "# CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\n",
    "\n",
    "# # Make sure these exist as the rest of the code assumes it\n",
    "# os.makedirs(BINARIES_PATH, exist_ok=True)\n",
    "# os.makedirs(CHECKPOINTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (dataloader.sampler.num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c10858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90979bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define hyperparameters\n",
    "# num_layers = 2\n",
    "# num_heads_per_layer = [8, 8]\n",
    "# num_features_per_layer = [300, 128, num_classes]  # Adjust num_classes based on your task\n",
    "# add_skip_connection = True\n",
    "# bias = True\n",
    "# dropout = 0.6\n",
    "# layer_type = LayerType.IMP3  # Choose the desired implementation\n",
    "# log_attention_weights = False  # Set to True if you want to log attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomGATLayer(nn.Module):\n",
    "#     def __init__(self, in_features, out_features, num_edge_types, dropout=0.6, alpha=0.2):\n",
    "#         super(CustomGATLayer, self).__init__()\n",
    "#         self.num_edge_types = num_edge_types\n",
    "\n",
    "#         # Node feature transformation\n",
    "#         self.W = nn.Linear(in_features, out_features)\n",
    "\n",
    "#         # Edge attention mechanism for each edge type\n",
    "#         self.attention_weights = nn.ModuleList([nn.Linear(2 * out_features, 1) for _ in range(num_edge_types)])\n",
    "#         self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, node_features, edge_index, edge_type):\n",
    "#         # Node feature transformation\n",
    "#         h = self.W(node_features)\n",
    "\n",
    "#         # Attention mechanism for each edge type\n",
    "#         attention_weights = [torch.exp(self.leaky_relu(att(torch.cat([h[edge_index[0]], h[edge_index[1]]], dim=-1))))\n",
    "#                              for att in self.attention_weights]\n",
    "\n",
    "#         # Compute weighted sum of neighbor features\n",
    "#         aggregated_features = sum(attention_weights[i] * h[edge_index[1]] for i in range(self.num_edge_types))\n",
    "\n",
    "#         # Apply dropout\n",
    "#         aggregated_features = self.dropout(aggregated_features)\n",
    "\n",
    "#         return aggregated_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GATLayerWithEdgeType(GATLayer):\n",
    "#     def __init__(self, num_in_features, num_out_features, num_of_heads, num_edge_types, concat=True, activation=nn.ELU(),\n",
    "#                  dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "#         super().__init__(num_in_features, num_out_features, num_of_heads, concat, activation, dropout_prob,\n",
    "#                          add_skip_connection, bias, log_attention_weights)\n",
    "\n",
    "#         # New trainable parameters for edge type embeddings\n",
    "#         self.edge_type_embeddings = nn.Parameter(torch.Tensor(num_edge_types, num_of_heads, num_out_features))\n",
    "#         self.init_params(LayerType.WITH_EDGE_TYPE)\n",
    "\n",
    "#     def forward(self, data, edge_type):\n",
    "#         in_nodes_features, edge_index = data  # unpack data\n",
    "#         num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "\n",
    "#         in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "#         # Project node features to NH independent output features\n",
    "#         nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "#         nodes_features_proj = self.dropout(nodes_features_proj)\n",
    "\n",
    "#         # Calculate attention scores for source and target nodes based on edge type\n",
    "#         scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "#         scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "#         # Lift the scores based on edge index\n",
    "#         scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "\n",
    "#         # Embedding for edge type\n",
    "#         edge_type_embedding = self.edge_type_embeddings[edge_type]\n",
    "\n",
    "#         # Apply the scoring function with edge type embeddings\n",
    "#         scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted + edge_type_embedding)\n",
    "\n",
    "#         # Neighborhood-aware softmax\n",
    "#         attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "#         attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "#         # Element-wise product with weighted and projected neighborhood feature vectors\n",
    "#         nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "#         # Aggregate neighbors\n",
    "#         out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "#         # Residual/skip connections, concat, and bias\n",
    "#         out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "#         return (out_nodes_features, edge_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "119px",
    "width": "235px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
