{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3cd42",
   "metadata": {},
   "source": [
    "Put libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "553916bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch, os, pickle, sys,torch.nn.init as init, dgl, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# from GAT import GAT\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a0066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Path: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\\utils\\constans.py\n",
      "Project Directory: C:\\Users\\edayo\\Downloads\\4y2t\\THSST-2\\ug_thesis\\ER_GAT\n"
     ]
    }
   ],
   "source": [
    "script_path = os.path.abspath(\"utils\\\\constans.py\")  # Replace __file__ with the path to your script if in a notebook\n",
    "\n",
    "# Determine the project directory by moving up two levels (adjust as needed)\n",
    "project_directory = os.path.dirname(os.path.dirname(script_path))\n",
    "\n",
    "print(\"Script Path:\", script_path)\n",
    "print(\"Project Directory:\", project_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc1152",
   "metadata": {},
   "source": [
    "Code related to GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecf3858",
   "metadata": {
    "code_folding": [
     1,
     6,
     17,
     63,
     139
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class DialogueGraphDataLoader(DataLoader):\n",
    "    def __init__(self, node_features_list, edge_index_list, batch_size=1, shuffle=False):\n",
    "        graph_dataset = DialogueGraphDataset(node_features_list, edge_index_list)\n",
    "        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=dialogue_graph_collate_fn)\n",
    "\n",
    "class DialogueGraphDataset(Dataset):\n",
    "    def __init__(self, node_features_list, edge_index_list):\n",
    "        self.node_features_list = node_features_list\n",
    "        self.edge_index_list = edge_index_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.edge_index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.node_features_list[idx], self.edge_index_list[idx]\n",
    "\n",
    "def dialogue_graph_collate_fn(batch):\n",
    "    node_features_list, edge_index_list = zip(*batch)\n",
    "    \n",
    "    node_features_list_combined = []\n",
    "    num_nodes_seen = 0\n",
    "\n",
    "    for node_features, edge_index in zip(node_features_list, edge_index_list):\n",
    "        # Assuming node_features is a tuple (text_embeddings, speakers_list)\n",
    "        text_embeddings, speakers_list = node_features\n",
    "        combined_features = (text_embeddings, speakers_list)\n",
    "\n",
    "        node_features_list_combined.append(combined_features)\n",
    "\n",
    "        # Translate the range of edge_index\n",
    "        edge_index_list.append(edge_index + num_nodes_seen)\n",
    "        num_nodes_seen += len(text_embeddings)\n",
    "\n",
    "    # Merge the dialogue graphs into a single graph with multiple connected components\n",
    "    node_features_combined = [torch.cat(features, 1) for features in zip(*node_features_list_combined)]\n",
    "    edge_index = torch.cat(edge_index_list, 1)\n",
    "\n",
    "    return node_features_combined, edge_index\n",
    "\n",
    "# get one hot encoding to normalize values\n",
    "def get_ohe(edge_types):\n",
    "    # Number of classes\n",
    "    one_hot_encoding = []\n",
    "\n",
    "    # Convert to one-hot encoding\n",
    "    for edge_type in edge_types:\n",
    "        if edge_type==0:\n",
    "            one_hot_encoding.append([1. ,0. ,0.])\n",
    "        elif edge_type==1:\n",
    "            one_hot_encoding.append([0. ,1. ,0.])\n",
    "        elif edge_type==2:\n",
    "            one_hot_encoding.append([0. ,0. ,1.])\n",
    "    return torch.tensor(one_hot_encoding)\n",
    "\n",
    "# print(edge_feats)\n",
    "\n",
    "# print(sample[1]) #target: node 1\n",
    "# output: [[1, -0.018013518303632736], [4, -0.013125997968018055]]\n",
    "# print(sample[1][1]) #src: node 1\n",
    "# output: [4, -0.013125997968018055]\n",
    "# print(sample[1][1][1]) #attention val\n",
    "# output: -0.013125997968018055\n",
    "def get_inferred_edgetypes1(dialog, edge_types):\n",
    "    inferred_edge_types = []\n",
    "    inferred_edge_indices = []\n",
    "    # the input is by dialog\n",
    "    for target_node in dialog.values():\n",
    "        if len(target_node) == 1:      \n",
    "            inferred_edge_types.append(0)\n",
    "            inferred_edge_indices.append(0)\n",
    "        else:\n",
    "#             print(target_node)\n",
    "            edge_index = target_node[0][0]\n",
    "            highest_attention = target_node[0][1]\n",
    "    #         getting the relation type of a node\n",
    "            for src_node in target_node[1:]:\n",
    "#                 print(src_node)\n",
    "                if highest_attention < src_node[1]:\n",
    "                    highest_attention = src_node[1]\n",
    "                    edge_index = src_node[0]\n",
    "#                     print(src_node)\n",
    "            inferred_edge_indices.append(edge_index)\n",
    "            inferred_edge_types.append(edge_types[edge_index].tolist())\n",
    "#             print(len(edge_types), edge_index)\n",
    "    #         print(\"=======\")\n",
    "    return inferred_edge_indices, inferred_edge_types\n",
    "\n",
    "\n",
    "# [[0, [-0.006794655695557594, 0.11656633019447327, 0.10856685042381287]]]\n",
    "\n",
    "# [[1, [0.47986578941345215, 0.24290838837623596, 0.20059877634048462]], [4, [-0.005781680811196566, 0.06976183503866196, -0.0009530335664749146]]]\n",
    "\n",
    "# [[2, [0.4331742525100708, 0.20570839941501617, 0.18644499778747559]], [5, [0.4922890067100525, 0.21909664571285248, 0.09882635623216629]], [8, [-0.006040609907358885, 0.052320029586553574, 0.025565672665834427]]]\n",
    "\n",
    "def get_inferred_edgetypes2(edges_target_nodes, sample_edge_types, size_dialog, dialog_id):\n",
    "    inferred_edge_types = []\n",
    "    for target_idx in range(size_dialog):\n",
    "    #     print(sample_edgetypes[target_idx])\n",
    "        # window size\n",
    "        # print(len(sample_edgetypes[sample_idx]))\n",
    "        num_edges = len(edges_target_nodes[target_idx])\n",
    "        # display list of edges\n",
    "        # print(sample_edgetypes[sample_idx][0])\n",
    "        # highest observed attention score\n",
    "        if num_edges == 1:\n",
    "        #     print((max(sample_edgetypes[target_idx][0][1])))\n",
    "        #     highest_attn_score = max(sample_edgetypes[target_idx][0][1])\n",
    "\n",
    "    #         print(np.argmax(sample_edgetypes[target_idx][0][1]))\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "#           referencing edge_idx for edgetype\n",
    "#             inferred_edge_types.append(sample_edge_types[edge_idx].tolist())\n",
    "#           referencing egat prediction for edgetype\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "\n",
    "#             print(\"Actual and Predicted EdgeType: \", sample_edge_types[edge_idx].tolist(), edgetype_idx)\n",
    "        #     print(edge_idx)\n",
    "        else:\n",
    "            highest_attn_score = max(edges_target_nodes[target_idx][0][1])\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            for sample_edge in range(1,num_edges):\n",
    "            #     print((max(sample_edgetypes[target_idx][sample_edge][1])))\n",
    "                cur_highest_attn_score = max(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                if cur_highest_attn_score > highest_attn_score:\n",
    "                    highest_attn_score = cur_highest_attn_score\n",
    "                # edge_type with highest attention\n",
    "                    edgetype_idx = np.argmax(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                    edge_idx = edges_target_nodes[target_idx][sample_edge][0]\n",
    "        #     comparison\n",
    "#             inferred_edge_types.append(sample_edge_types[edge_idx].tolist())\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "\n",
    "#             print(\"Actual and Predicted EdgeType: \", sample_edge_types[edge_idx].tolist(), edgetype_idx)\n",
    "\n",
    "    return inferred_edge_types\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f4cb1c",
   "metadata": {
    "code_folding": [
     0,
     51,
     52,
     73
    ]
   },
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        torch.manual_seed(42)\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "        \n",
    "        # Edge type embeddings\n",
    "        torch.manual_seed(42)  # Set your desired seed value\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "        \n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "#         print(\"node_features.shape: \",node_features.shape, \" edge_indices: \", edge_indices.shape)\n",
    "#         print(\"edge_type.shape: \",  edge_type.shape)\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "#         print(\"h_linear.shape after linear_proj of node_features: \",h_linear.shape)\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "#         print(\"h_linear.shape after view: \",h_linear.shape)\n",
    "        # Transpose dimensions of h_linear to match edge_type_embedding's shape\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "#         print(\"h_linear.shape after permuting dimension: \",h_linear.shape)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "#         print(\"edge_type_embedding.shape after transpose: \",edge_type_embedding.shape)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "#         print(\"attention_scores..shape after matmul h_linear and edge_type_emb: \",attention_scores.shape)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "         # Weighted sum of neighbor node representations\n",
    "#         print(\"attention_coefficients.shape after softmax: \",attention_coefficients.shape)\n",
    "#       the one below is for edges\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).mean(dim=2)\n",
    "#         the one below is for attention heads\n",
    "#         updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).sum(dim=1)\n",
    "#         print(\"updated_representation.shape after matmul of trasposed attn_coef and h_linear and sum at dim=1: \",updated_representation.shape)\n",
    "\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores\n",
    "\n",
    "class EGATConv(nn.Module):\n",
    "    r\"\"\"\n",
    "    \n",
    "    Description\n",
    "    -----------\n",
    "    Apply Graph Attention Layer over input graph. EGAT is an extension\n",
    "    of regular `Graph Attention Network <https://arxiv.org/pdf/1710.10903.pdf>`__ \n",
    "    handling edge features, detailed description is available in\n",
    "    `Rossmann-Toolbox <https://pubmed.ncbi.nlm.nih.gov/34571541/>`__ (see supplementary data).\n",
    "     The difference appears in the method how unnormalized attention scores :math:`e_{ij}`\n",
    "     are obtain:\n",
    "        \n",
    "    .. math::\n",
    "        e_{ij} &= \\vec{F} (f_{ij}^{\\prime})\n",
    "\n",
    "        f_{ij}^{\\prim} &= \\mathrm{LeakyReLU}\\left(A [ h_{i} \\| f_{ij} \\| h_{j}]\\right)\n",
    "\n",
    "    where :math:`f_{ij}^{\\prim}` are edge features, :math:`\\mathrm{A}` is weight matrix and \n",
    "    :math: `\\vec{F}` is weight vector. After that resulting node features \n",
    "    :math:`h_{i}^{\\prim}` are updated in the same way as in regular GAT. \n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_node_feats : int\n",
    "        Input node feature size :math:`h_{i}`.\n",
    "    in_edge_feats : int\n",
    "        Input edge feature size :math:`f_{ij}`.\n",
    "    out_node_feats : int\n",
    "        Output nodes feature size.\n",
    "    out_edge_feats : int\n",
    "        Output edge feature size.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    bias : bool, optional\n",
    "        If True, learns a bias term. Defaults: ``True``.\n",
    "        \n",
    "    Examples\n",
    "    ----------\n",
    "    >>> import dgl\n",
    "    >>> import torch as th\n",
    "    >>> from dgl.nn import EGATConv\n",
    "    >>> \n",
    "    >>> num_nodes, num_edges = 8, 30\n",
    "    >>>#define connections\n",
    "    >>> u, v = th.randint(num_nodes, num_edges), th.randint(num_nodes, num_edges) \n",
    "    >>> graph = dgl.graph((u,v))    \n",
    "\n",
    "    >>> node_feats = th.rand((num_nodes, 20)) \n",
    "    >>> edge_feats = th.rand((num_edges, 12))\n",
    "    >>> egat = EGATConv(in_node_feats=20,\n",
    "                          in_edge_feats=12,\n",
    "                          out_node_feats=15,\n",
    "                          out_edge_feats=10,\n",
    "                          num_heads=3)\n",
    "    >>> #forward pass                    \n",
    "    >>> new_node_feats, new_edge_feats = egat(graph, node_feats, edge_feats)\n",
    "    >>> new_node_feats.shape, new_edge_feats.shape\n",
    "    ((8, 3, 12), (30, 3, 10))\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_node_feats,\n",
    "                 in_edge_feats,\n",
    "                 out_node_feats,\n",
    "                 out_edge_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 **kw_args):\n",
    "        \n",
    "        super().__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._out_node_feats = out_node_feats\n",
    "        self._out_edge_feats = out_edge_feats\n",
    "        \n",
    "        self.fc_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=bias)\n",
    "        self.fc_ni = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_nj = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        \n",
    "        # Attention parameter\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reinitialize learnable parameters.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(42)  # You can use any integer value as the seed\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.xavier_normal_(self.fc_node.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "        init.xavier_normal_(self.attn, gain=gain)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "        r\"\"\"\n",
    "        Compute new node and edge features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : DGLGraph\n",
    "            The graph.\n",
    "        nfeats : torch.Tensor\n",
    "            The input node feature of shape :math:`(*, D_{in})`\n",
    "            where:\n",
    "                :math:`D_{in}` is size of input node feature,\n",
    "                :math:`*` is the number of nodes.\n",
    "        efeats: torch.Tensor\n",
    "             The input edge feature of shape :math:`(*, F_{in})`\n",
    "             where:\n",
    "                 :math:`F_{in}` is size of input node feauture,\n",
    "                 :math:`*` is the number of edges.\n",
    "        get_attention : bool, optional\n",
    "                Whether to return the attention values. Default to False.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pair of torch.Tensor\n",
    "            node output features followed by edge output features\n",
    "            The node output feature of shape :math:`(*, H, D_{out})` \n",
    "            The edge output feature of shape :math:`(*, H, F_{out})`\n",
    "            where:\n",
    "                :math:`H` is the number of heads,\n",
    "                :math:`D_{out}` is size of output node feature,\n",
    "                :math:`F_{out}` is size of output edge feature.            \n",
    "        \"\"\"\n",
    "        \n",
    "        with graph.local_scope():\n",
    "            # TODO allow node src and dst feats\n",
    "            graph.edata['f'] = efeats\n",
    "            graph.ndata['h'] = nfeats\n",
    "            # calc edge attention\n",
    "            # same trick way as in dgl.nn.pytorch.GATConv, but also includes edge feats\n",
    "            # https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py#L297\n",
    "            f_ni = self.fc_ni(nfeats)\n",
    "            f_nj = self.fc_nj(nfeats)\n",
    "            f_fij = self.fc_fij(efeats)\n",
    "            graph.srcdata.update({'f_ni' : f_ni})\n",
    "            graph.dstdata.update({'f_nj' : f_nj})\n",
    "            #graph.edata.update({'f_fij' : f_fij})\n",
    "            # add ni, nj factors\n",
    "            graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "            # add fij to node factor\n",
    "            f_out = graph.edata.pop('f_tmp') + f_fij \n",
    "            if self.bias is not None:\n",
    "                f_out+= self.bias\n",
    "            f_out = nn.functional.leaky_relu(f_out)\n",
    "            f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "            # compute attention factor\n",
    "            e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.edata['a'] = edge_softmax(graph, e)\n",
    "            graph.ndata['h_out'] = self.fc_node(nfeats).view(-1, self._num_heads, self._out_node_feats)\n",
    "            # calc weighted sum \n",
    "            graph.update_all(fn.u_mul_e('h_out', 'a', 'm'),\n",
    "                            fn.sum('m', 'h_out'))\n",
    "\n",
    "            h_out = graph.ndata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "            if get_attention:\n",
    "                return h_out, f_out, graph.edata.pop('a')\n",
    "            else:\n",
    "                return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e3ed3",
   "metadata": {},
   "source": [
    "<h3>Methods definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53e5417",
   "metadata": {
    "code_folding": [
     0,
     20,
     24,
     42,
     58
    ]
   },
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    # Initialize an empty list to store pairs\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "#     node_pairs_dict = {}\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 0\n",
    "        while (val <= 3)  and (i+val <= end_idx):\n",
    "            target_idx = i+val\n",
    "#                 print(target_idx)\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "#                 node_pairs_dict[i] = target_idx\n",
    "            val = val+1\n",
    "    \n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "\n",
    "    # Iterate through pairs of nodes\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "\n",
    "#         # Add source node to target node's neighbors\n",
    "#         if target_node not in adjacency_list_dict:\n",
    "#             adjacency_list_dict[target_node] = [source_node]\n",
    "#         else:\n",
    "#             adjacency_list_dict[target_node].append(source_node)\n",
    "\n",
    "        # Add target node to source node's neighbors\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "\n",
    "    return adjacency_list_dict\n",
    "# print(ranges[:1])\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        \n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    \n",
    "    for i in range(dialogs_len): #2140 dialogs\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "#         print(i, \" th dialogue\")\n",
    "#         print(i, dialog_speakers)\n",
    "        for j in range(dialog_len): #num utterances\n",
    "            src_node = dialog_nodes_pairs[j] # j = key = src node\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k] # k = value = targ node\n",
    "                # edge_types = torch.tensor([0, 1, 2]) \n",
    "                # 0: cur-self, 1: past-self, 2: past-other/past-inter\n",
    "                                \n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "#                     print(\"This is 0 \", node_i_idx, node_j_idx)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "#                         print(\"This is 1 \", node_i_idx, node_j_idx)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "#                         print(\"This is 2 \", node_i_idx, node_j_idx)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))  \n",
    "                    \n",
    "    return whole_edge_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20cb34a8",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder.pkl\")\n",
    "encoded_speaker_list = []\n",
    "if checkFile is False:\n",
    "    print(\"Run first the prototype_context_encoder to generate this file\")\n",
    "else:\n",
    "    file = open('data/dump/speaker_encoder.pkl', \"rb\")\n",
    "    encoded_speaker_list, ranges = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f02158e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "file_path = 'embed/updated_representation_list.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    updated_representations = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48400c3a",
   "metadata": {},
   "source": [
    "<h3> Getting the updated representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3af5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = get_all_adjacency_list(ranges)\n",
    "edge_types = get_all_edge_type_list(edge_indices, encoded_speaker_list)\n",
    "edge_indices = get_all_adjacency_list(ranges, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658dd100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges:  50\n",
      "tensor([[ 0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  4,  4,\n",
      "          4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "          9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 12, 12, 13],\n",
      "        [ 0,  1,  2,  3,  1,  2,  3,  4,  2,  3,  4,  5,  3,  4,  5,  6,  4,  5,\n",
      "          6,  7,  5,  6,  7,  8,  6,  7,  8,  9,  7,  8,  9, 10,  8,  9, 10, 11,\n",
      "          9, 10, 11, 12, 10, 11, 12, 13, 11, 12, 13, 12, 13, 13]])\n"
     ]
    }
   ],
   "source": [
    "# i = 38\n",
    "# print(updated_representations[i].shape)\n",
    "print(\"Number of edges: \", len(edge_indices[0][0]))\n",
    "print(edge_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fcf05f",
   "metadata": {},
   "source": [
    "<h4>Instantiating the GAT (first implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d385187",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in_features = 300\n",
    "num_out_features = 300\n",
    "num_heads = 4\n",
    "num_edge_types = 3\n",
    "\n",
    "gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c55310",
   "metadata": {},
   "source": [
    "Infering edge type of nodes from 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8831f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_prime shape:  torch.Size([14, 50]) attention_coef shape:  torch.Size([14, 300, 50])\n"
     ]
    }
   ],
   "source": [
    "i = 0 #dialogue id\n",
    "h_prime, attention_coef = gat_layer((updated_representations[i], edge_indices[i]), edge_types[i])\n",
    "# print(f\"dialogue_representation[{i}] shape:\", updated_representations[i].shape)\n",
    "# print(\"Attention coef shape:\", attention_coef.shape)\n",
    "print(\"h_prime shape: \", h_prime.shape, \"attention_coef shape: \", attention_coef.shape)\n",
    "# source node\n",
    "# index represent the edge\n",
    "target_nodes = edge_indices[i][1].tolist() #first idx represent dialogue id\n",
    "\n",
    "sample = {}\n",
    "sample_edgetypes = []\n",
    "for target_i in set(target_nodes):\n",
    "    sample[target_i] = []\n",
    "# print(target_nodes)\n",
    "for target_node, idx in zip(target_nodes, range(len(target_nodes))):\n",
    "#     print(target_node, idx)\n",
    "    sample[target_node].append([idx, h_prime[target_node][idx].tolist()])\n",
    "\n",
    "list_edge_idx, inferred_edgetypes = get_inferred_edgetypes1(sample, edge_types[i])\n",
    "sample_edgetypes.append(inferred_edgetypes)\n",
    "\n",
    "# print(edge_types[i]) #candidate edgetypes\n",
    "# print(edge_indices[i]) #first row is the src node idx and second row is the target node idx\n",
    "# print(sample_edgetypes) #inferred edge types "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebf0fc",
   "metadata": {},
   "source": [
    "<h4>Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab03412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'anger', 'joy', 'fear', 'disgust', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/dump/label_decoder.pkl', 'rb')\n",
    "label_decoder = pickle.load(file)\n",
    "file.close()\n",
    "label_decoder = list(label_decoder.values())\n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0030c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/train_labels.pkl\")\n",
    "\n",
    "if checkFile is False:\n",
    "    print(\"Please run the context_encoder nodebook to save label file\")\n",
    "    \n",
    "else:\n",
    "    file = open('data/dump/train_labels.pkl', 'rb')\n",
    "    y_train = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195067f2",
   "metadata": {},
   "source": [
    "Graph visualization (for W8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0660ec2f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from pyvis.network import Network\n",
    "\n",
    "# # Create a new Pyvis Network instance\n",
    "# net = Network()\n",
    "\n",
    "# # Add nodes with labels and names\n",
    "# nodes = [(0, 'Node 0'), (1, 'Node 1'), (2, 'Node 2'), (3, 'Node 3')]\n",
    "# labels = [0, 1, 0, 1]  # Example labels for nodes\n",
    "# color_mapping = {0: 'red', 1: 'blue'}  # Define color mapping for labels\n",
    "# for node_id, node_name, label in zip(range(len(nodes)), nodes, labels):\n",
    "#     net.add_node(node_id, label=node_name[1], color=color_mapping[label])\n",
    "\n",
    "# # Add edges with weights and arrows\n",
    "# edges = [(0, 1, 0.5), (1, 2, 0.8), (2, 3, 0.6), (3, 0, 0.7)]  # Example edges with weights\n",
    "# for source, target, weight in edges:\n",
    "#     net.add_edge(source, target, value=weight, arrows='to')\n",
    "\n",
    "# # Set options for the network visualization\n",
    "# net.set_options(\"\"\"\n",
    "# var options = {\n",
    "#   \"edges\": {\n",
    "#     \"arrows\": {\n",
    "#       \"to\": {\n",
    "#         \"enabled\": true\n",
    "#       }\n",
    "#     }\n",
    "#   },\n",
    "#   \"nodes\": {\n",
    "#     \"font\": {\n",
    "#       \"size\": 20\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "# \"\"\")\n",
    "\n",
    "# # Show the network visualization\n",
    "# net.show(\"graph.html\", notebook=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074b57b",
   "metadata": {},
   "source": [
    "<h5>Unsupervised Visualizarion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713520bf",
   "metadata": {},
   "source": [
    "<h6> Visualize 1 instance then scale up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5df4edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming h_prime contains the node embeddings\n",
    "utt_size=13\n",
    "labels = torch.tensor(y_train[:utt_size+1])\n",
    "\n",
    "cherrypicked_nodes = []\n",
    "for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "    cherrypicked_nodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "cherrypicked_nodes = torch.tensor(cherrypicked_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94164d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cherrypicked_nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eff43ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_6448\\1138302530.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_prime_np = torch.tensor(cherrypicked_nodes).numpy()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "h_prime_np = torch.tensor(cherrypicked_nodes).numpy()\n",
    "\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=3, perplexity=5, random_state=42)\n",
    "h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# Plot the node embeddings with different colors for each label\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, emotion in zip(range(len(label_decoder)),label_decoder): \n",
    "    indices = (labels == label).nonzero().squeeze()\n",
    "    plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "plt.xlabel('Dimension 1', color=\"white\")\n",
    "plt.ylabel('Dimension 2', color=\"white\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626f6ac",
   "metadata": {},
   "source": [
    "<h6>Now for all train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b26b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the data in pickle format ##\n",
    "checkFile = os.path.isfile(\"data/dump/1st_gat.pkl\")\n",
    "if checkFile is False:\n",
    "    \n",
    "    \"Start of getting output of 1st GAT\"\n",
    "    all_inferred_edgetypes = []\n",
    "    list_all_edge_idx = []\n",
    "    cherrypicked_nodes = []\n",
    "    for dialog, dialog_id in zip(updated_representations, range(len(updated_representations))):\n",
    "        h_prime, attention_coef = gat_layer((dialog, edge_indices[dialog_id]), edge_types[dialog_id])\n",
    "    #     all_attention\n",
    "        # print(f\"dialogue_representation[{i}] shape:\", updated_representations[i].shape)\n",
    "        # print(\"Attention coef shape:\", attention_coef.shape)\n",
    "        # print(\"h_prime shape:\", h_prime.shape)\n",
    "        # source node\n",
    "        # index represent the edge\n",
    "        target_nodes = edge_indices[dialog_id][1].tolist() #first idx represent dialogue id\n",
    "\n",
    "        sample_edgetypes = {}\n",
    "        for i in set(target_nodes):\n",
    "            sample_edgetypes[i] = []\n",
    "\n",
    "        for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "            sample_edgetypes[target_node].append([edge_idx, h_prime[target_node][edge_idx].tolist()])\n",
    "    #     print(len(sample_edgetypes), dialog_id)\n",
    "        list_edge_idx, inferred_edgetypes = get_inferred_edgetypes1(sample_edgetypes,  edge_types[dialog_id])\n",
    "        list_all_edge_idx.append(list_edge_idx)\n",
    "        all_inferred_edgetypes.append(inferred_edgetypes)\n",
    "\n",
    "\n",
    "        for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "            cherrypicked_nodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "\n",
    "    #     print(dialog_id)\n",
    "    cherrypicked_nodes = torch.tensor(cherrypicked_nodes)\n",
    "    cherrypicked_nodes.shape\n",
    "    \"end of getting output of 1st GAT\"\n",
    "    pickle.dump([cherrypicked_nodes, all_inferred_edgetypes],\n",
    "            open('data/dump/1st_gat', 'wb'))\n",
    "    \n",
    "else:\n",
    "    file = open('data/dump/1st_gat.pkl', 'rb')\n",
    "    cherry_picked_nodes, all_inferred_edgetypes = pickle.load(file)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70d3b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_6448\\394207145.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_prime_np = torch.tensor(cherrypicked_nodes).numpy()\n"
     ]
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "labels = torch.tensor(y_train) \n",
    "h_prime_np = torch.tensor(cherrypicked_nodes).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12c94c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f077bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a8387",
   "metadata": {},
   "source": [
    "Scale up 'till you get the edgetypes of all train dialogs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c51f7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(all_inferred_edgetypes),\n",
    "     'label': y_train,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6f5d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label        0     1     2    3    4    5    6\n",
      "edgetype                                      \n",
      "0         2218   407   941   97  117  277  591\n",
      "2         3742  1093  1371  241  247  599  899\n",
      "The P-Value of the ChiSq Test is: 6.372109133776597e-20\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "CrosstabResult=pd.crosstab(index=df_eda['edgetype'],columns=df_eda['label'])\n",
    "print(CrosstabResult)\n",
    "\n",
    "# Performing Chi-sq test\n",
    "ChiSqResult = chi2_contingency(CrosstabResult)\n",
    "\n",
    "# P-Value is the Probability of H0 being True\n",
    "# If P-Value > 0.05 then only we Accept the assumption(H0)\n",
    "# H0: The variables are not correlated with each other.\n",
    "\n",
    "print('The P-Value of the ChiSq Test is:', ChiSqResult[1])\n",
    "if ChiSqResult[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ec138",
   "metadata": {},
   "source": [
    "Below is a sample usage of GAT (second implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d365dc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:  tensor([3, 5, 5, 1, 7, 3, 4, 0, 3, 1, 5, 4, 3, 0, 0, 2, 2, 6, 1, 7, 3, 3, 7, 6,\n",
      "        5, 5, 6, 5, 2, 3])\n",
      "v:  tensor([6, 3, 7, 0, 2, 4, 2, 6, 4, 0, 6, 1, 3, 0, 3, 5, 1, 1, 0, 1, 4, 1, 3, 3,\n",
      "        6, 3, 6, 3, 4, 7])\n",
      "Graph(num_nodes=8, num_edges=30,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "node_feats.shape:  torch.Size([8, 20])\n",
      "edge_feats.shape:  torch.Size([30, 12])\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 8\n",
    "num_edges = 30\n",
    "#define connections\n",
    "u = torch.randint(num_nodes, (num_edges,))\n",
    "print(\"u: \", u)\n",
    "v = torch.randint(num_nodes, (num_edges,)) \n",
    "print(\"v: \", v)\n",
    "graph = dgl.graph((u,v))    \n",
    "print(graph)\n",
    "node_feats = torch.rand((num_nodes, 20)) \n",
    "print(\"node_feats.shape: \", node_feats.shape)\n",
    "edge_feats = torch.rand((num_edges, 12))\n",
    "print(\"edge_feats.shape: \", edge_feats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fabbe5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use as regular torch/dgl layer work similar as GATConv from dgl library\n",
    "# egat = EGATConv(in_node_feats=num_node_feats,\n",
    "#                 in_edge_feats=num_edge_feats,\n",
    "#                 out_node_feats=10,\n",
    "#                 out_edge_feats=10,\n",
    "#                 num_heads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3799c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_node_feats, new_edge_feats = egat(graph, node_feats, edge_feats)\n",
    "#new_node_feats.shape = (*, num_heads, out_node_feats)\n",
    "#new_eode_feats.shape = (*, num_heads, out_edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6903cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "egat = EGATConv(in_node_feats=300,\n",
    "                    in_edge_feats=3,\n",
    "                    out_node_feats=300,\n",
    "                    out_edge_feats=3,\n",
    "                    num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a76b43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the data in pickle format ##\n",
    "checkFile = os.path.isfile(\"data/dump/2nd_gat.pkl\")\n",
    "if checkFile is False:\n",
    "    \"Start of getting output of 1st GAT\"\n",
    "    inferred_edgetypes2 = []\n",
    "    all_node_feats = []\n",
    "    for dialog_id in range(len(updated_representations)):\n",
    "        graph = dgl.graph((edge_indices[dialog_id][0],edge_indices[dialog_id][1]))    \n",
    "        edge_feats = get_ohe(edge_types[dialog_id])\n",
    "        new_node_feats, new_edge_feats = egat(graph, updated_representations[dialog_id], edge_feats)\n",
    "    #     print(new_node_feats.shape, new_edge_feats.shape)\n",
    "\n",
    "        target_nodes = edge_indices[dialog_id][1].tolist() #first idx represent dialogue id\n",
    "        mean_edge_feats = new_edge_feats.mean(dim=1)\n",
    "\n",
    "        all_node_feats.append(new_node_feats.mean(dim=1).tolist())\n",
    "\n",
    "        sample_edgetypes = {}\n",
    "        for i in set(target_nodes):\n",
    "            sample_edgetypes[i] = []\n",
    "\n",
    "        for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "            sample_edgetypes[target_node].append([edge_idx, mean_edge_feats[edge_idx].tolist()])\n",
    "\n",
    "        sample_edgetypes = get_inferred_edgetypes2(sample_edgetypes, \n",
    "                                edge_types[dialog_id], \n",
    "                                len(updated_representations[dialog_id]),\n",
    "                                dialog_id)\n",
    "        inferred_edgetypes2.append(sample_edgetypes)\n",
    "\n",
    "    all_node_feats = torch.tensor(flatten_extend(all_node_feats))\n",
    "\n",
    "    \"end of getting output of 2nd GAT\"\n",
    "    pickle.dump([all_node_feats, inferred_edgetypes2],\n",
    "            open('data/dump/2nd_gat', 'wb'))\n",
    "    \n",
    "else:\n",
    "    file = open('data/dump/2nd_gat.pkl', 'rb')\n",
    "    all_node_feats, inferred_edgetypes2 = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c99dfa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda2 = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(inferred_edgetypes2),\n",
    "     'label': y_train,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c5a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label        0    1     2    3    4    5    6\n",
      "edgetype                                     \n",
      "0         3657  971  1333  230  205  548  894\n",
      "1         2207  499   920  105  155  321  545\n",
      "2           96   30    59    3    4    7   51\n",
      "The P-Value of the ChiSq Test is: 6.228942926996122e-09\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "CrosstabResult2=pd.crosstab(index=df_eda2['edgetype'],columns=df_eda2['label'])\n",
    "print(CrosstabResult2)\n",
    "\n",
    "# Performing Chi-sq test\n",
    "ChiSqResult2 = chi2_contingency(CrosstabResult2)\n",
    "\n",
    "# P-Value is the Probability of H0 being True\n",
    "# If P-Value > 0.05 then only we Accept the assumption(H0)\n",
    "# H0: The variables are not correlated with each other.\n",
    "\n",
    "print('The P-Value of the ChiSq Test is:', ChiSqResult2[1])\n",
    "if ChiSqResult2[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583899db",
   "metadata": {},
   "source": [
    "Testing on 1 dialog data before scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd5890e7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 4, 300]) torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "dialog_id=0\n",
    "\n",
    "graph = dgl.graph((edge_indices[dialog_id][0],edge_indices[dialog_id][1]))    \n",
    "edge_feats = get_ohe(edge_types[dialog_id])\n",
    "new_node_feats, new_edge_feats = egat(graph, updated_representations[dialog_id], edge_feats)\n",
    "print(new_node_feats.shape, new_edge_feats.shape)\n",
    "#target_nodes = edge_indices[dialog_id][1].tolist() #first idx represent dialogue id\n",
    "h_prime_mean = new_node_feats.mean(dim=1)\n",
    "utt_size=13\n",
    "labels = torch.tensor(y_train[:utt_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdfcf5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 4, 300])\n",
      "torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(new_node_feats.shape)\n",
    "print(new_edge_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7fb0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_6448\\4041947391.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_prime_np = torch.tensor(h_prime_mean).numpy()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "h_prime_np = torch.tensor(h_prime_mean).numpy()\n",
    "\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# Plot the node embeddings with different colors for each label\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, emotion in zip(range(len(label_decoder)),label_decoder): \n",
    "    indices = (labels == label).nonzero().squeeze()\n",
    "    plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "plt.xlabel('Dimension 1', color=\"white\")\n",
    "plt.ylabel('Dimension 2', color=\"white\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c5ab69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_node_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "789bea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "590c2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # Convert tensor to numpy array\n",
    "    h_prime_np = torch.tensor(all_node_feats).numpy()\n",
    "    labels = torch.tensor(y_train)\n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "        print(h_prime_tsne.shape)\n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2a25b",
   "metadata": {},
   "source": [
    "Store the 2nd GAT output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "119px",
    "width": "235px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
