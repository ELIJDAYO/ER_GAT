{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cebd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1470c3",
   "metadata": {},
   "source": [
    "<h3> Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a4fb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ActivationLayer(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super(ActivationLayer, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = FCLayer(input_dim, hidden_dim)\n",
    "        self.activation1 = ActivationLayer(tanh)\n",
    "        self.fc2 = FCLayer(hidden_dim, output_dim)\n",
    "        self.activation2 = ActivationLayer(sigmoid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        return x\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "246bf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to balance class distribution using oversampling\n",
    "def oversample_data(X_train, Y_train, num_classes):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            oversampled_indices = np.random.choice(indices, size=max_class_count - len(indices), replace=True)\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return torch.tensor(X_train), torch.tensor(Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503aae",
   "metadata": {},
   "source": [
    "<h4> Extract train labels and label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/labels_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "y_train = torch.tensor(y_train)\n",
    "    \n",
    "file_path = 'data/dump/label_decoder.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7136ff",
   "metadata": {},
   "source": [
    "<h4> Extract test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "117dcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/labels_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    file = open('data/dump/labels_test.pkl', 'rb')\n",
    "    y_test = pickle.load(file)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3e716",
   "metadata": {},
   "source": [
    "<h4> Getting BERT and GAT outputs for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e3426c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_BERT_train.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    u_primes = pickle.load(file)\n",
    "\n",
    "    concatenated_tensors = []\n",
    "    for dialogue_tensor in u_primes:\n",
    "        concatenated_tensors.extend(dialogue_tensor)\n",
    "\n",
    "    tensorUtterancesTrain = torch.stack(concatenated_tensors)\n",
    "\n",
    "file_path = \"embed/h_prime_BERT-GAT_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    cherryPickedNodesTrain, _ = pickle.load(file)\n",
    "\n",
    "file_path = \"embed/h_prime_BERT-EGAT_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    allNodeFeatsTrain, _ = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9766f",
   "metadata": {},
   "source": [
    "<h4> Getting BERT and GAT outputs for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ba9f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_BERT_test.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    u_primes = pickle.load(file)\n",
    "\n",
    "    concatenated_tensors = []\n",
    "    for dialogue_tensor in u_primes:\n",
    "        concatenated_tensors.extend(dialogue_tensor)\n",
    "\n",
    "    tensorUtterancesTest = torch.stack(concatenated_tensors)\n",
    "\n",
    "file_path = \"embed/h_prime_BERT-GAT_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    cherryPickedNodesTest, _ = pickle.load(file)\n",
    "\n",
    "file_path = \"embed/h_prime_BERT-EGAT_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    allNodeFeatsTest, _ = pickle.load(file)\n",
    "    \n",
    "_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba837d",
   "metadata": {},
   "source": [
    "<h4> Getting BERT and GAT outputs for the valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a2f89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do the same code as above once you have u' and h' of valid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e174164",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "479a3b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a070ce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a43fa315",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "<h3> Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c909e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = tensor_utterances\n",
    "# Y_train = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caf09478",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Define the number of features (k) to select\n",
    "# k = 100  # Adjust this value as needed\n",
    "\n",
    "# # Initialize SelectKBest with the desired score function (e.g., f_classif for classification tasks)\n",
    "# selector = SelectKBest(score_func=f_classif, k=k)\n",
    "\n",
    "# # Fit SelectKBest on the training data and target variable\n",
    "# selector.fit(X_train, Y_train)\n",
    "\n",
    "# # Get the indices of the selected features\n",
    "# selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "# # Get the scores of the selected features\n",
    "# feature_scores = selector.scores_[selected_indices]\n",
    "\n",
    "# # Display the scores along with their corresponding indices\n",
    "# # for idx, score in zip(selected_indices, feature_scores):\n",
    "# #     print(f\"Feature index: {idx}, Score: {score}\")\n",
    "\n",
    "# X_train_selected = X_train[:, selected_indices]\n",
    "# print(X_train_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264084c",
   "metadata": {},
   "source": [
    "Pass u' (BERT) and h' (GAT or EGAT) into this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6501b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_features(encoded_features):\n",
    "    scaler = MinMaxScaler()\n",
    "#       \"FeatureSelected+BERT+GAT: \", concatenatedRepresentationTrain2.shape, \"\\n\",\n",
    "    features_scaled = scaler.fit_transform(encoded_features.clone().detach())\n",
    "    return torch.tensor(features_scaled)\n",
    "\n",
    "def get_selected_features(encoded_features, labels, top_n):\n",
    "    # Apply Min-Max scaling to make the data non-negative\n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "\n",
    "    # Initialize SelectKBest with the desired score function (e.g., f_classif for classification tasks)\n",
    "    selector = SelectKBest(score_func=f_classif, k=100)\n",
    "    # Assuming feature is your feature matrix (12840 instances x 300 dimensions)\n",
    "    # and y_train is your target labels\n",
    "\n",
    "    # Initialize a dictionary to store the indices of top features for each class\n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "    # Calculate the relevance of each feature to each class using chi-squared test\n",
    "    for label in range(7):  # Assuming you have 7 classes\n",
    "        # Create a binary mask indicating instances belonging to the current class\n",
    "        mask = (labels == label)\n",
    "\n",
    "        # SelectKBest with chi2 as the scoring function\n",
    "        selector = SelectKBest(score_func=chi2, k=top_n)  # Select top 20 features\n",
    "        selector.fit(features_scaled, mask)  # Fit SelectKBest to the data\n",
    "        # Get the indices of the top 20 features\n",
    "        top_features_indices = np.argsort(selector.scores_)[-top_n:]\n",
    "        scores = selector.scores_[top_features_indices]\n",
    "        # Store the indices in the dictionary\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    # Print the top features for each class\n",
    "    # for label, indices in top_features_by_class.items():\n",
    "    #     print(f\"Label {label_decoder[label]}: idx {', '.join(map(str, indices))}\")\n",
    "    #     print(top_scores[label])\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "    # Select the desired features\n",
    "    selected_features = encoded_features[:, concatenated_features_indices]\n",
    "#     print(selected_features.shape)\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1af030",
   "metadata": {},
   "source": [
    "Selected h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74744ee3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# X_train = all_node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13cad676",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Apply Min-Max scaling to make the data non-negative\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# # Initialize SelectKBest with the desired score function (e.g., f_classif for classification tasks)\n",
    "# selector = SelectKBest(score_func=f_classif, k=100)\n",
    "# # Assuming X_train is your feature matrix (12840 instances x 300 dimensions)\n",
    "# # and y_train is your target labels\n",
    "\n",
    "# # Initialize a dictionary to store the indices of top features for each class\n",
    "# top_features_by_class = {}\n",
    "# top_scores = {}\n",
    "# # Calculate the relevance of each feature to each class using chi-squared test\n",
    "# for label in range(7):  # Assuming you have 7 classes\n",
    "#     # Create a binary mask indicating instances belonging to the current class\n",
    "#     mask = (Y_train == label)\n",
    "\n",
    "#     # SelectKBest with chi2 as the scoring function\n",
    "#     selector = SelectKBest(score_func=chi2, k=20)  # Select top 20 features\n",
    "#     selector.fit(X_train_scaled, mask)  # Fit SelectKBest to the data\n",
    "#     # Get the indices of the top 20 features\n",
    "#     top_features_indices = np.argsort(selector.scores_)[-20:]\n",
    "#     scores = selector.scores_[top_features_indices]\n",
    "#     # Store the indices in the dictionary\n",
    "#     top_features_by_class[label] = top_features_indices\n",
    "#     top_scores[label] = scores\n",
    "    \n",
    "# # Print the top features for each class\n",
    "# for label, indices in top_features_by_class.items():\n",
    "#     print(f\"Label {label_decoder[label]}: idx {', '.join(map(str, indices))}\")\n",
    "#     print(top_scores[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca3e54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc72c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46d79a94",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efccc17d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca73f7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Selected features of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd9e733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT+EGAT\n",
    "# selectedUPrime, BERT_trainIndices = get_selected_features(tensorUtterancesTrain, y_train, 20)\n",
    "# selectedHPrime, GAT_trainIndices1 = get_selected_features(allNodeFeatsTrain, y_train, 20)\n",
    "# concatenatedRepresentationTrain1 = torch.cat((selectedUPrime, selectedHPrime), dim=1)\n",
    "# BERT+GAT\n",
    "# selectedHPrime, GAT_trainIndices2 = get_selected_features(allNodeFeatsTrain, y_train, 20)\n",
    "# concatenatedRepresentationTrain2 = torch.cat((selectedUPrime, selectedHPrime), dim=1)\n",
    "# raw-BERT\n",
    "rawCtxRepresentationTrain = get_norm_features(tensorUtterancesTrain)\n",
    "# selected-BERT\n",
    "# ctxRepresentationTrain = selectedUPrime\n",
    "\n",
    "# raw-BERT+GAT \n",
    "concatenatedRepresentationTrain1 = torch.cat((get_norm_features(tensorUtterancesTrain), \n",
    "                                              get_norm_features(cherryPickedNodesTrain)), dim=1)\n",
    "# raw-BERT+EGAT\n",
    "concatenatedRepresentationTrain2 = torch.cat((get_norm_features(tensorUtterancesTrain), \n",
    "                                              get_norm_features(allNodeFeatsTrain)), dim=1)\n",
    "\n",
    "# print(\"Sizes of different combination of train data\\n\",\n",
    "#       \"FeatureSelected+BERT+EGAT: \", concatenatedRepresentationTrain1.shape, \"\\n\",\n",
    "#       \"FeatureSelected+BERT+GAT: \", concatenatedRepresentationTrain2.shape, \"\\n\",\n",
    "#       \"BERT: \", rawCtxRepresentationTrain.shape, \"\\n\",\n",
    "#       \"FeatureSelected+BERT: \", ctxRepresentationTrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756dd76",
   "metadata": {},
   "source": [
    "Selected features of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84716dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT+EGAT\n",
    "# selectedUPrime = tensorUtterancesTest[:, BERT_trainIndices]\n",
    "# selectedHPrime = allNodeFeatsTest[:, GAT_trainIndices1]\n",
    "# concatenatedRepresentationTest1 = torch.cat((selectedUPrime, selectedHPrime), dim=1)\n",
    "# BERT+GAT\n",
    "# selectedHPrime = allNodeFeatsTest[:, GAT_trainIndices2]\n",
    "# concatenatedRepresentationTest2 = torch.cat((selectedUPrime, selectedHPrime), dim=1)\n",
    "# raw-BERT\n",
    "rawCtxRepresentationTest = get_norm_features(tensorUtterancesTest)\n",
    "# selected-BERT\n",
    "# ctxRepresentationTest = selectedUPrime\n",
    "# raw-BERT+GAT \n",
    "concatenatedRepresentationTest1 = torch.cat((get_norm_features(tensorUtterancesTest), \n",
    "                                             get_norm_features(cherryPickedNodesTest)), dim=1)\n",
    "# raw-BERT+EGAT\n",
    "concatenatedRepresentationTest2 = torch.cat((get_norm_features(tensorUtterancesTest), \n",
    "                                             get_norm_features(allNodeFeatsTest)), dim=1)\n",
    "\n",
    "# print(\"Sizes of different combination of train data\\n\",\n",
    "#       \"FeatureSelected+BERT+EGAT: \", concatenatedRepresentationTest1.shape, \"\\n\",\n",
    "#       \"FeatureSelected+BERT+GAT: \", concatenatedRepresentationTest2.shape, \"\\n\",\n",
    "#       \"BERT: \", rawCtxRepresentationTest.shape, \"\\n\",\n",
    "#       \"FeatureSelected+BERT: \", ctxRepresentationTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebb893e7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def prep_data(features, labels, isTrain):\n",
    "    num_instances = len(features)\n",
    "    num_classes = 7\n",
    "\n",
    "    # Rescale input features\n",
    "    # selected_features = concatenated_representation / np.linalg.norm(concatenated_representation, axis=1, keepdims=True)\n",
    "\n",
    "    # Apply data resampling (oversampling) to balance class distribution\n",
    "    if isTrain:\n",
    "        X_set, Y_set = oversample_data(features, labels, num_classes)\n",
    "    else:\n",
    "        X_set, Y_set = features, labels\n",
    "\n",
    "    # Calculate class weights for class weighting\n",
    "#     class_counts = np.bincount(labels)\n",
    "#     total_instances = np.sum(class_counts)\n",
    "    # class_weights = torch.tensor([total_instances / (num_classes * count) for count in class_counts], dtype=torch.float32)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
    "    Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
    "    # print(X_train_tensor.shape, Y_train_tensor.shape)\n",
    "    # X_train_tensor = torch.tensor(selected_features)\n",
    "    # Y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(Y_set, return_counts=True)\n",
    "\n",
    "    # Print the counts for each unique label\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        print(f\"Label {label_decoder[label]}: {count} occurrences\")\n",
    "\n",
    "    print(X_tensor.shape, Y_tensor.shape)\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a71c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X_set, Y_set, input_dim, hidden_dim, num_epochs, num_classes):\n",
    "    # Initialize the model\n",
    "    model = MyNetwork(input_dim, hidden_dim, num_classes)\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    # Train the model\n",
    "    print_interval = 1  # Print tqdm every epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        # Use tqdm for progress tracking\n",
    "        with tqdm(total=len(X_set), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for inputs, labels in zip(X_set, Y_set):\n",
    "                inputs = torch.tensor(inputs.clone().detach(), dtype=torch.float32)\n",
    "                labels = torch.tensor(labels.clone().detach(), dtype=torch.long)\n",
    "                # Forward pass\n",
    "                outputs = model(inputs.unsqueeze(0))  # Add batch dimension\n",
    "                loss = criterion(outputs, labels.unsqueeze(0))  # Add batch dimension to labels\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Print average loss and accuracy per epoch\n",
    "        if (epoch + 1) % print_interval == 0:\n",
    "            epoch_loss = total_loss / total_instances\n",
    "            epoch_accuracy = correct_predictions / total_instances\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b435c98b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_emotions(model, X_tensor, Y_tensor, isTrain):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Predict on the data\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convert predicted tensor to numpy array\n",
    "    predicted = predicted.numpy()\n",
    "\n",
    "    # Calculate classification report\n",
    "    report = classification_report(Y_tensor, predicted, target_names=label_decoder.values(), output_dict=True)\n",
    "\n",
    "    # Print classification report\n",
    "    for label, metrics in report.items():\n",
    "        if label != 'accuracy':  # Skip printing accuracy as it's a separate metric\n",
    "            print(f'Class: {label}')\n",
    "            print(f'Precision: {metrics[\"precision\"]:.4f}')\n",
    "            print(f'Recall: {metrics[\"recall\"]:.4f}')\n",
    "            print(f'F1 Score: {metrics[\"f1-score\"]:.4f}')\n",
    "            print()\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f'Accuracy: {report[\"accuracy\"]:.4f}')\n",
    "\n",
    "    # Print averages\n",
    "    print(f'Average Precision: {report[\"macro avg\"][\"precision\"]:.4f}')\n",
    "    print(f'Average Recall: {report[\"macro avg\"][\"recall\"]:.4f}')\n",
    "    print(f'Average F1 Score: {report[\"macro avg\"][\"f1-score\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d2b72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create table of loss and accuracy during training\n",
    "# TODO also compute the time it takes to complete the train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a14b7b",
   "metadata": {},
   "source": [
    "<h4> Train and validate BERT+EGAT given 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "731a5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3259066277.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3259066277.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label anger: 1500 occurrences\n",
      "Label disgust: 364 occurrences\n",
      "Label fear: 338 occurrences\n",
      "Label joy: 2312 occurrences\n",
      "Label neutral: 5960 occurrences\n",
      "Label sadness: 876 occurrences\n",
      "Label surprise: 1490 occurrences\n",
      "torch.Size([12840, 1536]) torch.Size([12840])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|                                                                             | 0/12840 [00:00<?, ?it/s]C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3023422335.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs.clone().detach(), dtype=torch.float32)\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3023422335.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels.clone().detach(), dtype=torch.long)\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.6686, Accuracy: 0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 1.5923, Accuracy: 0.5297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 1.5670, Accuracy: 0.5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 1.5553, Accuracy: 0.5730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 1.5465, Accuracy: 0.5783\n",
      "Train BERT+EGAT given 15 epochs - Elapsed time: 242.17850708961487 seconds\n",
      "Class: anger\n",
      "Accuracy: 0.5781\n",
      "Precision: 0.5781\n",
      "Recall: 0.0740\n",
      "F1 Score: 0.1312\n",
      "\n",
      "Class: disgust\n",
      "Accuracy: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Class: fear\n",
      "Accuracy: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Class: joy\n",
      "Accuracy: 0.7079\n",
      "Precision: 0.7079\n",
      "Recall: 0.1657\n",
      "F1 Score: 0.2685\n",
      "\n",
      "Class: neutral\n",
      "Accuracy: 0.5019\n",
      "Precision: 0.5019\n",
      "Recall: 0.9946\n",
      "F1 Score: 0.6671\n",
      "\n",
      "Class: sadness\n",
      "Accuracy: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Class: surprise\n",
      "Accuracy: 0.6915\n",
      "Precision: 0.6915\n",
      "Recall: 0.1369\n",
      "F1 Score: 0.2286\n",
      "\n",
      "Class: accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain BERT+EGAT given 15 epochs - Elapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mclassify_emotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfcClf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_trainTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_trainTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36mclassify_emotions\u001b[1;34m(model, X_tensor, Y_tensor, isTrain)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, metrics \u001b[38;5;129;01min\u001b[39;00m report\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "X_trainTensor, Y_trainTensor = prep_data(concatenatedRepresentationTrain2, y_train, False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fcClf = model_train(X_set=X_trainTensor, \n",
    "                    Y_set=Y_trainTensor,\n",
    "                    input_dim=X_trainTensor.shape[1], \n",
    "                    hidden_dim=256, \n",
    "                    num_epochs=5, \n",
    "                    num_classes=7)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Train BERT+EGAT given 15 epochs - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d293b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_emotions(fcClf, X_trainTensor, Y_trainTensor, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50c657f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label anger: 516 occurrences\n",
      "Label disgust: 99 occurrences\n",
      "Label fear: 60 occurrences\n",
      "Label joy: 495 occurrences\n",
      "Label neutral: 1615 occurrences\n",
      "Label sadness: 263 occurrences\n",
      "Label surprise: 352 occurrences\n",
      "torch.Size([3400, 1536]) torch.Size([3400])\n",
      "Class: anger\n",
      "Precision: 0.6494\n",
      "Recall: 0.1938\n",
      "F1 Score: 0.2985\n",
      "\n",
      "Class: disgust\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Class: fear\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Class: joy\n",
      "Precision: 0.5140\n",
      "Recall: 0.2970\n",
      "F1 Score: 0.3764\n",
      "\n",
      "Class: neutral\n",
      "Precision: 0.5698\n",
      "Recall: 0.9759\n",
      "F1 Score: 0.7195\n",
      "\n",
      "Class: sadness\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Class: surprise\n",
      "Precision: 0.5979\n",
      "Recall: 0.3295\n",
      "F1 Score: 0.4249\n",
      "\n",
      "Class: macro avg\n",
      "Precision: 0.3330\n",
      "Recall: 0.2566\n",
      "F1 Score: 0.2599\n",
      "\n",
      "Class: weighted avg\n",
      "Precision: 0.5059\n",
      "Recall: 0.5703\n",
      "F1 Score: 0.4858\n",
      "\n",
      "Accuracy: 0.5703\n",
      "Average Precision: 0.3330\n",
      "Average Recall: 0.2566\n",
      "Average F1 Score: 0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3259066277.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3259066277.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\edayo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict the test set\n",
    "X_testTensor, Y_testTensor = prep_data(concatenatedRepresentationTest2, y_test, False)\n",
    "classify_emotions(fcClf, X_testTensor, Y_testTensor, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e94626",
   "metadata": {},
   "source": [
    "<h4> Train and validate BERT+GAT given 2 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "332efcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(concatenatedRepresentationTrain1, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5879735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_trainTensor, Y_trainTensor = prep_data(X_train, Y_train, False)\n",
    "# fcClf = model_train(X_set=X_trainTensor, \n",
    "#                     Y_set=Y_trainTensor,\n",
    "#                     input_dim=X_trainTensor.shape[1], \n",
    "#                     hidden_dim=256, \n",
    "#                     num_epochs=20, \n",
    "#                     num_classes=7)\n",
    "# classify_emotions(fcClf, X_trainTensor, Y_trainTensor, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c71f314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the test set\n",
    "# X_testTensor, Y_testTensor = prep_data(X_test, Y_test, False)\n",
    "# classify_emotions(fcClf, X_testTensor, Y_testTensor, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254c7e5",
   "metadata": {},
   "source": [
    "placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57c145a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3259066277.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3259066277.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label anger: 5960 occurrences\n",
      "Label disgust: 5960 occurrences\n",
      "Label fear: 5960 occurrences\n",
      "Label joy: 5960 occurrences\n",
      "Label neutral: 5960 occurrences\n",
      "Label sadness: 5960 occurrences\n",
      "Label surprise: 5960 occurrences\n",
      "torch.Size([41720, 1536]) torch.Size([41720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|                                                                            | 0/41720 [00:00<?, ?it/s]C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3023422335.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs.clone().detach(), dtype=torch.float32)\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_23752\\3023422335.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels.clone().detach(), dtype=torch.long)\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.4025, Accuracy: 0.5331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m X_trainTensor, Y_trainTensor \u001b[38;5;241m=\u001b[39m prep_data(concatenatedRepresentationTrain1, y_train, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m fcClf \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_trainTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mY_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_trainTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_trainTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     13\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(X_set, Y_set, input_dim, hidden_dim, num_epochs, num_classes)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_trainTensor, Y_trainTensor = prep_data(concatenatedRepresentationTrain1, y_train, True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fcClf = model_train(X_set=X_trainTensor, \n",
    "                    Y_set=Y_trainTensor,\n",
    "                    input_dim=X_trainTensor.shape[1], \n",
    "                    hidden_dim=256, \n",
    "                    num_epochs=10, \n",
    "                    num_classes=7)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Trai BERT+GAT given 15 epochs - Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "classify_emotions(fcClf, X_trainTensor, Y_trainTensor, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa10a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set\n",
    "X_testTensor, Y_testTensor = prep_data(concatenatedRepresentationTest1, y_test, False)\n",
    "classify_emotions(fcClf, X_testTensor, Y_testTensor, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5dc686",
   "metadata": {},
   "source": [
    "<h4> Train and validate BERT (x feature selection) given 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f4578",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "X_trainTensor, Y_trainTensor = prep_data(rawCtxRepresentationTrain, y_train, True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fcClf = model_train(X_set=X_trainTensor, \n",
    "                    Y_set=Y_trainTensor,\n",
    "                    input_dim=X_trainTensor.shape[1], \n",
    "                    hidden_dim=256, \n",
    "                    num_epochs=10, \n",
    "                    num_classes=7)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training BERT+EGAT given 15 epochs - Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "classify_emotions(fcClf, X_trainTensor, Y_trainTensor, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set\n",
    "X_testTensor, Y_testTensor = prep_data(rawCtxRepresentationTest, y_test, False)\n",
    "classify_emotions(fcClf, X_testTensor, Y_testTensor, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac86ff",
   "metadata": {},
   "source": [
    "<h4> Train and validate BERT (o feature selection) given 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataLoader, X_trainTensor, Y_trainTensor = get_data_loader(ctxRepresentationTrain, y_train, True)\n",
    "# # TODO 3rd argument is tunable\n",
    "# fcClf = model_train(dataLoader, \n",
    "#                     input_dim=X_trainTensor.shape[1], \n",
    "#                     output_dim=50, \n",
    "#                     num_epochs=2, \n",
    "#                     num_classes=7)\n",
    "# classify_emotions(fcClf, X_trainTensor, Y_trainTensor, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the test set\n",
    "# _, X_testTensor, Y_testTensor = get_data_loader(ctxRepresentationTest, y_test, False)\n",
    "# classify_emotions(fcClf, X_testTensor, Y_testTensor, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
