{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cebd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4fb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ActivationLayer(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super(ActivationLayer, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = FCLayer(input_dim, hidden_dim)\n",
    "        self.activation1 = ActivationLayer(tanh)\n",
    "        self.fc2 = FCLayer(hidden_dim, output_dim)\n",
    "        self.activation2 = ActivationLayer(sigmoid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        return x\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246bf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to balance class distribution using oversampling\n",
    "def oversample_data(X_train, Y_train):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            oversampled_indices = np.random.choice(indices, size=max_class_count - len(indices), replace=True)\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files\n",
    "checkFile = os.path.isfile(\"data/dump/train_labels.pkl\")\n",
    "\n",
    "if not checkFile:\n",
    "    print(\"Please run the context_encoder notebook to save label file\")\n",
    "    \n",
    "else:\n",
    "    file = open('data/dump/train_labels.pkl', 'rb')\n",
    "    y_train = pickle.load(file)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    file.close()\n",
    "    \n",
    "file = open('data/dump/label_decoder.pkl', 'rb')\n",
    "label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e3426c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12840, 300]) torch.Size([12840, 300])\n"
     ]
    }
   ],
   "source": [
    "# loading files 2\n",
    "file_path = 'embed/u_prime2.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    updated_representations = pickle.load(file)\n",
    "\n",
    "    # Concatenate all the tensors representing individual utterances\n",
    "    concatenated_tensors = []\n",
    "    for dialogue_tensor in updated_representations:\n",
    "        concatenated_tensors.extend(dialogue_tensor)\n",
    "\n",
    "# Convert the concatenated list of tensors into a single tensor\n",
    "tensor_utterances = torch.stack(concatenated_tensors)\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/BERT_h_prime1.pkl\")\n",
    "if not checkFile:\n",
    "    print(\"Run relationTypeEncoder2 encoder before running classifier\")\n",
    "    \n",
    "else:\n",
    "    file = open('data/dump/BERT_h_prime1.pkl', 'rb')\n",
    "    cherry_picked_nodes, _ = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/BERT_h_prime2.pkl\")\n",
    "if not checkFile:\n",
    "    print(\"Run relationTypeEncoder2 before running classifier\")\n",
    "    \n",
    "else:\n",
    "    file = open('data/dump/BERT_h_prime2.pkl', 'rb')\n",
    "    all_node_feats, _ = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "_ = None\n",
    "print(cherry_picked_nodes.shape, all_node_feats.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e174164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479a3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a070ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a43fa315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "Prep data and EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c909e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tensor_utterances\n",
    "Y_train = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf09478",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Define the number of features (k) to select\n",
    "# k = 100  # Adjust this value as needed\n",
    "\n",
    "# # Initialize SelectKBest with the desired score function (e.g., f_classif for classification tasks)\n",
    "# selector = SelectKBest(score_func=f_classif, k=k)\n",
    "\n",
    "# # Fit SelectKBest on the training data and target variable\n",
    "# selector.fit(X_train, Y_train)\n",
    "\n",
    "# # Get the indices of the selected features\n",
    "# selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "# # Get the scores of the selected features\n",
    "# feature_scores = selector.scores_[selected_indices]\n",
    "\n",
    "# # Display the scores along with their corresponding indices\n",
    "# # for idx, score in zip(selected_indices, feature_scores):\n",
    "# #     print(f\"Feature index: {idx}, Score: {score}\")\n",
    "\n",
    "# X_train_selected = X_train[:, selected_indices]\n",
    "# print(X_train_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264084c",
   "metadata": {},
   "source": [
    "Selected feature u'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6501b577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label anger: idx 246, 497, 600, 74, 396, 278, 637, 82, 483, 411, 262, 589, 346, 200, 289, 395, 591, 161, 87, 544\n",
      "[2.34643138 2.34656414 2.46194175 2.50832557 2.53195847 2.69447376\n",
      " 2.75094615 2.75745995 2.88995412 2.9498358  2.97115567 3.04870125\n",
      " 3.25441899 3.26687332 3.42034638 3.8293954  3.89758703 3.98234341\n",
      " 4.95180723 6.69162142]\n",
      "Label disgust: idx 411, 734, 168, 325, 220, 118, 710, 187, 183, 693, 487, 86, 113, 673, 87, 544, 346, 200, 430, 625\n",
      "[1.1263909  1.12715288 1.14107922 1.15446211 1.16027577 1.18437507\n",
      " 1.21307733 1.22602702 1.23209869 1.25558807 1.27585881 1.28278117\n",
      " 1.28912602 1.30334911 1.345847   1.46037267 1.51213768 1.53303158\n",
      " 1.7282929  1.76445789]\n",
      "Label fear: idx 247, 553, 426, 92, 295, 425, 748, 403, 560, 374, 12, 287, 188, 652, 663, 598, 734, 60, 411, 6\n",
      "[1.32962887 1.33329598 1.35888197 1.37588902 1.38860804 1.39378474\n",
      " 1.39860036 1.40970726 1.4384687  1.45376044 1.47879477 1.50039816\n",
      " 1.54102315 1.5556053  1.67205718 1.76377323 1.82892902 1.87085238\n",
      " 2.12376455 2.47376155]\n",
      "Label joy: idx 363, 441, 164, 633, 407, 554, 497, 278, 83, 684, 182, 96, 593, 376, 345, 544, 28, 483, 555, 87\n",
      "[4.23619326 4.23752269 4.26267363 4.37277764 4.37641977 4.50030329\n",
      " 4.74355165 4.84397538 4.96605064 4.99516344 5.29760617 5.40866584\n",
      " 5.46627212 5.50411077 5.69510425 5.82232265 5.84365888 6.15433481\n",
      " 6.56735107 6.94438543]\n",
      "Label neutral: idx 613, 652, 349, 572, 549, 403, 453, 157, 487, 183, 159, 685, 116, 532, 359, 108, 641, 346, 410, 6\n",
      "[4.23732241 4.30466998 4.32932959 4.44764192 4.56175203 4.61202326\n",
      " 4.72843699 4.73930153 4.93811614 5.04246097 5.08653378 5.11009628\n",
      " 5.25105196 5.56245636 5.74051309 6.1540542  6.94847012 7.33800059\n",
      " 8.52671753 9.81691455]\n",
      "Label sadness: idx 475, 552, 6, 556, 532, 366, 378, 191, 281, 657, 200, 586, 735, 346, 664, 211, 719, 357, 190, 663\n",
      "[2.36231222 2.50273843 2.62502392 2.7101138  2.71498237 2.8043238\n",
      " 2.88036666 3.01815869 3.02341917 3.08566432 3.12912075 3.18162655\n",
      " 3.25099252 3.37331287 3.56386631 3.70061163 3.83739322 4.20073655\n",
      " 4.21117259 7.95904628]\n",
      "Label surprise: idx 617, 608, 711, 107, 34, 762, 198, 54, 116, 475, 577, 36, 580, 302, 664, 402, 607, 274, 127, 756\n",
      "[ 7.82740499  7.98046827  7.99483927  8.26570544  8.282366    8.55948958\n",
      "  8.65278143  8.97999993  9.44139333 10.19162124 10.38761136 10.55451333\n",
      " 11.28864392 11.96803456 12.31968272 13.25119993 14.01361199 14.78946399\n",
      " 15.43930153 16.03061344]\n"
     ]
    }
   ],
   "source": [
    "# Apply Min-Max scaling to make the data non-negative\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Initialize SelectKBest with the desired score function (e.g., f_classif for classification tasks)\n",
    "selector = SelectKBest(score_func=f_classif, k=100)\n",
    "# Assuming X_train is your feature matrix (12840 instances x 300 dimensions)\n",
    "# and y_train is your target labels\n",
    "\n",
    "# Initialize a dictionary to store the indices of top features for each class\n",
    "top_features_by_class = {}\n",
    "top_scores = {}\n",
    "# Calculate the relevance of each feature to each class using chi-squared test\n",
    "for label in range(7):  # Assuming you have 7 classes\n",
    "    # Create a binary mask indicating instances belonging to the current class\n",
    "    mask = (Y_train == label)\n",
    "\n",
    "    # SelectKBest with chi2 as the scoring function\n",
    "    selector = SelectKBest(score_func=chi2, k=20)  # Select top 20 features\n",
    "    selector.fit(X_train_scaled, mask)  # Fit SelectKBest to the data\n",
    "    # Get the indices of the top 20 features\n",
    "    top_features_indices = np.argsort(selector.scores_)[-20:]\n",
    "    scores = selector.scores_[top_features_indices]\n",
    "    # Store the indices in the dictionary\n",
    "    top_features_by_class[label] = top_features_indices\n",
    "    top_scores[label] = scores\n",
    "    \n",
    "# Print the top features for each class\n",
    "for label, indices in top_features_by_class.items():\n",
    "    print(f\"Label {label_decoder[label]}: idx {', '.join(map(str, indices))}\")\n",
    "    print(top_scores[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "997102e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_features_set = set()\n",
    "for label, indices in top_features_by_class.items():\n",
    "    concatenated_features_set.update(indices)\n",
    "\n",
    "concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "# concatenated_features_indices = []\n",
    "# for indices in top_features_by_class.values():\n",
    "#     concatenated_features_indices.extend(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018bc3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12840, 114])\n"
     ]
    }
   ],
   "source": [
    "concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "# Select the desired features from X_train\n",
    "selected_features1 = tensor_utterances[:, concatenated_features_indices]\n",
    "print(selected_features1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1af030",
   "metadata": {},
   "source": [
    "Selected h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74744ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13cad676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label anger: idx 167, 99, 139, 140, 175, 128, 276, 210, 84, 227, 204, 43, 288, 58, 244, 216, 92, 194, 184, 129\n",
      "[1.24176531 1.32689859 1.33852027 1.36537897 1.39285872 1.39810761\n",
      " 1.42481072 1.49667837 1.51571794 1.53939416 1.6200533  1.66673762\n",
      " 1.86713552 1.91877562 2.03711769 2.04876067 2.37751768 2.4650082\n",
      " 2.69172596 3.35008585]\n",
      "Label disgust: idx 239, 208, 250, 152, 115, 176, 183, 214, 55, 123, 101, 194, 13, 200, 136, 171, 283, 204, 129, 234\n",
      "[0.39419684 0.39860541 0.40854525 0.41389446 0.43105479 0.43148324\n",
      " 0.44555406 0.51492261 0.52748753 0.54495951 0.55794862 0.61718555\n",
      " 0.62381285 0.63588488 0.67294284 0.74115633 0.74915705 0.78864054\n",
      " 0.86503108 1.03449513]\n",
      "Label fear: idx 223, 18, 78, 293, 163, 67, 101, 108, 104, 210, 221, 71, 175, 93, 177, 13, 225, 48, 234, 55\n",
      "[0.36006296 0.37594075 0.37779532 0.38241712 0.38545364 0.39580486\n",
      " 0.40210837 0.45645068 0.45660078 0.46898822 0.47881793 0.4958627\n",
      " 0.51283995 0.52126024 0.54828596 0.55156159 0.59597655 0.70761942\n",
      " 0.70814717 0.95198839]\n",
      "Label joy: idx 222, 27, 288, 2, 275, 177, 14, 93, 291, 8, 58, 99, 201, 143, 135, 129, 244, 210, 194, 43\n",
      "[1.54838496 1.54862927 1.55057002 1.61593612 1.64742821 1.70112769\n",
      " 1.72847651 1.73025375 1.89043866 1.93917888 1.9440076  1.95573403\n",
      " 2.55185641 2.58289914 2.76612515 2.78085153 3.44601542 3.8413023\n",
      " 3.8455047  4.13183592]\n",
      "Label neutral: idx 233, 189, 75, 172, 164, 132, 234, 23, 104, 205, 52, 215, 67, 264, 13, 180, 86, 154, 83, 204\n",
      "[0.89172501 0.90748897 0.91165702 0.94303109 0.95499025 0.96159553\n",
      " 0.99541412 1.01119965 1.06041095 1.13014414 1.13526395 1.22860803\n",
      " 1.24591689 1.40794977 1.49227472 1.59977236 1.64499864 1.80755284\n",
      " 1.82986071 3.56862286]\n",
      "Label sadness: idx 196, 9, 298, 129, 205, 154, 4, 48, 143, 210, 119, 242, 236, 204, 297, 225, 52, 274, 238, 83\n",
      "[0.72945014 0.75049952 0.80066502 0.80125881 0.86722474 0.93596682\n",
      " 0.95595361 0.95969178 0.9912086  1.04988858 1.07969625 1.09408652\n",
      " 1.12008048 1.21036138 1.23115541 1.26526321 1.29493821 1.63329275\n",
      " 1.99901024 2.22455478]\n",
      "Label surprise: idx 61, 45, 95, 115, 18, 80, 2, 69, 179, 57, 254, 19, 231, 267, 192, 184, 173, 89, 65, 77\n",
      "[1.212132   1.2450587  1.24558897 1.27561851 1.32192656 1.33386683\n",
      " 1.39780551 1.40748547 1.48686173 1.53150608 1.57424451 1.78790847\n",
      " 1.88778325 1.97374507 1.98280201 2.1376452  2.72403324 2.95440239\n",
      " 3.29504524 4.118243  ]\n"
     ]
    }
   ],
   "source": [
    "# Apply Min-Max scaling to make the data non-negative\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Initialize SelectKBest with the desired score function (e.g., f_classif for classification tasks)\n",
    "selector = SelectKBest(score_func=f_classif, k=100)\n",
    "# Assuming X_train is your feature matrix (12840 instances x 300 dimensions)\n",
    "# and y_train is your target labels\n",
    "\n",
    "# Initialize a dictionary to store the indices of top features for each class\n",
    "top_features_by_class = {}\n",
    "top_scores = {}\n",
    "# Calculate the relevance of each feature to each class using chi-squared test\n",
    "for label in range(7):  # Assuming you have 7 classes\n",
    "    # Create a binary mask indicating instances belonging to the current class\n",
    "    mask = (Y_train == label)\n",
    "\n",
    "    # SelectKBest with chi2 as the scoring function\n",
    "    selector = SelectKBest(score_func=chi2, k=20)  # Select top 20 features\n",
    "    selector.fit(X_train_scaled, mask)  # Fit SelectKBest to the data\n",
    "    # Get the indices of the top 20 features\n",
    "    top_features_indices = np.argsort(selector.scores_)[-20:]\n",
    "    scores = selector.scores_[top_features_indices]\n",
    "    # Store the indices in the dictionary\n",
    "    top_features_by_class[label] = top_features_indices\n",
    "    top_scores[label] = scores\n",
    "    \n",
    "# Print the top features for each class\n",
    "for label, indices in top_features_by_class.items():\n",
    "    print(f\"Label {label_decoder[label]}: idx {', '.join(map(str, indices))}\")\n",
    "    print(top_scores[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b5d8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_features_set = set()\n",
    "for label, indices in top_features_by_class.items():\n",
    "    concatenated_features_set.update(indices)\n",
    "\n",
    "concatenated_features_indices = list(concatenated_features_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37ad95c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12840, 102])\n"
     ]
    }
   ],
   "source": [
    "concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "# Select the desired features from X_train\n",
    "selected_features2 = tensor_utterances[:, concatenated_features_indices]\n",
    "print(selected_features2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca3e54f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0850,  0.2415, -0.1310, -0.1509, -0.4730,  0.2069,  0.2905, -0.2111,\n",
       "        -0.1705,  0.1179,  0.0678,  0.3224,  0.0701, -0.1988,  0.0966,  0.4576,\n",
       "         0.2275, -0.3285,  0.0900, -0.0196,  0.5463,  0.2167, -0.1009,  0.0902,\n",
       "         0.1786,  0.1149, -0.2084, -0.1492, -0.1536, -0.4069,  0.2688,  0.2200,\n",
       "         0.0887, -0.0920,  0.4038,  0.0625, -0.2448,  0.1404, -0.1904, -0.2601,\n",
       "         0.0509,  0.2384,  0.1512,  0.1104, -0.0750, -0.0030, -0.2030, -0.2019,\n",
       "         0.0770,  0.0610,  0.0168,  0.1289, -0.0702,  0.2344, -0.1975,  0.1174,\n",
       "        -0.2626,  0.0767, -0.0280, -0.0576,  0.1835, -0.0877,  0.0558, -0.3354,\n",
       "        -0.5473,  0.2981,  0.3413,  0.0841,  0.2141,  0.0806,  0.1045,  0.0841,\n",
       "        -0.0943, -0.1115, -0.2186, -0.1445,  0.2650, -0.1412,  0.2795, -0.2352,\n",
       "        -0.1543, -0.1018,  0.1010, -0.4150, -0.1031, -0.1541,  0.0580, -0.0073,\n",
       "         0.1114,  0.1634,  0.2207, -0.0395,  0.1615, -0.2581, -0.0831,  0.0563,\n",
       "        -0.1234, -0.0139,  0.4461, -0.2810,  0.1687, -0.0182, -0.0913, -0.1865,\n",
       "        -0.0421, -0.0717, -0.0185,  0.1394, -0.5815,  0.3383, -0.2993,  0.3086,\n",
       "        -0.2620,  0.6758])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc72c402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9118e-01, -8.0589e-02, -1.3453e-01, -2.1708e-01,  4.1972e-01,\n",
       "         2.4973e-01,  6.6335e-02, -3.6764e-01,  2.6825e-01, -1.4974e-01,\n",
       "         3.8856e-02, -4.3070e-02,  1.0537e-01, -8.2093e-02,  1.9674e-01,\n",
       "         1.5461e-01,  6.8548e-02,  9.7370e-03, -6.9421e-02,  1.4242e-01,\n",
       "         4.2998e-01, -5.4965e-03, -4.7828e-01,  1.6158e-01, -2.2648e-02,\n",
       "        -9.8701e-02,  1.1491e-01, -8.9931e-02, -2.0842e-01,  3.4715e-01,\n",
       "         2.6876e-01,  2.2542e-01,  8.8338e-02,  1.4181e-01,  6.1919e-02,\n",
       "        -3.5121e-02,  1.4036e-01,  4.1194e-01, -7.5740e-02, -4.1709e-01,\n",
       "         3.8579e-02,  8.8523e-02,  5.9374e-02,  1.5188e-01,  1.2799e-01,\n",
       "        -4.2667e-02, -2.2676e-01,  9.2587e-03, -1.7325e-01, -3.0602e-01,\n",
       "        -2.4307e-01, -1.9749e-01, -3.0208e-02, -6.5170e-02,  2.7194e-01,\n",
       "        -8.3018e-02, -2.5097e-03,  2.1087e-02,  2.9575e-01, -5.3700e-02,\n",
       "        -2.9273e-01,  1.8353e-01,  3.0357e-01,  5.1770e-01,  1.4428e-01,\n",
       "         3.7037e-01,  3.4677e-01,  8.4134e-02,  3.2776e-01, -2.7702e-01,\n",
       "         3.9760e+00,  2.8406e-01, -3.5799e-01,  4.4821e-03,  2.6299e-01,\n",
       "         2.1961e-01,  3.8928e-01,  1.2934e-01,  1.5178e-01, -2.1157e-01,\n",
       "         2.6144e-02, -2.1625e-01,  2.3215e-01,  1.6011e-01, -1.5039e-01,\n",
       "         5.4821e-02,  2.6356e-01, -2.1402e-04, -3.0803e-01,  1.3217e-01,\n",
       "         1.1365e-01, -1.4345e-01, -4.2818e-01, -1.5435e-01, -1.5012e-01,\n",
       "         6.9060e-02,  3.6798e-01,  1.4209e-01, -2.0597e-01,  1.1256e-01,\n",
       "         1.6893e-02, -1.4220e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46d79a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efccc17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca73f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Selected feature's GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9b3b4",
   "metadata": {},
   "source": [
    "current progress (9pm March 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd9e733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12840, 216])\n"
     ]
    }
   ],
   "source": [
    "# Assuming cnn_bilstm_representations and gat_representations are PyTorch tensors\n",
    "concatenated_representation = torch.cat((selected_features1, selected_features2), dim=1)\n",
    "\n",
    "# concatenated_representation1 = torch.cat((tensor_utterances, cherry_picked_nodes), dim=1)\n",
    "#  concatenated_representation2 = torch.cat((cherry_picked_nodes, all_node_feats), dim=1)\n",
    "print(concatenated_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1cb6e9",
   "metadata": {},
   "source": [
    "Training and predicting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632daa54",
   "metadata": {},
   "source": [
    "1st version (only feature engineering and u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc4cf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(selected_features.shape)\n",
    "# # Generate sample data\n",
    "# num_instances = len(selected_features)\n",
    "# input_dim = selected_features.shape[1]\n",
    "# num_classes = 7\n",
    "\n",
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# Y_train = y_train\n",
    "# # X_train = torch.randn(num_instances, input_dim)\n",
    "# # Assuming Y_train is a vector containing the label indices (0 to num_classes-1) for each instance\n",
    "# # Y_train = torch.randint(0, num_classes, (num_instances,))\n",
    "\n",
    "# # Calculate class weights to balance the loss function\n",
    "# class_counts = torch.bincount(Y_train)\n",
    "# # class_weights = torch.tensor([0.15, 0.03, 0.20, 0.09, 0.15, 0.23, 0.04])\n",
    "\n",
    "# # Initialize the model\n",
    "# model = MyNetwork(input_dim, 7, num_classes)\n",
    "# print(model)\n",
    "# # Define loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss(weight=None)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# # Train the model\n",
    "# num_epochs = 3000\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     outputs = model(X_train)\n",
    "#     loss = criterion(outputs, Y_train)\n",
    "\n",
    "#     # Backward and optimize\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if (epoch+1) % 100 == 0:  # Reduced printing frequency for faster training progress monitoring\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fde7f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict on the training data\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_train)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = (predicted == Y_train).sum().item() / num_instances\n",
    "# print(f'Training Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# unique_labels, label_counts = np.unique(predicted, return_counts=True)\n",
    "\n",
    "# # Print the counts for each unique label\n",
    "# for label, count in zip(unique_labels, label_counts):\n",
    "#     print(f\"Label {label}: {count} occurrences\")\n",
    "# print(\"------------------------\")\n",
    "\n",
    "# unique_labels, label_counts = np.unique(Y_train, return_counts=True)\n",
    "\n",
    "# # Print the counts for each unique label\n",
    "# for label, count in zip(unique_labels, label_counts):\n",
    "#     print(f\"Label {label}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4795946",
   "metadata": {},
   "source": [
    "2nd version (feature engineered u', class weighting, data resampling, cost-sensitive learning, regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebb893e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 5960 occurrences\n",
      "Label 1: 5960 occurrences\n",
      "Label 2: 5960 occurrences\n",
      "Label 3: 5960 occurrences\n",
      "Label 4: 5960 occurrences\n",
      "Label 5: 5960 occurrences\n",
      "Label 6: 5960 occurrences\n",
      "torch.Size([41720, 216]) torch.Size([41720])\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "num_instances = len(concatenated_representation)\n",
    "input_dim = concatenated_representation.shape[1]\n",
    "num_classes = 7\n",
    "\n",
    "# Rescale input features\n",
    "# selected_features = concatenated_representation / np.linalg.norm(concatenated_representation, axis=1, keepdims=True)\n",
    "\n",
    "# Apply data resampling (oversampling) to balance class distribution\n",
    "X_train, Y_train = oversample_data(concatenated_representation, y_train)\n",
    "\n",
    "# Calculate class weights for class weighting\n",
    "class_counts = np.bincount(y_train)\n",
    "total_instances = np.sum(class_counts)\n",
    "# class_weights = torch.tensor([total_instances / (num_classes * count) for count in class_counts], dtype=torch.float32)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "# print(X_train_tensor.shape, Y_train_tensor.shape)\n",
    "# X_train_tensor = torch.tensor(selected_features)\n",
    "# Y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "unique_labels, label_counts = np.unique(Y_train, return_counts=True)\n",
    "\n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    print(f\"Label {label}: {count} occurrences\")\n",
    "\n",
    "print(X_train_tensor.shape, Y_train_tensor.shape)\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "# Define batch size for DataLoader\n",
    "batch_size = 1\n",
    "\n",
    "# Create a PyTorch DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = MyNetwork(input_dim, 50, num_classes)\n",
    "# print(class_weights)\n",
    "# Define loss function and optimizer with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=None)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a71c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 1.5077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 1.4350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Loss: 1.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Loss: 1.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 1.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 100\n",
    "print_interval = 20  # Print tqdm every 30 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    counter = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        counter += 1\n",
    "    \n",
    "    # Print average loss per epoch\n",
    "    if (epoch + 1) % print_interval == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b435c98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.6487\n",
      "F1 Score for Class anger: 0.6141\n",
      "F1 Score for Class disgust: 0.7808\n",
      "F1 Score for Class fear: 0.7850\n",
      "F1 Score for Class joy: 0.5539\n",
      "F1 Score for Class neutral: 0.4701\n",
      "F1 Score for Class sadness: 0.6773\n",
      "F1 Score for Class surprise: 0.6597\n",
      "Label anger: 5153 occurrences\n",
      "Label disgust: 4908 occurrences\n",
      "Label fear: 4929 occurrences\n",
      "Label joy: 6288 occurrences\n",
      "Label neutral: 9380 occurrences\n",
      "Label sadness: 5098 occurrences\n",
      "Label surprise: 5964 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict on the training data\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Calculate F1 score per class\n",
    "f1_per_class = f1_score(Y_train_tensor, predicted, average=None)\n",
    "f1 = f1_score(Y_train_tensor, predicted, average='macro')\n",
    "\n",
    "print(f'Training F1 Score: {f1:.4f}')\n",
    "\n",
    "unique_labels, label_counts = np.unique(predicted, return_counts=True)\n",
    "\n",
    "# Print F1 score for each class\n",
    "for i, f1 in enumerate(f1_per_class):\n",
    "    print(f'F1 Score for Class {label_decoder[i]}: {f1:.4f}')\n",
    "    \n",
    "# Print the counts for each unique label\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    print(f\"Label {label_decoder[label]}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19c5f4",
   "metadata": {},
   "source": [
    "3rd version is 2nd version + ensembled FC classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
