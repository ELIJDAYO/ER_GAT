{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from graph_context_dataset import FeatureEngineeredDataset\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import random\n",
    "from model import FCClassifier, DATASET_PATH\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b22dd",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "\n",
    " - dataset_original\n",
    " - dataset_drop_noise\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1470c3",
   "metadata": {},
   "source": [
    "<h3> Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a68406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4fb045",
   "metadata": {
    "code_folding": [
     0,
     9,
     18,
     21,
     24,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# class FCLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(FCLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# class ActivationLayer(nn.Module):\n",
    "#     def __init__(self, activation_fn):\n",
    "#         super(ActivationLayer, self).__init__()\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.activation_fn(x)\n",
    "#         return x\n",
    "\n",
    "# def tanh(x):\n",
    "#     return torch.tanh(x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return torch.sigmoid(x)\n",
    "# # loss function and its derivative\n",
    "# def mse(y_true, y_pred):\n",
    "#     return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "# def mse_prime(y_true, y_pred):\n",
    "#     return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246bf76e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oversample_data(X_train, Y_train, num_classes):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            # Calculate the number of instances to oversample for this class\n",
    "            num_to_oversample = max_class_count - len(indices)\n",
    "            # Randomly select instances with replacement to oversample\n",
    "            oversampled_indices = np.random.choice(indices, size=num_to_oversample, replace=True)\n",
    "            # Append the oversampled instances to the original data\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return torch.tensor(X_train), torch.tensor(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8349606b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    if not tensor_list:\n",
    "        raise ValueError(\"The tensor list is empty\")\n",
    "\n",
    "    feature_dim = tensor_list[0].shape[1]\n",
    "    for tensor in tensor_list:\n",
    "        if tensor.shape[1] != feature_dim:\n",
    "            raise ValueError(\"All tensors must have the same feature dimension\")\n",
    "    \n",
    "    concatenated_tensor = torch.cat(tensor_list, dim=0)\n",
    "    \n",
    "    return concatenated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503aae",
   "metadata": {},
   "source": [
    "<h4> Import labels and label decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/labels_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_dev.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_val = pickle.load(file)\n",
    "y_val = torch.tensor(y_val)\n",
    "    \n",
    "file_path = 'data/dump/' + dataset_path + '/label_decoder.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3e716",
   "metadata": {},
   "source": [
    "<h4> Import the BERT base-node outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c53ba",
   "metadata": {},
   "source": [
    "first we disregard the u' and directly train the h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396cc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_train.pkl\",\n",
    "]\n",
    "\n",
    "test_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_test.pkl\",\n",
    "]\n",
    "\n",
    "val_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_BERT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_DGCN_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv1_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_GATv2_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT_RGAT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_dev.pkl\",\n",
    "]\n",
    "\n",
    "dictKey = {\n",
    "    0 : 'bert',\n",
    "    1 : 'bert-select-few',\n",
    "    2 : 'bert-select-mod',\n",
    "    3 : 'bert-select-more',\n",
    "    4 : 'dgcn',\n",
    "    5 : 'dgcn-select',\n",
    "    6 : 'gatv1',\n",
    "    7 : 'gatv1-select',\n",
    "    8 : 'gatv1-edge',\n",
    "    9 : 'gatv1-edge-select',\n",
    "    10 : 'gatv2-edge',\n",
    "    11 : 'gatv2-edge-select',\n",
    "    12 : 'rgat',\n",
    "    13 : 'rgat-select',\n",
    "    14 : 'egat',\n",
    "    15 : 'egat-select',\n",
    "    16 : 'bert-select-mod-dgcn',\n",
    "    17 : 'bert-select-mod-gatv1',\n",
    "    18 : 'bert-select-mod-gatv1-edge',\n",
    "    19 : 'bert-select-mod-gatv2-edge',\n",
    "    20 : 'bert-select-mod-rgat',\n",
    "    21 : 'bert-select-mod-egat',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff1721",
   "metadata": {},
   "source": [
    "<h4> Getting BERT and GAT outputs for all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0c4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeaturesList = []\n",
    "testFeaturesList = []\n",
    "valFeaturesList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3426c4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    return torch.cat(tensor_list, dim=0)\n",
    "\n",
    "for file_path in train_file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "#         print(type(data))\n",
    "#         if isinstance(data, list):\n",
    "#             print(\"instance of list, \", data[0].shape)\n",
    "#         else:\n",
    "#             print(\"instance of tensor, \", data.shape)\n",
    "        if file_path != train_file_paths[-1]: \n",
    "            trainFeaturesList.append(concatenate_tensors(data))\n",
    "        else:\n",
    "            trainFeaturesList.append(data)\n",
    "            \n",
    "for file_path in test_file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        if file_path != test_file_paths[-1]: \n",
    "            testFeaturesList.append(concatenate_tensors(data))\n",
    "        else:\n",
    "            testFeaturesList.append(data)\n",
    "            \n",
    "for file_path in val_file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        if file_path != val_file_paths[-1]: \n",
    "            valFeaturesList.append(concatenate_tensors(data))\n",
    "        else:\n",
    "            valFeaturesList.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e174164",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "479a3b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a070ce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43fa315",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "<h3> Feature Selection and creating data combination for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Define select feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6501b577",
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def get_norm_features(encoded_features):\n",
    "    scaler = MinMaxScaler()\n",
    "#       \"FeatureSelected+BERT+GAT: \", concatenatedRepresentationTrain2.shape, \"\\n\",\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "    return torch.tensor(features_scaled)\n",
    "\n",
    "def get_selected_features(encoded_features, labels, top_n):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(encoded_features)\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=100)\n",
    "\n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        # Create a binary mask indicating instances belonging to the current class\n",
    "        mask = (labels == label)\n",
    "\n",
    "        # SelectKBest with chi2 as the scoring function\n",
    "        selector = SelectKBest(score_func=chi2, k=top_n)  # Select top 20 features\n",
    "        selector.fit(features_scaled, mask)  # Fit SelectKBest to the data\n",
    "        # Get the indices of the top 20 features\n",
    "        top_features_indices = np.argsort(selector.scores_)[-top_n:]\n",
    "        scores = selector.scores_[top_features_indices]\n",
    "        # Store the indices in the dictionary\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "    # Select the desired features\n",
    "    selected_features = encoded_features[:, concatenated_features_indices]\n",
    "#     print(selected_features.shape)\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d79a94",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efccc17d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca73f7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Now prepare the data that will be ued to train the classifier, there are 20 combinations. And pick top 7 combinations yielding top F1 weighted-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61905fb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "trainList = []\n",
    "testList = []\n",
    "valList = []\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/trainList.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/testList.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/valList.pkl\"\n",
    "\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3: \n",
    "    with open(file_path1, \"rb\") as file:\n",
    "        trainList = pickle.load(file)\n",
    "    with open(file_path2, \"rb\") as file:\n",
    "        testList = pickle.load(file)\n",
    "    with open(file_path3, \"rb\") as file:\n",
    "        valList = pickle.load(file)\n",
    "else:\n",
    "    trainFeaturesList.append(data)\n",
    "    #1\n",
    "    trainList.append(trainFeaturesList[0])\n",
    "    testList.append(testFeaturesList[0])\n",
    "    valList.append(valFeaturesList[0])\n",
    "    #2\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 16)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #3\n",
    "    selectedTrainFeatures1b, indicesFeatures1b = get_selected_features(trainFeaturesList[0], y_train, 32)\n",
    "    selectedTestFeatures1b = testFeaturesList[0][:, indicesFeatures1b]\n",
    "    selectedValFeatures1b = valFeaturesList[0][:, indicesFeatures1b]\n",
    "    trainList.append(selectedTrainFeatures1b)\n",
    "    testList.append(selectedTestFeatures1b)\n",
    "    valList.append(selectedValFeatures1b)\n",
    "    #4\n",
    "    selectedTrainFeatures1c, indicesFeatures1c = get_selected_features(trainFeaturesList[0], y_train, 64)\n",
    "    selectedTestFeatures1c = testFeaturesList[0][:, indicesFeatures1c]\n",
    "    selectedValFeatures1c = valFeaturesList[0][:, indicesFeatures1c]\n",
    "    trainList.append(selectedTrainFeatures1c)\n",
    "    testList.append(selectedTestFeatures1c)\n",
    "    valList.append(selectedValFeatures1c)\n",
    "    #5\n",
    "    trainList.append(trainFeaturesList[1])\n",
    "    testList.append(testFeaturesList[1])\n",
    "    valList.append(valFeaturesList[1])\n",
    "    #6\n",
    "    selectedTrainFeatures2, indicesFeatures2 = get_selected_features(trainFeaturesList[1], y_train, 12)\n",
    "    selectedTestFeatures2 = testFeaturesList[1][:, indicesFeatures2]\n",
    "    selectedValFeatures2 = valFeaturesList[1][:, indicesFeatures2]\n",
    "    trainList.append(selectedTrainFeatures2)\n",
    "    testList.append(selectedTestFeatures2)\n",
    "    valList.append(selectedValFeatures2)\n",
    "    #7\n",
    "    trainList.append(trainFeaturesList[2])\n",
    "    testList.append(testFeaturesList[2])\n",
    "    valList.append(valFeaturesList[2])\n",
    "    #8\n",
    "    selectedTrainFeatures3, indicesFeatures3 = get_selected_features(trainFeaturesList[2], y_train, 12)\n",
    "    selectedTestFeatures3 = testFeaturesList[2][:, indicesFeatures3]\n",
    "    selectedValFeatures3 = valFeaturesList[2][:, indicesFeatures3]\n",
    "    trainList.append(selectedTrainFeatures3)\n",
    "    testList.append(selectedTestFeatures3)\n",
    "    valList.append(selectedValFeatures3)\n",
    "    #9\n",
    "    trainList.append(trainFeaturesList[3])\n",
    "    testList.append(testFeaturesList[3])\n",
    "    valList.append(valFeaturesList[3])\n",
    "    #10\n",
    "    selectedTrainFeatures4, indicesFeatures4 = get_selected_features(trainFeaturesList[3], y_train, 12)\n",
    "    selectedTestFeatures4 = testFeaturesList[3][:, indicesFeatures4]\n",
    "    selectedValFeatures4 = valFeaturesList[3][:, indicesFeatures4]\n",
    "    trainList.append(selectedTrainFeatures4)\n",
    "    testList.append(selectedTestFeatures4)\n",
    "    valList.append(selectedValFeatures4)\n",
    "    #11\n",
    "    trainList.append(trainFeaturesList[4])\n",
    "    testList.append(testFeaturesList[4])\n",
    "    valList.append(valFeaturesList[4])\n",
    "    #12\n",
    "    selectedTrainFeatures5, indicesFeatures5 = get_selected_features(trainFeaturesList[4], y_train, 12)\n",
    "    selectedTestFeatures5 = testFeaturesList[4][:, indicesFeatures5]\n",
    "    selectedValFeatures5 = valFeaturesList[4][:, indicesFeatures5]\n",
    "    trainList.append(selectedTrainFeatures5)\n",
    "    testList.append(selectedTestFeatures5)\n",
    "    valList.append(selectedValFeatures5)\n",
    "    #13\n",
    "    trainList.append(trainFeaturesList[5])\n",
    "    testList.append(testFeaturesList[5])\n",
    "    valList.append(valFeaturesList[5])\n",
    "    #14\n",
    "    selectedTrainFeatures6, indicesFeatures6 = get_selected_features(trainFeaturesList[5], y_train, 12)\n",
    "    selectedTestFeatures6 = testFeaturesList[5][:, indicesFeatures6]\n",
    "    selectedValFeatures6 = valFeaturesList[5][:, indicesFeatures6]\n",
    "    trainList.append(selectedTrainFeatures6)\n",
    "    testList.append(selectedTestFeatures6)\n",
    "    valList.append(selectedValFeatures6)\n",
    "    #15\n",
    "    trainList.append(trainFeaturesList[6])\n",
    "    testList.append(testFeaturesList[6])\n",
    "    valList.append(valFeaturesList[6])\n",
    "    #16\n",
    "    selectedTrainFeatures7, indicesFeatures7 = get_selected_features(trainFeaturesList[6][0], y_train, 12)\n",
    "    selectedTestFeatures7 = testFeaturesList[6][0][:, indicesFeatures7]\n",
    "    selectedValFeatures7 = testFeaturesList[6][0][:, indicesFeatures7]\n",
    "    trainList.append(selectedTrainFeatures7)\n",
    "    testList.append(selectedTestFeatures7)\n",
    "    valList.append(selectedValFeatures7)\n",
    "    selectedNormTrainFeatures1 = get_norm_features(selectedTrainFeatures1b)\n",
    "    selectedNormTestFeatures1 = get_norm_features(selectedTestFeatures1b)\n",
    "    selectedNormValFeatures1 = get_norm_features(selectedValFeatures1b)\n",
    "\n",
    "    #17\n",
    "    trainNormFeatures2 = get_norm_features(trainFeaturesList[1].detach().numpy())\n",
    "    testNormFeatures2 = get_norm_features(testFeaturesList[1].detach().numpy())\n",
    "    valNormFeatures2 = get_norm_features(valFeaturesList[1].detach().numpy())\n",
    "    concatenatedTrainFeatures2 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures2), dim=1)\n",
    "    concatenatedTestFeatures2 = torch.cat((selectedNormTestFeatures1, testNormFeatures2), dim=1)\n",
    "    concatenatedValFeatures2 = torch.cat((selectedNormValFeatures1, valNormFeatures2), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures2)\n",
    "    testList.append(concatenatedTestFeatures2)\n",
    "    valList.append(concatenatedValFeatures2)\n",
    "    #18\n",
    "    trainNormFeatures3 = get_norm_features(trainFeaturesList[2].detach().numpy())\n",
    "    testNormFeatures3 = get_norm_features(testFeaturesList[2].detach().numpy())\n",
    "    valNormFeatures3 = get_norm_features(valFeaturesList[2].detach().numpy())\n",
    "    concatenatedTrainFeatures3 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures3), dim=1)\n",
    "    concatenatedTestFeatures3 = torch.cat((selectedNormTestFeatures1, testNormFeatures3), dim=1)\n",
    "    concatenatedValFeatures3 = torch.cat((selectedNormValFeatures1, valNormFeatures3), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures3)\n",
    "    testList.append(concatenatedTestFeatures3)\n",
    "    valList.append(concatenatedValFeatures3)\n",
    "    #19\n",
    "    trainNormFeatures4 = get_norm_features(trainFeaturesList[3].detach().numpy())\n",
    "    testNormFeatures4 = get_norm_features(testFeaturesList[3].detach().numpy())\n",
    "    valNormFeatures4 = get_norm_features(valFeaturesList[3].detach().numpy())\n",
    "    concatenatedTrainFeatures4 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures4), dim=1)\n",
    "    concatenatedTestFeatures4 = torch.cat((selectedNormTestFeatures1, testNormFeatures4), dim=1)\n",
    "    concatenatedValFeatures4 = torch.cat((selectedNormValFeatures1, valNormFeatures4), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures4)\n",
    "    testList.append(concatenatedTestFeatures4)\n",
    "    valList.append(concatenatedValFeatures4)\n",
    "    #20\n",
    "    trainNormFeatures5 = get_norm_features(trainFeaturesList[4].detach().numpy())\n",
    "    testNormFeatures5 = get_norm_features(testFeaturesList[4].detach().numpy())\n",
    "    valNormFeatures5 = get_norm_features(valFeaturesList[4].detach().numpy())\n",
    "    concatenatedTrainFeatures5 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures5), dim=1)\n",
    "    concatenatedTestFeatures5 = torch.cat((selectedNormTestFeatures1, testNormFeatures5), dim=1)\n",
    "    concatenatedValFeatures5 = torch.cat((selectedNormValFeatures1, valNormFeatures5), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures5)\n",
    "    testList.append(concatenatedTestFeatures5)\n",
    "    valList.append(concatenatedValFeatures5)\n",
    "\n",
    "    #21\n",
    "    trainNormFeatures6 = get_norm_features(trainFeaturesList[5].detach().numpy())\n",
    "    testNormFeatures6 = get_norm_features(testFeaturesList[5].detach().numpy())\n",
    "    valNormFeatures6 = get_norm_features(valFeaturesList[5].detach().numpy())\n",
    "    concatenatedTrainFeatures6 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures6), dim=1)\n",
    "    concatenatedTestFeatures6 = torch.cat((selectedNormTestFeatures1, testNormFeatures6), dim=1)\n",
    "    concatenatedValFeatures6 = torch.cat((selectedNormValFeatures1, valNormFeatures6), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures6)\n",
    "    testList.append(concatenatedTestFeatures6)\n",
    "    valList.append(concatenatedValFeatures6)\n",
    "\n",
    "    #22\n",
    "    trainNormFeatures7 = get_norm_features(trainFeaturesList[6][0].detach().numpy())\n",
    "    testNormFeatures7 = get_norm_features(testFeaturesList[6][0].detach().numpy())\n",
    "    valNormFeatures7 = get_norm_features(valFeaturesList[6][0].detach().numpy())\n",
    "    concatenatedTrainFeatures7 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures7), dim=1)\n",
    "    concatenatedTestFeatures7 = torch.cat((selectedNormTestFeatures1, testNormFeatures7), dim=1)\n",
    "    concatenatedValFeatures7 = torch.cat((selectedNormValFeatures1, valNormFeatures7), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures7)\n",
    "    testList.append(concatenatedTestFeatures7)\n",
    "    valList.append(concatenatedValFeatures7)\n",
    "\n",
    "    with open(file_path1, 'wb') as file:\n",
    "        pickle.dump(trainList, file)\n",
    "    with open(file_path2, 'wb') as file:\n",
    "        pickle.dump(testList, file)\n",
    "    with open(file_path3, 'wb') as file:\n",
    "        pickle.dump(valList, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb893e7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def prep_data(features, labels, isOversample):\n",
    "    num_instances = len(features)\n",
    "    num_classes = 7\n",
    "\n",
    "    # Rescale input features\n",
    "    # selected_features = concatenated_representation / np.linalg.norm(concatenated_representation, axis=1, keepdims=True)\n",
    "\n",
    "    # Apply data resampling (oversampling) to balance class distribution\n",
    "    if isOversample:\n",
    "        X_set, Y_set = oversample_data(features, labels, num_classes)\n",
    "    else:\n",
    "        X_set, Y_set = features, labels\n",
    "\n",
    "    # Calculate class weights for class weighting\n",
    "#     class_counts = np.bincount(labels)\n",
    "#     total_instances = np.sum(class_counts)\n",
    "    # class_weights = torch.tensor([total_instances / (num_classes * count) for count in class_counts], dtype=torch.float32)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
    "    Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
    "    # print(X_train_tensor.shape, Y_train_tensor.shape)\n",
    "    # X_train_tensor = torch.tensor(selected_features)\n",
    "    # Y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(Y_set, return_counts=True)\n",
    "\n",
    "    # Print the counts for each unique label\n",
    "#     for label, count in zip(unique_labels, label_counts):\n",
    "#         print(f\"Label {label_decoder[label]}: {count} occurrences\")\n",
    "\n",
    "#     print(X_tensor.shape, Y_tensor.shape)\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a71c886",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train1(X_set, Y_set, num_epochs=20, batch_size=32, loss_difference_threshold=0.01, \n",
    "                 hidden_dims=[256, 128], dropout_rate=0.5, lr=0.0001, optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss):\n",
    "    output_dim = 7  # Number of classes\n",
    "    model = MyNetwork(len(X_set[0]), hidden_dims, output_dim, dropout_rate)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    print_interval = 1  # Print tqdm every epoch\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_set, Y_set)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    epoch_num = num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.float()  # Ensure inputs are float32\n",
    "                labels = labels.long()   # Ensure labels are long\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "        if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "            epoch_num = epoch\n",
    "            break\n",
    "\n",
    "        previous_loss = epoch_loss\n",
    "\n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "635c4f57",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train2(X_set, y_set, num_epochs=20, batch_size=32, early_stopping_threshold=0.01,\n",
    "                 hidden_dim=128, dropout_prob=0.5, learning_rate=0.0005, optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss):\n",
    "    input_dim = len(X_set[0])  # Size of the input features\n",
    "    output_dim = 7  # Number of classes\n",
    "\n",
    "    model = FCClassifier(input_dim, hidden_dim, output_dim, dropout_prob)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    dataset = TensorDataset(X_set, y_set)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    epoch_losses = []  # List to store loss values for each epoch\n",
    "    epoch_num = num_epochs\n",
    "    # Training loop\n",
    "    with tqdm(total=num_epochs, unit=\"epoch\", desc=\"Training\") as tepoch:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_features, batch_labels in dataloader:\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update running loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Calculate and store average loss for the epoch\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_losses.append(epoch_loss)\n",
    "\n",
    "            # Update tqdm description\n",
    "            tepoch.set_postfix(loss=epoch_loss)\n",
    "            tepoch.update()\n",
    "\n",
    "            # Check for early stopping\n",
    "            if epoch > 0 and abs(epoch_losses[-2] - epoch_losses[-1]) < early_stopping_threshold:\n",
    "                epoch_num = epoch\n",
    "                break\n",
    "\n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b435c98b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_emotions(model, X_tensor, Y_tensor, typeSet, isSimpleFC, i_dict):\n",
    "    # Set the model to evaluation mode\n",
    "    if X_tensor.dtype != torch.float32:\n",
    "        X_tensor = X_tensor.float()\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    # Predict on the data\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convert predicted tensor to numpy array\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    Y_tensor = Y_tensor.cpu().numpy()\n",
    "\n",
    "    # Calculate classification report\n",
    "    report = classification_report(Y_tensor, predicted, target_names=label_decoder.values(), output_dict=True, zero_division=0)\n",
    "\n",
    "    # Extract metrics\n",
    "    accuracy = report['accuracy']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    f1_micro = report.get('micro avg', {}).get('f1-score', accuracy)\n",
    "    f1_macro = report.get('macro avg', {}).get('f1-score', 0.0) \n",
    "    \n",
    "    if typeSet == \"validation\":\n",
    "        print(\"Classified: \", dictKey[i_dict])\n",
    "    \n",
    "    return dictKey[i_dict], typeSet, isSimpleFC, accuracy, recall, weighted_f1, f1_micro, f1_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52d8752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b35df7e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|███████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 121.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 768])\n",
      "1 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 90])\n",
      "2 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 171])\n",
      "3 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 304])\n",
      "4 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 64])\n",
      "5 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 34])\n",
      "6 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 64])\n",
      "7 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 41])\n",
      "8 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 64])\n",
      "9 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 39])\n",
      "10 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 64])\n",
      "11 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 39])\n",
      "12 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 64])\n",
      "13 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 44])\n",
      "14 <class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 12176, 64])\n",
      "15 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 38])\n",
      "16 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 235])\n",
      "17 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 235])\n",
      "18 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 235])\n",
      "19 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 235])\n",
      "20 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 235])\n",
      "21 <class 'torch.Tensor'>\n",
      "torch.Size([12176, 235])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for trainSet, testSet, valSet in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "    print(i, type(trainSet))\n",
    "    if isinstance(trainSet, list):\n",
    "        print(type(trainSet[0]))\n",
    "        sample = trainSet[0]\n",
    "        print(sample.shape)\n",
    "    else:\n",
    "        print(trainSet.squeeze(0).shape)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e99ea",
   "metadata": {},
   "source": [
    "<b> This is where value for isSimpleFC is decided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61a58505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return torch.tensor(data)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c6a93e4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:  24%|█████████████▉                                            | 12/50 [00:55<02:55,  4.61s/epoch, loss=1.32]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   8%|████▋                                                      | 4/50 [00:07<01:30,  1.97s/epoch, loss=1.46]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███████                                                    | 6/50 [00:15<01:53,  2.58s/epoch, loss=1.42]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:  10%|█████▉                                                     | 5/50 [00:15<02:16,  3.03s/epoch, loss=1.42]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   8%|████▋                                                      | 4/50 [00:07<01:29,  1.94s/epoch, loss=1.54]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:05<01:29,  1.90s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:05<01:23,  1.77s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:04<01:15,  1.61s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|███▌                                                       | 3/50 [00:05<01:21,  1.73s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:05<01:19,  1.70s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:05<01:22,  1.75s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:04<01:16,  1.63s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:05<01:19,  1.69s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   6%|███▌                                                       | 3/50 [00:05<01:23,  1.78s/epoch, loss=1.55]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|████▋                                                      | 4/50 [00:08<01:33,  2.03s/epoch, loss=1.52]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   8%|████▋                                                      | 4/50 [00:07<01:20,  1.75s/epoch, loss=1.51]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   8%|████▋                                                      | 4/50 [00:15<02:55,  3.81s/epoch, loss=1.52]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:  12%|███████▏                                                    | 6/50 [00:22<02:42,  3.70s/epoch, loss=1.5]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:   8%|████▋                                                      | 4/50 [00:14<02:48,  3.66s/epoch, loss=1.52]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:  10%|█████▉                                                     | 5/50 [00:18<02:46,  3.69s/epoch, loss=1.51]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▉                                                     | 5/50 [00:18<02:43,  3.64s/epoch, loss=1.51]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n",
      "Training:  10%|█████▉                                                     | 5/50 [00:18<02:42,  3.62s/epoch, loss=1.51]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X_set.clone().detach(), dtype=torch.float32).clone().detach()\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_9496\\2313527096.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y_set.clone().detach(), dtype=torch.long).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        df_results_sorted = pickle.load(file)\n",
    "else:\n",
    "    results = []\n",
    "    num_epochs = 50\n",
    "    batch_size = 8\n",
    "    i = 0\n",
    "\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(trainSet.clone().detach(), y_train, False)\n",
    "#         deepFC\n",
    "        model, _ = model_train1(X_tensor, Y_tensor, num_epochs, batch_size)\n",
    "        result = classify_emotions(model, X_tensor.clone().detach(), Y_tensor.clone().detach(), 'train', False, i)\n",
    "#         results.append(result)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(testSet.clone().detach(), y_test, False)\n",
    "        result = classify_emotions(model, X_tensor.clone().detach(), Y_tensor.clone().detach(), 'test', False, i)\n",
    "        results.append(result)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(trainSet, y_train, False)\n",
    "#         simpleFC\n",
    "        model, _ = model_train2(X_tensor, Y_tensor, num_epochs, batch_size)\n",
    "\n",
    "        result = classify_emotions(model, X_tensor, Y_tensor, 'train', True, i)\n",
    "#         results.append(result)\n",
    "\n",
    "        X_tensor, Y_tensor = prep_data(testSet, y_test, False)\n",
    "        result = classify_emotions(model, X_tensor, Y_tensor, 'test', True, i)\n",
    "        results.append(result)\n",
    "        i += 1\n",
    "\n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "416f7968",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# tmp\n",
    "# columns = list(df_results_sorted.columns)\n",
    "\n",
    "# # Modify the 2nd and 3rd column names\n",
    "# columns[1] = 'typeSet'\n",
    "# columns[2] = 'isSimpleFC'\n",
    "\n",
    "# # Assign the new column names back to the DataFrame\n",
    "# df_results_sorted.columns = columns\n",
    "# df_results_sorted[\"typeSet\"] = \"test\"\n",
    "# with open(file_path, 'wb') as file:\n",
    "#     pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed1d0610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.510217</td>\n",
       "      <td>0.510217</td>\n",
       "      <td>0.407871</td>\n",
       "      <td>0.510217</td>\n",
       "      <td>0.203370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.501858</td>\n",
       "      <td>0.501858</td>\n",
       "      <td>0.389807</td>\n",
       "      <td>0.501858</td>\n",
       "      <td>0.180678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.500619</td>\n",
       "      <td>0.500619</td>\n",
       "      <td>0.382530</td>\n",
       "      <td>0.500619</td>\n",
       "      <td>0.172942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.495666</td>\n",
       "      <td>0.495666</td>\n",
       "      <td>0.380247</td>\n",
       "      <td>0.495666</td>\n",
       "      <td>0.173107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.499071</td>\n",
       "      <td>0.499071</td>\n",
       "      <td>0.379425</td>\n",
       "      <td>0.499071</td>\n",
       "      <td>0.171555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.489474</td>\n",
       "      <td>0.489474</td>\n",
       "      <td>0.346339</td>\n",
       "      <td>0.489474</td>\n",
       "      <td>0.135263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>bert-select-mod-gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>0.314911</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>0.101491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477709</td>\n",
       "      <td>0.477709</td>\n",
       "      <td>0.311506</td>\n",
       "      <td>0.477709</td>\n",
       "      <td>0.095291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>egat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477709</td>\n",
       "      <td>0.477709</td>\n",
       "      <td>0.310341</td>\n",
       "      <td>0.477709</td>\n",
       "      <td>0.094153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gatv2-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gatv1-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gatv1-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rgat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bert-select-mod-dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bert-select-mod-gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bert-select-mod-gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bert-select-mod-rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gatv1-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dgcn-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>egat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gatv1-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gatv2-edge-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rgat-select</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert-select-mod-dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert-select-mod-gatv1</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert-select-mod-gatv1-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert-select-mod-gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bert-select-mod-rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.092284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.476780</td>\n",
       "      <td>0.476780</td>\n",
       "      <td>0.308058</td>\n",
       "      <td>0.476780</td>\n",
       "      <td>0.092243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dgcn-select</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.476780</td>\n",
       "      <td>0.476780</td>\n",
       "      <td>0.308058</td>\n",
       "      <td>0.476780</td>\n",
       "      <td>0.092243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              data_combination typeSet  isSimpleFC  Accuracy    Recall  \\\n",
       "22                        bert    test        True  0.510217  0.510217   \n",
       "24             bert-select-mod    test        True  0.501858  0.501858   \n",
       "0                         bert    test       False  0.500619  0.500619   \n",
       "23             bert-select-few    test        True  0.495666  0.495666   \n",
       "25            bert-select-more    test        True  0.499071  0.499071   \n",
       "3             bert-select-more    test       False  0.489474  0.489474   \n",
       "39       bert-select-mod-gatv1    test        True  0.479876  0.479876   \n",
       "2              bert-select-mod    test       False  0.477709  0.477709   \n",
       "37                 egat-select    test        True  0.477709  0.477709   \n",
       "6                        gatv1    test       False  0.477090  0.477090   \n",
       "33           gatv2-edge-select    test        True  0.477090  0.477090   \n",
       "28                       gatv1    test        True  0.477090  0.477090   \n",
       "29                gatv1-select    test        True  0.477090  0.477090   \n",
       "30                  gatv1-edge    test        True  0.477090  0.477090   \n",
       "31           gatv1-edge-select    test        True  0.477090  0.477090   \n",
       "32                  gatv2-edge    test        True  0.477090  0.477090   \n",
       "35                 rgat-select    test        True  0.477090  0.477090   \n",
       "34                        rgat    test        True  0.477090  0.477090   \n",
       "36                        egat    test        True  0.477090  0.477090   \n",
       "38        bert-select-mod-dgcn    test        True  0.477090  0.477090   \n",
       "40  bert-select-mod-gatv1-edge    test        True  0.477090  0.477090   \n",
       "41  bert-select-mod-gatv2-edge    test        True  0.477090  0.477090   \n",
       "42        bert-select-mod-rgat    test        True  0.477090  0.477090   \n",
       "7                 gatv1-select    test       False  0.477090  0.477090   \n",
       "5                  dgcn-select    test       False  0.477090  0.477090   \n",
       "4                         dgcn    test       False  0.477090  0.477090   \n",
       "15                 egat-select    test       False  0.477090  0.477090   \n",
       "8                   gatv1-edge    test       False  0.477090  0.477090   \n",
       "9            gatv1-edge-select    test       False  0.477090  0.477090   \n",
       "10                  gatv2-edge    test       False  0.477090  0.477090   \n",
       "11           gatv2-edge-select    test       False  0.477090  0.477090   \n",
       "12                        rgat    test       False  0.477090  0.477090   \n",
       "13                 rgat-select    test       False  0.477090  0.477090   \n",
       "14                        egat    test       False  0.477090  0.477090   \n",
       "16        bert-select-mod-dgcn    test       False  0.477090  0.477090   \n",
       "17       bert-select-mod-gatv1    test       False  0.477090  0.477090   \n",
       "18  bert-select-mod-gatv1-edge    test       False  0.477090  0.477090   \n",
       "19  bert-select-mod-gatv2-edge    test       False  0.477090  0.477090   \n",
       "20        bert-select-mod-rgat    test       False  0.477090  0.477090   \n",
       "21        bert-select-mod-egat    test       False  0.477090  0.477090   \n",
       "1              bert-select-few    test       False  0.477090  0.477090   \n",
       "43        bert-select-mod-egat    test        True  0.477090  0.477090   \n",
       "26                        dgcn    test        True  0.476780  0.476780   \n",
       "27                 dgcn-select    test        True  0.476780  0.476780   \n",
       "\n",
       "    Weighted-F1  F1-micro  F1-macro  \n",
       "22     0.407871  0.510217  0.203370  \n",
       "24     0.389807  0.501858  0.180678  \n",
       "0      0.382530  0.500619  0.172942  \n",
       "23     0.380247  0.495666  0.173107  \n",
       "25     0.379425  0.499071  0.171555  \n",
       "3      0.346339  0.489474  0.135263  \n",
       "39     0.314911  0.479876  0.101491  \n",
       "2      0.311506  0.477709  0.095291  \n",
       "37     0.310341  0.477709  0.094153  \n",
       "6      0.308193  0.477090  0.092284  \n",
       "33     0.308193  0.477090  0.092284  \n",
       "28     0.308193  0.477090  0.092284  \n",
       "29     0.308193  0.477090  0.092284  \n",
       "30     0.308193  0.477090  0.092284  \n",
       "31     0.308193  0.477090  0.092284  \n",
       "32     0.308193  0.477090  0.092284  \n",
       "35     0.308193  0.477090  0.092284  \n",
       "34     0.308193  0.477090  0.092284  \n",
       "36     0.308193  0.477090  0.092284  \n",
       "38     0.308193  0.477090  0.092284  \n",
       "40     0.308193  0.477090  0.092284  \n",
       "41     0.308193  0.477090  0.092284  \n",
       "42     0.308193  0.477090  0.092284  \n",
       "7      0.308193  0.477090  0.092284  \n",
       "5      0.308193  0.477090  0.092284  \n",
       "4      0.308193  0.477090  0.092284  \n",
       "15     0.308193  0.477090  0.092284  \n",
       "8      0.308193  0.477090  0.092284  \n",
       "9      0.308193  0.477090  0.092284  \n",
       "10     0.308193  0.477090  0.092284  \n",
       "11     0.308193  0.477090  0.092284  \n",
       "12     0.308193  0.477090  0.092284  \n",
       "13     0.308193  0.477090  0.092284  \n",
       "14     0.308193  0.477090  0.092284  \n",
       "16     0.308193  0.477090  0.092284  \n",
       "17     0.308193  0.477090  0.092284  \n",
       "18     0.308193  0.477090  0.092284  \n",
       "19     0.308193  0.477090  0.092284  \n",
       "20     0.308193  0.477090  0.092284  \n",
       "21     0.308193  0.477090  0.092284  \n",
       "1      0.308193  0.477090  0.092284  \n",
       "43     0.308193  0.477090  0.092284  \n",
       "26     0.308058  0.476780  0.092243  \n",
       "27     0.308058  0.476780  0.092243  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e36b6",
   "metadata": {},
   "source": [
    "<h4> Select top 10 unique data combinations then tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36973bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58e8a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1 (isSimpleFC=False): []\n",
      "Combination 2 (isSimpleFC=True): ['bert', 'bert-select-more', 'bert-select-mod', 'bert-select-few', 'bert-select-mod-gatv1-edge', 'bert-select-mod-gatv1', 'bert-select-mod-rgat', 'bert-select-mod-gatv2-edge', 'dgcn', 'dgcn-select']\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 10\n",
    "counter = 0\n",
    "combination1 = []\n",
    "combination2 = []\n",
    "seen_combinations = set()\n",
    "\n",
    "for idx, row in df_results_sorted.iterrows():\n",
    "    if counter >= max_iterations:\n",
    "        break\n",
    "\n",
    "    if row['data_combination'] in seen_combinations:\n",
    "        continue\n",
    "\n",
    "    if row['isSimpleFC']:\n",
    "        combination2.append(row['data_combination'])\n",
    "    else:\n",
    "        combination1.append(row['data_combination'])\n",
    "\n",
    "    seen_combinations.add(row['data_combination'])\n",
    "    counter += 1\n",
    "\n",
    "# Display the results\n",
    "print(\"Combination 1 (isSimpleFC=False):\", combination1)\n",
    "print(\"Combination 2 (isSimpleFC=True):\", combination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54513411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices for isSimpleFC=False: []\n",
      "Indices for isSimpleFC=True: [0, 1, 2, 3, 4, 5, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "indices1 = [key for key, value in dictKey.items() if value in combination1]\n",
    "indices2 = [key for key, value in dictKey.items() if value in combination2]\n",
    "\n",
    "print(\"Indices for isSimpleFC=False:\", indices1)\n",
    "print(\"Indices for isSimpleFC=True:\", indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62a5e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainDeepList = [trainList[i] for i in indices1]\n",
    "selectedTestDeepList = [testList[i] for i in indices1]\n",
    "selectedValDeepList = [valList[i] for i in indices1]\n",
    "\n",
    "len(selectedTrainDeepList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ccae3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3a1b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainSet in selectedTrainList:\n",
    "#     print(type(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3fee3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedTrainList = [trainList[i] for i in indices2]\n",
    "selectedTestList = [testList[i] for i in indices2]\n",
    "selectedValList = [valList[i] for i in indices2]\n",
    "\n",
    "len(selectedTrainList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c266050",
   "metadata": {},
   "source": [
    "<h4> Tuning using random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da2dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should call both model_train1 and 2\n",
    "\n",
    "def objective_func(X_train, X_test, X_val, \n",
    "               y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC):\n",
    "    results = []\n",
    "    hyperparams_string = (\n",
    "        f'num_epochs={hyperparams[\"num_epochs\"]} '\n",
    "        f'batch_size={hyperparams[\"batch_size\"]} '\n",
    "        f'loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]} '\n",
    "        f'hidden_dims={hyperparams[\"hidden_dims\"]} '\n",
    "        f'dropout_rate={hyperparams[\"dropout_rate\"]} '\n",
    "        f'learning_rate={hyperparams[\"learning_rate\"]} '\n",
    "        f'optimizers={hyperparams[\"optimizers\"]} '\n",
    "        f'criteria={hyperparams[\"criteria\"]}'\n",
    "    )    \n",
    "    print(hyperparams_string)\n",
    "            \n",
    "#     X_tensor, Y_tensor = prep_data(X_train.clone().detach(), y_train, False)\n",
    "    X_train_tensor = to_tensor(X_train)\n",
    "    y_train_tensor = to_tensor(y_train).long()\n",
    "    X_val_tensor = to_tensor(X_val)\n",
    "    y_val_tensor = to_tensor(y_val).long()\n",
    "    X_test_tensor = to_tensor(X_test)\n",
    "    y_test_tensor = to_tensor(y_test).long()\n",
    "# train\n",
    "    start_time = time.time()\n",
    "    if isSimpleFC:\n",
    "        model, num_epoch = model_train2(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "                            hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "                            hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "                            hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])        \n",
    "    else:\n",
    "        model, num_epoch = model_train1(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "                            hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "                            hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "                            hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "# val\n",
    "#     X_tensor, Y_tensor = prep_data(X_val.clone().detach(), y_val, False)\n",
    "    result = classify_emotions(model, X_val_tensor, y_val_tensor, \\\n",
    "                               'validation', isSimpleFC, i_dict)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    result = list(result)\n",
    "    hyperparams_string = f'num_epochs={hyperparams[\"num_epochs\"]}-batch_size={hyperparams[\"batch_size\"]}-loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]}-hidden_dims={hyperparams[\"hidden_dims\"]}-dropout_rate={hyperparams[\"dropout_rate\"]}-learning_rate={hyperparams[\"learning_rate\"]}-optimizers={hyperparams[\"optimizers\"]}-criteria={hyperparams[\"criteria\"]}'\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "# test\n",
    "#     X_tensor, Y_tensor = prep_data(X_test.clone().detach(), y_test, False)\n",
    "    result = classify_emotions(model, X_test_tensor, y_test_tensor, \\\n",
    "                               'test', isSimpleFC, i_dict)\n",
    "    \n",
    "    result = list(result)\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='data_combination', ascending=False)\n",
    "    \n",
    "    return df_results_sorted\n",
    "\n",
    "\n",
    "# def objective_func(X_train, X_test, X_val, \n",
    "#                y_train, y_test, y_val, hyperparams, i_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f998054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X_train, X_test, X_val, \\\n",
    "                  y_train, y_test, y_val, \\\n",
    "                  param_grid, isSimpleFC, i_dict, MAX_EVALS = 15):\n",
    "    \n",
    "    sub_total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    \n",
    "    for i in range(MAX_EVALS):\n",
    "        hyperparams = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        try:\n",
    "            new_results = objective_func(X_train, X_test, X_val,  y_train, y_test, y_val,\n",
    "                                hyperparams, i_dict, isSimpleFC)\n",
    "            sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparams {hyperparams}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    return sub_total_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7295744e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'num_epochs': [50, 80, 120],\n",
    "    'batch_size': [1, 4, 32, 64],\n",
    "    'loss_difference_threshold': [0.01, 0.001],\n",
    "    'hidden_dims': [[256, 128], [128, 64], [64, 32]],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}\n",
    "param_grid2 = {\n",
    "    'num_epochs': [50, 80, 120],\n",
    "    'batch_size': [1, 4, 32, 64],\n",
    "    'loss_difference_threshold': [0.01, 0.001],\n",
    "    'hidden_dims': [128, 256, 512],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcf2b3",
   "metadata": {},
   "source": [
    "<h5> First find the best hyperparameter combination for the DeepClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "163a1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamTuning(X_trainSet, X_testSet, X_valSet, y_train, y_test, y_val, isSimpleFC, param_grid, indices):\n",
    "    total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    for i in range(len(indices)):\n",
    "        print(\"============ PART \", i, \"============\")\n",
    "        X_train = X_trainSet[i]\n",
    "        X_test = X_testSet[i]\n",
    "        X_val = X_valSet[i]\n",
    "\n",
    "        sub_total_results = random_search(X_train, X_test, X_val, y_train, y_test, y_val,\n",
    "                     param_grid, isSimpleFC, indices[i])\n",
    "        total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e1fd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/deep_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results1_sorted = pickle.load(file)\n",
    "else:\n",
    "    total_results1 = hyperparamTuning(selectedTrainDeepList, selectedTestDeepList, selectedValDeepList, \\\n",
    "                                 y_train, y_test, y_val, False, param_grid1, indices1)\n",
    "    \n",
    "    total_results1_sorted = total_results1.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results1_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90c5d2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [data_combination, typeSet, isSimpleFC, Accuracy, Recall, Weighted-F1, F1-micro, F1-macro, train_time, hyperparams, num_epoch]\n",
       "Index: []"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "\n",
    "total_results1_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6d089c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ PART  0 ============\n",
      "num_epochs=80 batch_size=64 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|██████████████████████████████████████████                | 58/80 [01:12<00:27,  1.26s/epoch, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert\n",
      "num_epochs=50 batch_size=1 loss_difference_threshold=0.01 hidden_dims=512 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█████████▌                                           | 9/50 [14:11<1:04:40, 94.65s/epoch, loss=-1.11e+8]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         total_results2_sorted \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m----> 8\u001b[0m     total_results2 \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparamTuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselectedTrainList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselectedTestList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselectedValList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     total_results2_sorted \u001b[38;5;241m=\u001b[39m total_results2\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeighted-F1\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36mhyperparamTuning\u001b[1;34m(X_trainSet, X_testSet, X_valSet, y_train, y_test, y_val, isSimpleFC, param_grid, indices)\u001b[0m\n\u001b[0;32m      7\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m X_testSet[i]\n\u001b[0;32m      8\u001b[0m     X_val \u001b[38;5;241m=\u001b[39m X_valSet[i]\n\u001b[1;32m---> 10\u001b[0m     sub_total_results \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misSimpleFC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     total_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([sub_total_results, total_results], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_results\n",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36mrandom_search\u001b[1;34m(X_train, X_test, X_val, y_train, y_test, y_val, param_grid, isSimpleFC, i_dict, MAX_EVALS)\u001b[0m\n\u001b[0;32m      9\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m {k: random\u001b[38;5;241m.\u001b[39msample(v, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m param_grid\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     new_results \u001b[38;5;241m=\u001b[39m \u001b[43mobjective_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misSimpleFC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     sub_total_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([sub_total_results, new_results], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36mobjective_func\u001b[1;34m(X_train, X_test, X_val, y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC)\u001b[0m\n\u001b[0;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isSimpleFC:\n\u001b[1;32m---> 28\u001b[0m     model, num_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_difference_threshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dims\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcriteria\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     model, num_epoch \u001b[38;5;241m=\u001b[39m model_train1(X_train_tensor, y_train_tensor, hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     34\u001b[0m                         hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_difference_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m     35\u001b[0m                         hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m], hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     36\u001b[0m                         hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m], hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriteria\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mmodel_train2\u001b[1;34m(X_set, y_set, num_epochs, batch_size, early_stopping_threshold, hidden_dim, dropout_prob, learning_rate, optimizer_class, criterion_class)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 32\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Update running loss\u001b[39;00m\n\u001b[0;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/BERT_data_for_classifier/results/simple_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results2_sorted = pickle.load(file)\n",
    "else: \n",
    "    total_results2 = hyperparamTuning(selectedTrainList, selectedTestList, selectedValList, \\\n",
    "                                     y_train, y_test, y_val, True, param_grid2, indices2)\n",
    "    \n",
    "    total_results2_sorted = total_results2.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results2_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "    \n",
    "total_results2_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
