{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7856ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, pickle, collections, importlib, datetime, torch, nltk, pandas as pd, numpy as np, random\n",
    "from chardet import detect\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict, Counter\n",
    "from wordebd import WORDEBD\n",
    "from vocab import Vocab, Vectors\n",
    "from munch import Munch\n",
    "from cnnlstmseq import CNNLSTMseq\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from model import MaskedEdgeAttention, MaskedNLLLoss, LSTMModel, GRUModel, DialogRNNModel, DialogueGCNModel, DialogueGCN_DailyModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from model import batch_graphify, MaskedNLLLoss, LSTMModel, GRUModel, DialogRNNModel, DialogueGCNModel, DialogueGCN_DailyModel\n",
    "# Autoreload extensions (if you're using Jupyter Notebook or IPython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f23de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d7c41",
   "metadata": {},
   "source": [
    "<b>Make sure to specify which dataset to use\n",
    "<br>\n",
    " - dataset_original\n",
    "<br>\n",
    " - dataset_drop_noise\n",
    "<br>\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5dd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbba9e3",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, filters, kernel_sizes, dropout):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=K) for K in kernel_sizes])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * filters, output_size)\n",
    "        self.feature_dim = output_size\n",
    "\n",
    "    def init_pretrained_embeddings_from_numpy(self, pretrained_word_vectors):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x, umask):\n",
    "        if len(x.size()) == 2:  # Check if x has only 2 dimensions\n",
    "            num_utt, num_words = x.size()\n",
    "            batch = 1\n",
    "        else:\n",
    "            num_utt, batch, num_words = x.size()\n",
    "\n",
    "        x = x.type(torch.LongTensor)\n",
    "        x = x.view(-1, num_words)  # Flatten to (num_utt * batch, num_words)\n",
    "        torch.manual_seed(SEED)\n",
    "        emb = self.embedding(x)  # Embed (num_utt * batch, num_words) -> (num_utt * batch, num_words, embedding_dim)\n",
    "        emb = emb.transpose(-2, -1).contiguous()  # (num_utt * batch, num_words, embedding_dim) -> (num_utt * batch, embedding_dim, num_words)\n",
    "\n",
    "        convoluted = [F.relu(conv(emb)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in convoluted]\n",
    "        concated = torch.cat(pooled, 1)\n",
    "        features = F.relu(self.fc(self.dropout(concated)))  # Apply dropout and fully connected layer\n",
    "        features = features.view(num_utt, batch, -1)  # Reshape back to (num_utt, batch, feature_dim)\n",
    "\n",
    "        mask = umask.unsqueeze(-1).type(torch.FloatTensor)  # (batch, num_utt) -> (batch, num_utt, 1)\n",
    "        mask = mask.transpose(0, 1)  # (batch, num_utt, 1) -> (num_utt, batch, 1)\n",
    "        mask = mask.repeat(1, 1, self.feature_dim)  # (num_utt, batch, 1) -> (num_utt, batch, feature_dim)\n",
    "        features = features * mask  # Apply mask\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a273b63b",
   "metadata": {
    "code_folding": [
     0,
     5,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def get_encoding_type(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']\n",
    "\n",
    "def detect_misspelling(source):\n",
    "    pass\n",
    "\n",
    "def replace_spelling(source):\n",
    "    return re.sub(\"Åf\", \"'\", source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e24b0d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    '''\n",
    "    Preprocess text data\n",
    "    @param data: list of text examples\n",
    "    @return preprocessed_data: list of preprocessed text examples\n",
    "    '''\n",
    "    preprocessed_data = []\n",
    "    for example in data:\n",
    "        # Convert to lowercase\n",
    "        example = example.lower()\n",
    "        # Remove punctuation\n",
    "        example = re.sub(r'[^\\w\\s]', '', example)\n",
    "        preprocessed_data.append(example)\n",
    "    return preprocessed_data\n",
    "\n",
    "def load_pretrained_glove():\n",
    "    print(\"Loading GloVe...\")\n",
    "    glv_vector = {}\n",
    "    f = open('embed/glove/glove.840B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float')\n",
    "            glv_vector[word] = coefs\n",
    "        except ValueError:\n",
    "            continue\n",
    "    f.close()\n",
    "    print(\"Completed loading pretrained GloVe model.\")\n",
    "    return glv_vector\n",
    "\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "def _read_words(data, convmode=None):\n",
    "    '''    \n",
    "    Count the occurrences of all words\n",
    "    @param convmode: str, None for non conversational scope, 'naive' for classic or naive approach, 'conv' for conversation depth into account (one additional dim and nested values)\n",
    "    @param data: list of examples\n",
    "    @return words: list of words (with duplicates)\n",
    "    '''    \n",
    "    words = []\n",
    "    if convmode is None:\n",
    "        for example in data:\n",
    "            words += example.split()\n",
    "    return words\n",
    "\n",
    "def find_value_ranges(lst):\n",
    "    value_ranges = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            value_ranges.append((start_index, i - 1))\n",
    "            start_index = i\n",
    "\n",
    "    # Add the last range\n",
    "    value_ranges.append((start_index, len(lst) - 1))\n",
    "\n",
    "    return value_ranges\n",
    "\n",
    "def seed_everything(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d2cf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12840, 12)\n",
      "(3400, 12)\n",
      "(1462, 12)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "X_train = pd.read_csv('data/' + dataset_path + '/train_sent_emo_dya.csv', encoding='shift_jis')\n",
    "X_test = pd.read_csv('data/' + dataset_path+ '/test_sent_emo_dya.csv', encoding='utf-8')\n",
    "X_dev = pd.read_csv('data/' + dataset_path + '/dev_sent_emo_dya.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first three rows\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5825e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to drop\n",
    "drop_features = list(X_train.columns[6:]) \n",
    "\n",
    "# Create DataFrame for target labels\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "y_dev = pd.DataFrame()\n",
    "\n",
    "y_train[\"Emotion\"] = X_train[\"Emotion\"].copy()\n",
    "y_test[\"Emotion\"] = X_test[\"Emotion\"].copy()\n",
    "y_dev[\"Emotion\"] = X_dev[\"Emotion\"].copy()\n",
    "\n",
    "y_train[\"Dialogue_ID\"] = X_train[\"Dialogue_ID\"].copy()\n",
    "y_test[\"Dialogue_ID\"] = X_test[\"Dialogue_ID\"].copy()\n",
    "y_dev[\"Dialogue_ID\"] = X_dev[\"Dialogue_ID\"].copy()\n",
    "\n",
    "# Drop features from X_train DataFrame\n",
    "X_train = X_train.drop(drop_features, axis=1)\n",
    "X_test = X_test.drop(drop_features, axis=1)\n",
    "X_dev = X_dev.drop(drop_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73b303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the \"Utterance\" column\n",
    "# X_train[\"Utterance\"] = preprocess_text(X_train[\"Utterance\"].tolist())\n",
    "# X_test[\"Utterance\"] = preprocess_text(X_test[\"Utterance\"].tolist())\n",
    "# X_dev[\"Utterance\"] = preprocess_text(X_dev[\"Utterance\"].tolist())\n",
    "# Print the first 14 rows of X_train DataFrame\n",
    "# print(X_train[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86036b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_encoder.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/label_decoder.pkl\")\n",
    "\n",
    "if not(checkFile1 and checkFile2):\n",
    "    labels = sorted(set(y_train.Emotion))\n",
    "    labelEncoder = {label: i for i, label in enumerate(labels)}\n",
    "    labelDecoder = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    pickle.dump(labelEncoder, open('data/dump/' + dataset_path + '/label_encoder.pkl', 'wb'))\n",
    "    pickle.dump(labelDecoder, open('data/dump/' + dataset_path + '/label_decoder.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('data/dump/label_encoder.pkl', 'rb')\n",
    "    file2 = open('data/dump/label_decoder.pkl', 'rb')\n",
    "    labelEncoder = pickle.load(file1)\n",
    "    labelDecoder = pickle.load(file2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6211e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the \"Emotion\" column in y_train\n",
    "y_train[\"Emotion\"] = y_train[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "y_test[\"Emotion\"] = y_test[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "y_dev[\"Emotion\"] = y_dev[\"Emotion\"].apply(lambda x: encode_labels(labelEncoder, x))\n",
    "\n",
    "# Copy the encoded \"Emotion\" column from y_train to X_train\n",
    "X_train[\"Emotion\"] = y_train[\"Emotion\"].copy()\n",
    "X_test[\"Emotion\"] = y_test[\"Emotion\"].copy()\n",
    "X_dev[\"Emotion\"] = y_dev[\"Emotion\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a15940aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "checkFile1 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_train.pkl\")\n",
    "checkFile2 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_test.pkl\")\n",
    "checkFile3 = os.path.isfile(\"data/dump/\" + dataset_path + \"/labels_dev.pkl\")\n",
    "\n",
    "if not (checkFile1 or checkFile2 or checkFile3):\n",
    "    pickle.dump(X_train[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_train.pkl', 'wb'))\n",
    "    pickle.dump(X_test[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(X_dev[\"Emotion\"], open('data/dump/' + dataset_path + '/labels_dev.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db6676b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "# checkFile = os.path.isfile(\"data/dump/train_labels.pkl\")\n",
    "\n",
    "# If the file doesn't exist, save the \"Emotion\" column of X_train DataFrame as train_labels.pkl\n",
    "# if not checkFile:\n",
    "#     pickle.dump(X_train[\"Emotion\"], open('data/dump/train_labels.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd7bc75",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Define the file path\n",
    "# file_path = os.path.join(os.getcwd(), \"data/wiki-news-300d-1M.vec\")\n",
    "    \n",
    "# # Check if the file exists\n",
    "# if os.path.isfile(file_path):\n",
    "#     print(f\"{file_path} exists\")\n",
    "# else:\n",
    "#     print(f\"The file does not exist in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb9d2864",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Define the vectors\n",
    "# vectors = Vectors(name=\"wiki-news-300d-1M.vec\", url=\"data/\", cache=\"data/\")\n",
    "\n",
    "# # Cache the vectors file\n",
    "# # vectors.cache(name=\"data/wiki-news-300d-1M.vec\", url=\"data/\", cache=\"data/\")\n",
    "\n",
    "# # Create vocabulary\n",
    "# vocab = Vocab(\n",
    "#     counter=collections.Counter(_read_words(X_train[\"Utterance\"])),\n",
    "#     vectors=vectors,\n",
    "#     specials=['<pad>', '<unk>'],\n",
    "#     min_freq=5\n",
    "# )\n",
    "\n",
    "# # Print word embedding statistics\n",
    "# wv_size = vocab.vectors.size()\n",
    "# print('Total num. of words: {}, word vector dimension: {}'.format(\n",
    "#     wv_size[0],\n",
    "#     wv_size[1]\n",
    "# ))\n",
    "\n",
    "# # Initialize word embeddings\n",
    "# ebd = WORDEBD(vocab, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb829b5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# vocab.stoi[\"get\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f5abcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6458f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"embed/glove/\" + dataset_path + \"/tokenizer.pkl\")\n",
    "\n",
    "## tokenize all sentences ##\n",
    "if not checkFile:\n",
    "    all_text = list(X_train[\"Utterance\"])\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "    pickle.dump(tokenizer, open('embed/glove/' + dataset_path + '/tokenizer.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('embed/glove/' + dataset_path + '/tokenizer.pkl', 'rb')\n",
    "    tokenizer = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1caa67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the sentences into sequences ##\n",
    "train_sequence = tokenizer.texts_to_sequences(list(X_train['Utterance']))\n",
    "dev_sequence = tokenizer.texts_to_sequences(list(X_dev['Utterance']))\n",
    "test_sequence = tokenizer.texts_to_sequences(list(X_test['Utterance']))\n",
    "\n",
    "X_train['sentence_length'] = [len(item) for item in train_sequence]\n",
    "X_dev['sentence_length'] = [len(item) for item in dev_sequence]\n",
    "X_test['sentence_length'] = [len(item) for item in test_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae3a80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_tokens = 250\n",
    "\n",
    "train_sequence = pad_sequences(train_sequence, maxlen=max_num_tokens, padding='post')\n",
    "dev_sequence = pad_sequences(dev_sequence, maxlen=max_num_tokens, padding='post')\n",
    "test_sequence = pad_sequences(test_sequence, maxlen=max_num_tokens, padding='post')\n",
    "\n",
    "X_train['sequence'] = list(train_sequence)\n",
    "X_dev['sequence'] = list(dev_sequence)\n",
    "X_test['sequence'] = list(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69435558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[['Utterance', 'sequence']][:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d04fb6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also I was the point person on my company’s transition from the KL-5 to GR-6 system.\n"
     ]
    }
   ],
   "source": [
    "print(X_train['Utterance'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ea2580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"embed/glove/\" + dataset_path + \"/glv_embedding_matrix.pkl\")\n",
    "\n",
    "if not checkFile:\n",
    "    glv_vector = load_pretrained_glove()\n",
    "    word_vector_length = len(glv_vector['the'])\n",
    "    word_index = tokenizer.word_index\n",
    "    inv_word_index = {v: k for k, v in word_index.items()}\n",
    "    num_unique_words = len(word_index)\n",
    "    glv_embedding_matrix = np.zeros((num_unique_words + 1, word_vector_length))\n",
    "    pickle.dump(glv_embedding_matrix, open('embed/glove/' + dataset_path + '/glv_embedding_matrix.pkl', 'wb'))\n",
    "else:\n",
    "    file1 = open('embed/glove/' + dataset_path + '/glv_embedding_matrix.pkl', 'rb')\n",
    "    glv_embedding_matrix = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ac41b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix')\n",
    "\n",
    "if not checkFile:\n",
    "    for j in range(1, num_unique_words + 1):\n",
    "        try:\n",
    "            glv_embedding_matrix[j] = glv_vector[inv_word_index[j]]\n",
    "        except KeyError:\n",
    "            glv_embedding_matrix[j] = np.random.randn(word_vector_length) / 200\n",
    "\n",
    "    np.ndarray.dump(glv_embedding_matrix, open('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix', 'wb'))\n",
    "    vocab_size = word_vector_length\n",
    "    print('Done. Completed preprocessing.')\n",
    "    \n",
    "else:\n",
    "#     file1 = open('embed/glove/pretrained_glv_embedding_matrix', 'rb')\n",
    "#     glv_embedding_matrix = pickle.load(file1)\n",
    "    glv_embedding_matrix = np.load(open('embed/glove/' + dataset_path + '/pretrained_glv_embedding_matrix', 'rb'), allow_pickle=True)\n",
    "    vocab_size, embedding_dim = glv_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc4bde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change D_m into\n",
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 100\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c5a55cc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# model1 = DialogueGCN_DailyModel('LSTM',\n",
    "#                                D_m, D_g, D_p, D_e, D_h, D_a, graph_h,\n",
    "#                                n_speakers=2,\n",
    "#                                max_seq_len=110,\n",
    "#                                window_past=0,\n",
    "#                                window_future=5,\n",
    "#                                vocab_size=vocab_size,\n",
    "#                                n_classes=7,\n",
    "#                                listener_state=False,\n",
    "#                                context_attention='general',\n",
    "#                                dropout=0.5,\n",
    "#                                nodal_attention=False,\n",
    "#                                no_cuda=False\n",
    "#                                )\n",
    "# loss_function1 = nn.NLLLoss()\n",
    "# optimizer1 = optim.Adam(model1.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58230ff1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# model2 = LSTMModel(D_m, D_e, D_h,\n",
    "#                               n_classes=7,\n",
    "#                               dropout=0.5)\n",
    "# loss_function2 = MaskedNLLLoss()\n",
    "# optimizer2 = optim.Adam(model2.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31b6cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_feat_extractor = CNNFeatureExtractor(vocab_size=vocab_size, embedding_dim=300, output_size=100, filters=50, kernel_sizes=(3, 4, 5), dropout=0.5)\n",
    "cnn_feat_extractor.init_pretrained_embeddings_from_numpy(glv_embedding_matrix)\n",
    "lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3855ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66639eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390ee85",
   "metadata": {},
   "source": [
    "Ignore the rest of the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10574f61",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Define the parameters using Munch\n",
    "# args = Munch({\n",
    "#     \"cnn_filter_sizes\": [3, 4, 5],\n",
    "#     \"cnn_num_filters\": 100,\n",
    "#     \"cuda\": -1,\n",
    "#     \"mode\": \"train\",\n",
    "#     \"snapshot\": '',\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a4583",
   "metadata": {},
   "source": [
    "Creating an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58701488",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Initialize the model with word embeddings and parameters\n",
    "# model = CNNLSTMseq(ebd, args)\n",
    "\n",
    "# # Print the model object\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f1dd207",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# print(\"{}, Building embedding\".format(\n",
    "#     datetime.datetime.now().strftime('%y/%m/%d %H:%M:%S')), flush=True)\n",
    "\n",
    "# if args.snapshot != '':\n",
    "#     if args.multitask:\n",
    "#         print(\"{}, Loading pretrained embedding from {}\".format(\n",
    "#             datetime.datetime.now().strftime('%y/%m/%d %H:%M:%S'),\n",
    "#             '%s_%s.ebd' % (args.snapshot, args.task)\n",
    "#         ))\n",
    "#         model.load_state_dict(torch.load('%s_%s.ebd' % (args.snapshot, args.task)), strict=False)\n",
    "#     else:\n",
    "#         print(\"{}, Loading pretrained embedding from {}\".format(\n",
    "#             datetime.datetime.now().strftime('%y/%m/%d %H:%M:%S'),\n",
    "#             '{}.ebd'.format(args.snapshot)\n",
    "#         ))\n",
    "#         model.load_state_dict(torch.load('{}.ebd'.format(args.snapshot)), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7828620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19929c11",
   "metadata": {},
   "source": [
    "Testing on smaller data. Uncomment to see the size of updated representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da518513",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Sample data\n",
    "# data = [\"how\", \"i see you hi\", \"foo my bad i want to see my babe\"]\n",
    "\n",
    "# # Initialize variables\n",
    "# input_data = []\n",
    "# max_seq_len_list = []\n",
    "\n",
    "# # Process each conversation in the data\n",
    "# for conversation in data:\n",
    "#     # Tokenize each conversation into words\n",
    "#     conversation_tokens = conversation.split()\n",
    "\n",
    "#     # Convert words to indices\n",
    "#     turn_indices = [vocab.stoi[word] if word in vocab.stoi else vocab.stoi['<unk>'] for word in conversation_tokens]\n",
    "#     turn_tensor = torch.tensor(turn_indices, dtype=torch.long)  # Specify data type as long\n",
    "\n",
    "#     # Pad sequences to a fixed length (adjust this based on your model requirements)\n",
    "#     max_seq_len = len(turn_tensor)\n",
    "#     max_seq_len_list.append(max_seq_len)\n",
    "#     padded_turn = torch.nn.functional.pad(turn_tensor, pad=(0, max_seq_len - len(turn_tensor)))\n",
    "\n",
    "#     # Append the padded turn\n",
    "#     input_data.append(padded_turn)\n",
    "\n",
    "# # Determine the maximum sequence length across all conversations\n",
    "# max_seq_len = max(5, max(max_seq_len_list))\n",
    "\n",
    "# # Pad all conversations to the same maximum sequence length\n",
    "# for i, padded_turn in enumerate(input_data):\n",
    "#     input_data[i] = torch.nn.functional.pad(padded_turn, pad=(0, max_seq_len - len(padded_turn)))\n",
    "\n",
    "# # Stack all padded turns along a new dimension to create batched input\n",
    "# input_data_stacked = torch.stack(input_data)\n",
    "\n",
    "# # Construct input data dictionary\n",
    "# input_data_dict = {'Utterance': input_data_stacked}\n",
    "\n",
    "# # Perform the forward pass (inference) to obtain the numerical representation\n",
    "# with torch.no_grad():  # Disable gradient calculation during inference\n",
    "#     output_representation = model(input_data_dict)\n",
    "\n",
    "# # Output representation now contains the numerical representation of the input text dialog data\n",
    "# # print(output_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d786d8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb140da",
   "metadata": {},
   "source": [
    "<h4> Contextualizing Train Data (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7abe85c5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# # Initialize variables\n",
    "# contexualEmbeddingsTrain = []\n",
    "# max_seq_len_list = []\n",
    "\n",
    "# # Check if the file exists\n",
    "# # If the file doesn't exist, compute updated representations and save them\n",
    "# if key:\n",
    "#     for range_pair, iteration in zip(rangesTrain, range(len(rangesTrain))):\n",
    "#         start_idx, end_idx = range_pair\n",
    "        \n",
    "#         if iteration % 100 == 0:\n",
    "#             print(f\"{iteration} out of {len(rangesTrain)}\")\n",
    "            \n",
    "#         conversation = X_train['Utterance'][start_idx:end_idx + 1]\n",
    "#         input_data = []\n",
    "\n",
    "#         for utterance in conversation:\n",
    "#             # Tokenize each conversation into words\n",
    "#             utterance_tokens = utterance.split()\n",
    "\n",
    "#             # Convert words to indices\n",
    "#             turn_indices = [vocab.stoi[word] if word in vocab.stoi else vocab.stoi['<unk>'] for word in utterance_tokens]   \n",
    "#             turn_tensor = torch.tensor(turn_indices, dtype=torch.long)  # Specify data type as long\n",
    "\n",
    "#             # Pad sequences to a fixed length (adjust this based on your model requirements)\n",
    "#             max_seq_len = len(turn_tensor)\n",
    "#             max_seq_len_list.append(max_seq_len)\n",
    "#             padded_turn = torch.nn.functional.pad(turn_tensor, pad=(0, max_seq_len - len(turn_tensor)))\n",
    "\n",
    "#             # Append the padded turn\n",
    "#             input_data.append(padded_turn)\n",
    "\n",
    "#         # Determine the maximum sequence length across all conversations\n",
    "#         max_seq_len = max(5, max(max_seq_len_list))\n",
    "\n",
    "#         # Pad all conversations to the same maximum sequence length\n",
    "#         for i, padded_turn in enumerate(input_data):\n",
    "#             input_data[i] = torch.nn.functional.pad(padded_turn, pad=(0, max_seq_len - len(padded_turn)))  \n",
    "\n",
    "#         # Stack all padded turns along a new dimension to create batched input\n",
    "#         input_data_stacked = torch.stack(input_data)\n",
    "\n",
    "#         # Construct input data dictionary\n",
    "#         input_data_dict = {'Utterance': input_data_stacked}\n",
    "\n",
    "#         # Perform the forward pass (inference) to obtain the numerical representation\n",
    "#         with torch.no_grad():  # Disable gradient calculation during inference\n",
    "#             output_representation = model(input_data_dict)\n",
    "\n",
    "#         contexualEmbeddingsTrain.append(output_representation)\n",
    "\n",
    "#         # Save the list to a file using pickle\n",
    "# #         if iteration % 800 == 0 | iteration == len(ranges):\n",
    "#     file_path = f'embed/u_prime_CNNBiLSTM_train.pkl'\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#             pickle.dump(contexualEmbeddingsTrain, file)\n",
    "# #             contexualEmbeddingsTrain = []\n",
    "\n",
    "# # If the file exists, load the list from the file\n",
    "# else:\n",
    "#     file_path = 'embed/u_prime_CNNBiLSTM_train.pkl'\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#         contexualEmbeddingsTrain = pickle.load(file)\n",
    "        \n",
    "# # Record end time\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate elapsed time\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Contexualized train data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7646a2",
   "metadata": {},
   "source": [
    "<h4> Contexualizing Test Data (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae993144",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# # Initialize variables\n",
    "# contexualEmbeddingsTest = []\n",
    "# max_seq_len_list = []\n",
    "\n",
    "# # Check if the file exists\n",
    "# # If the file doesn't exist, compute updated representations and save them\n",
    "# if key:\n",
    "#     for range_pair, iteration in zip(rangesTest, range(len(rangesTest))):\n",
    "#         start_idx, end_idx = range_pair\n",
    "        \n",
    "#         if iteration % 100 == 0:\n",
    "#             print(f\"{iteration} out of {len(rangesTest)}\")\n",
    "            \n",
    "#         conversation = X_test['Utterance'][start_idx:end_idx + 1]\n",
    "#         input_data = []\n",
    "\n",
    "#         for utterance in conversation:\n",
    "#             # Tokenize each conversation into words\n",
    "#             utterance_tokens = utterance.split()\n",
    "\n",
    "#             # Convert words to indices\n",
    "#             turn_indices = [vocab.stoi[word] if word in vocab.stoi else vocab.stoi['<unk>'] for word in utterance_tokens]   \n",
    "#             turn_tensor = torch.tensor(turn_indices, dtype=torch.long)  # Specify data type as long\n",
    "\n",
    "#             # Pad sequences to a fixed length (adjust this based on your model requirements)\n",
    "#             max_seq_len = len(turn_tensor)\n",
    "#             max_seq_len_list.append(max_seq_len)\n",
    "#             padded_turn = torch.nn.functional.pad(turn_tensor, pad=(0, max_seq_len - len(turn_tensor)))\n",
    "\n",
    "#             # Append the padded turn\n",
    "#             input_data.append(padded_turn)\n",
    "\n",
    "#         # Determine the maximum sequence length across all conversations\n",
    "#         max_seq_len = max(5, max(max_seq_len_list))\n",
    "\n",
    "#         # Pad all conversations to the same maximum sequence length\n",
    "#         for i, padded_turn in enumerate(input_data):\n",
    "#             input_data[i] = torch.nn.functional.pad(padded_turn, pad=(0, max_seq_len - len(padded_turn)))  \n",
    "\n",
    "#         # Stack all padded turns along a new dimension to create batched input\n",
    "#         input_data_stacked = torch.stack(input_data)\n",
    "\n",
    "#         # Construct input data dictionary\n",
    "#         input_data_dict = {'Utterance': input_data_stacked}\n",
    "\n",
    "#         # Perform the forward pass (inference) to obtain the numerical representation\n",
    "#         with torch.no_grad():  # Disable gradient calculation during inference\n",
    "#             output_representation = model(input_data_dict)\n",
    "\n",
    "#         contexualEmbeddingsTest.append(output_representation)\n",
    "\n",
    "#         # Save the list to a file using pickle\n",
    "# #         if iteration % 800 == 0 | iteration == len(ranges):\n",
    "#     file_path = f'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#             pickle.dump(contexualEmbeddingsTest, file)\n",
    "# #             contexualEmbeddingsTrain = []\n",
    "\n",
    "# # If the file exists, load the list from the file\n",
    "# else:\n",
    "#     file_path = 'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#         contexualEmbeddingsTest = pickle.load(file)\n",
    "        \n",
    "# # Record end time\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate elapsed time\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Contexualized test data - Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c6c03",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for train set¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07e5e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "577\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "rangesTrain = find_value_ranges(X_train[\"Dialogue_ID\"])\n",
    "print(len(rangesTrain))\n",
    "rangesTest = find_value_ranges(X_test[\"Dialogue_ID\"])\n",
    "print(len(rangesTest))\n",
    "rangesDev = find_value_ranges(X_dev[\"Dialogue_ID\"])\n",
    "print(len(rangesDev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4459dbb5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTrain:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_train['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTrain.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_train.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTrain, rangesTrain], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\")\n",
    "    encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf00b0",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2df3e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesTest:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_test['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersTest.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/speaker_encoder_test.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersTest, rangesTest], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/speaker_encoder_test.pkl', \"rb\")\n",
    "    encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e2384",
   "metadata": {},
   "source": [
    "<h4> Getting speaker encoder for val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d4eae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\")\n",
    "encodedSpeakersDev = []\n",
    "\n",
    "if not checkFile:\n",
    "    for range_pair in rangesDev:\n",
    "        start_idx, end_idx = range_pair\n",
    "        speaker_per_dialog = X_dev['Speaker'][start_idx:end_idx + 1].copy()\n",
    "        speaker_feature = sorted(set(speaker_per_dialog))\n",
    "        speaker_encoder = {feature: i for i, feature in enumerate(speaker_feature)}\n",
    "        speaker_decoder = {i: feature for i, feature in enumerate(speaker_feature)}\n",
    "\n",
    "        encoded_speaker = speaker_per_dialog.replace(speaker_encoder)\n",
    "        encodedSpeakersDev.append(encoded_speaker)\n",
    "\n",
    "    # Save encoded speaker list and ranges to a file using pickle\n",
    "    file_path = 'data/dump/' + dataset_path + '/speaker_encoder_dev.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump([encodedSpeakersDev, rangesDev], file)\n",
    "else:\n",
    "    # Load encoded speaker list and ranges from the existing pickle file\n",
    "    file = open('data/dump/' + dataset_path + '/speaker_encoder_dev.pkl', \"rb\")\n",
    "    encodedSpeakersDev, rangesDev = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5520fd",
   "metadata": {},
   "source": [
    "<h4> Testing the CNNFeatExtractor from DGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52402352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data = torch.tensor([\n",
    "#                         [[1, 2, 3, 0, 0],     # Utterance 1\n",
    "#                         [4, 5, 0, 0, 0]],    # Utterance 2\n",
    "#                        [[6, 7, 8, 9, 0],     # Utterance 3\n",
    "#                         [10, 0, 0, 0, 0]]])  # Utterance 4\n",
    "\n",
    "# umask_data = torch.tensor([[1, 1],        # Dialogue 1 has 2 utterances\n",
    "#                            [1, 0]])       # Dialogue 2 has 1 utterance\n",
    "# features = cnn_feat_extractor(x_data, umask_data)\n",
    "# emotions, hidden = lstm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9914a4ea",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def create_tensor_dataset(X_set, ranges_set):\n",
    "#     sequences = []\n",
    "#     for startIdx, endIdx in ranges_set:\n",
    "#         sequences.append(X_set[\"sequence\"][startIdx: endIdx + 1])\n",
    "#     sequences = [torch.FloatTensor(seq) for seq in sequences]\n",
    "#     return TensorDataset(torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True))\n",
    "\n",
    "# def context_encoding(file_path, X_set, ranges_set, batch_size=32):\n",
    "#     list_context = []\n",
    "\n",
    "#     # Create dataset and dataloader\n",
    "#     dataset = create_tensor_dataset(X_set[\"sequence\"], ranges_set)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     for batch in dataloader:\n",
    "#         textf_batch = batch[0]\n",
    "#         batch_size, num_utt, num_words = textf_batch.size()\n",
    "#         umask = torch.ones(batch_size, num_utt, dtype=torch.float32)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             U = cnn_feat_extractor(textf_batch, umask)\n",
    "#             emotions, hidden = lstm(U)\n",
    "        \n",
    "#         list_context.extend(emotions)\n",
    "\n",
    "#         # Free up memory\n",
    "#         del textf_batch, umask, U, emotions, hidden\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(list_context, file)\n",
    "\n",
    "#     return list_context\n",
    "\n",
    "# file_path1 = 'embed/u_prime_CNNBiLSTM_train.pkl'\n",
    "# file_path2 = 'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "\n",
    "# check_file1 = os.path.isfile(file_path1)\n",
    "# check_file2 = os.path.isfile(file_path2)\n",
    "\n",
    "# if not (check_file1 or check_file2):\n",
    "#     start_time = time.time()\n",
    "#     train_context = context_encoding(file_path1, X_train, rangesTrain)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"It took {end_time - start_time:.2f} seconds to encode train text\")\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     test_context = context_encoding(file_path2, X_test, rangesTest)\n",
    "#     end_time = time.time()\n",
    "#     print(f\"It took {end_time - start_time:.2f} seconds to encode test text\")\n",
    "# else:\n",
    "#     with open(file_path1, \"rb\") as file1:\n",
    "#         train_context = pickle.load(file1)\n",
    "#     with open(file_path2, \"rb\") as file2:\n",
    "#         test_context = pickle.load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1827fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_m = 100\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 100\n",
    "D_h = 100\n",
    "D_a = 100\n",
    "graph_h=100\n",
    "n_speakers=2\n",
    "max_seq_len=110\n",
    "window_past=0\n",
    "window_future=5\n",
    "# vocab_size=vocab_size\n",
    "n_classes=7\n",
    "listener_state=False\n",
    "context_attention='general'\n",
    "dropout=0.5\n",
    "nodal_attention=False\n",
    "no_cuda=True\n",
    "n_relations = 2 * n_speakers ** 2\n",
    "att_model = MaskedEdgeAttention(2 * D_e, max_seq_len, no_cuda)\n",
    "nodal_attention=True\n",
    "edge_type_mapping = {}\n",
    "for j in range(n_speakers):\n",
    "    for k in range(n_speakers):\n",
    "        edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "        edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f487a",
   "metadata": {},
   "source": [
    "<h4> Getting data required for graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "031aa98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oheEncodedSpeakersFlat[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dcbad5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:   3%|█▊                                                          | 8/270 [00:00<00:07, 35.51batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:   4%|██▌                                                        | 12/270 [00:00<00:06, 36.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([7, 1, 200])\n",
      "lengths:  [7]  j:  0\n",
      "torch.Size([12, 1, 200])\n",
      "lengths:  [12]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:   6%|███▍                                                       | 16/270 [00:01<00:37,  6.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 200])\n",
      "lengths:  [13]  j:  0\n",
      "torch.Size([10, 1, 200])\n",
      "lengths:  [10]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:   9%|█████                                                      | 23/270 [00:01<00:21, 11.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([15, 1, 200])\n",
      "lengths:  [15]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  10%|█████▉                                                     | 27/270 [00:02<00:16, 14.31batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([13, 1, 200])\n",
      "lengths:  [13]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  12%|███████▏                                                   | 33/270 [00:02<00:13, 17.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 200])\n",
      "lengths:  [13]  j:  0\n",
      "torch.Size([13, 1, 200])\n",
      "lengths:  [13]  j:  0\n",
      "torch.Size([14, 1, 200])\n",
      "lengths:  [14]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  13%|███████▊                                                   | 36/270 [00:02<00:13, 16.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 1, 200])\n",
      "lengths:  [17]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([10, 1, 200])\n",
      "lengths:  [10]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  15%|████████▉                                                  | 41/270 [00:02<00:10, 21.11batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([14, 1, 200])\n",
      "lengths:  [14]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  18%|██████████▋                                                | 49/270 [00:03<00:10, 22.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 200])\n",
      "lengths:  [16]  j:  0\n",
      "torch.Size([18, 1, 200])\n",
      "lengths:  [18]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  21%|████████████▋                                              | 58/270 [00:03<00:07, 28.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([14, 1, 200])\n",
      "lengths:  [14]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  23%|█████████████▌                                             | 62/270 [00:03<00:07, 26.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([17, 1, 200])\n",
      "lengths:  [17]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  26%|███████████████▌                                           | 71/270 [00:03<00:06, 32.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  29%|████████████████▊                                          | 77/270 [00:03<00:05, 35.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([19, 1, 200])\n",
      "lengths:  [19]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  31%|██████████████████▌                                        | 85/270 [00:04<00:06, 29.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 200])\n",
      "lengths:  [13]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([12, 1, 200])\n",
      "lengths:  [12]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  36%|████████████████████▉                                      | 96/270 [00:04<00:04, 35.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  39%|██████████████████████▌                                   | 105/270 [00:04<00:04, 38.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  41%|███████████████████████▋                                  | 110/270 [00:04<00:03, 40.94batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  44%|█████████████████████████▌                                | 119/270 [00:05<00:04, 37.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  49%|████████████████████████████▎                             | 132/270 [00:05<00:03, 45.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  51%|█████████████████████████████▍                            | 137/270 [00:05<00:03, 36.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([15, 1, 200])\n",
      "lengths:  [15]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  54%|███████████████████████████████▌                          | 147/270 [00:05<00:03, 39.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([7, 1, 200])\n",
      "lengths:  [7]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  56%|████████████████████████████████▋                         | 152/270 [00:05<00:03, 32.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([21, 1, 200])\n",
      "lengths:  [21]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  58%|█████████████████████████████████▌                        | 156/270 [00:06<00:03, 32.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([7, 1, 200])\n",
      "lengths:  [7]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  61%|███████████████████████████████████▍                      | 165/270 [00:06<00:02, 36.03batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  64%|█████████████████████████████████████▍                    | 174/270 [00:06<00:02, 34.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([14, 1, 200])\n",
      "lengths:  [14]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([10, 1, 200])\n",
      "lengths:  [10]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  66%|██████████████████████████████████████▏                   | 178/270 [00:06<00:03, 30.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([16, 1, 200])\n",
      "lengths:  [16]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  67%|███████████████████████████████████████                   | 182/270 [00:06<00:03, 25.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([20, 1, 200])\n",
      "lengths:  [20]  j:  0\n",
      "torch.Size([17, 1, 200])\n",
      "lengths:  [17]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  69%|███████████████████████████████████████▋                  | 185/270 [00:07<00:03, 23.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 200])\n",
      "lengths:  [20]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([10, 1, 200])\n",
      "lengths:  [10]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  71%|█████████████████████████████████████████                 | 191/270 [00:07<00:03, 21.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1, 200])\n",
      "lengths:  [12]  j:  0\n",
      "torch.Size([11, 1, 200])\n",
      "lengths:  [11]  j:  0\n",
      "torch.Size([16, 1, 200])\n",
      "lengths:  [16]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  73%|██████████████████████████████████████████                | 196/270 [00:07<00:02, 25.66batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  77%|████████████████████████████████████████████▉             | 209/270 [00:07<00:01, 35.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 200])\n",
      "lengths:  [10]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([7, 1, 200])\n",
      "lengths:  [7]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  79%|█████████████████████████████████████████████▉            | 214/270 [00:07<00:01, 36.03batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  83%|████████████████████████████████████████████████▎         | 225/270 [00:08<00:01, 38.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1, 200])\n",
      "lengths:  [7]  j:  0\n",
      "torch.Size([8, 1, 200])\n",
      "lengths:  [8]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  88%|██████████████████████████████████████████████████▉       | 237/270 [00:08<00:00, 43.95batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  90%|███████████████████████████████████████████████████▉      | 242/270 [00:08<00:00, 40.37batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 1, 200])\n",
      "lengths:  [14]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  92%|█████████████████████████████████████████████████████▎    | 248/270 [00:08<00:00, 44.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([16, 1, 200])\n",
      "lengths:  [16]  j:  0\n",
      "torch.Size([12, 1, 200])\n",
      "lengths:  [12]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress:  96%|███████████████████████████████████████████████████████▋  | 259/270 [00:09<00:00, 42.03batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 200])\n",
      "lengths:  [4]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([9, 1, 200])\n",
      "lengths:  [9]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Encoding Progress:  98%|████████████████████████████████████████████████████████▋ | 264/270 [00:09<00:00, 38.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n",
      "torch.Size([7, 1, 200])\n",
      "lengths:  [7]  j:  0\n",
      "torch.Size([6, 1, 200])\n",
      "lengths:  [6]  j:  0\n",
      "torch.Size([2, 1, 200])\n",
      "lengths:  [2]  j:  0\n",
      "torch.Size([10, 1, 200])\n",
      "lengths:  [10]  j:  0\n",
      "torch.Size([3, 1, 200])\n",
      "lengths:  [3]  j:  0\n",
      "torch.Size([5, 1, 200])\n",
      "lengths:  [5]  j:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:09<00:00, 28.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 1, 200])\n",
      "lengths:  [14]  j:  0\n",
      "It took 9.969781875610352 seconds to encode test text\n"
     ]
    }
   ],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, X_set, rangesSet, encodedSpeakersSet):\n",
    "        self.X_set = X_set\n",
    "        self.rangesSet = rangesSet\n",
    "        self.encodedSpeakersSet = encodedSpeakersSet\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rangesSet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        startIdx, endIdx = self.rangesSet[idx]\n",
    "        sequence = self.X_set[\"sequence\"][startIdx:endIdx+1].tolist()\n",
    "        qmask = self.encodedSpeakersSet[startIdx: endIdx+1]\n",
    "        return torch.FloatTensor(sequence), qmask\n",
    "\n",
    "# Define the ContextEncoding function\n",
    "def ContextEncoding(file_path, dataset):\n",
    "    all_emotions, all_umask, all_seq_lengths = [], [], []\n",
    "    all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths = [], [], [], [], []\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for textf, qmask in tqdm(dataloader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        textf = textf.squeeze(0)  # Remove batch dimension (1, utterance_size, embedding_size) -> (utterance_size, embedding_size)\n",
    "        umask = torch.FloatTensor([[1] * textf.size(0)])  # Adjust to (1, utterance_size)\n",
    "        lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "#         print(umask.shape, lengths)\n",
    "        U = cnn_feat_extractor(textf, umask)\n",
    "        emotions, hidden = lstm(U)\n",
    "        all_emotions.append(emotions)\n",
    "        \n",
    "        features, edge_index, \\\n",
    "        edge_norm, edge_type, \\\n",
    "        edge_index_lengths = batch_graphify(emotions, \n",
    "                                            qmask,\n",
    "                                            lengths,\n",
    "                                            window_past,\n",
    "                                            window_future,\n",
    "                                            edge_type_mapping,\n",
    "                                            att_model, \n",
    "                                            no_cuda)\n",
    "        all_umask.append(umask)\n",
    "        all_seq_lengths.append(lengths)\n",
    "        all_features.append(features)\n",
    "        all_edge_index.append(edge_index)\n",
    "        all_edge_norm.append(edge_norm)\n",
    "        all_edge_type.append(edge_type)\n",
    "        all_edge_index_lengths.append(edge_index_lengths)\n",
    "    # Concatenate all emotions to form a single 2D tensor\n",
    "    all_emotions = torch.cat(all_emotions, dim=0)  # (total_num_utterances, lstm_hidden_size)\n",
    "    \n",
    "    with open(file_path[0], 'wb') as file:\n",
    "        pickle.dump(all_emotions, file)\n",
    "        \n",
    "    with open(file_path[1], 'wb') as file:\n",
    "        pickle.dump([   all_umask, \\\n",
    "                        all_seq_lengths,\n",
    "                        all_features, \\\n",
    "                        all_edge_index, \\\n",
    "                        all_edge_norm, \\\n",
    "                        all_edge_type, \\\n",
    "                        all_edge_index_lengths], file)\n",
    "    \n",
    "    return all_emotions, all_features, all_edge_index, all_edge_norm, all_edge_type, all_edge_index_lengths\n",
    "\n",
    "# File paths\n",
    "file_path1 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_train.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl']\n",
    "file_path2 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_test.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl']\n",
    "file_path3 = ['embed/' + dataset_path + '/u_prime_CNNBiLSTM_dev.pkl', 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl']\n",
    "\n",
    "# Check if files exist\n",
    "checkFile1 = os.path.isfile(file_path1[0])\n",
    "checkFile2 = os.path.isfile(file_path2[0])\n",
    "checkFile3 = os.path.isfile(file_path3[0])\n",
    "\n",
    "if not checkFile1:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "\n",
    "    trainDataset = ContextDataset(X_train, rangesTrain, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trainContext, all_features, \\\n",
    "                 all_edge_index, \\\n",
    "                 all_edge_norm, \\\n",
    "                 all_edge_type, \\\n",
    "                 all_edge_index_lengths = ContextEncoding(file_path1, trainDataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train text\")\n",
    "\n",
    "if not checkFile2:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersTest for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    testDataset = ContextDataset(X_test, rangesTest, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    testContext, _, _, _, _, _ = ContextEncoding(file_path2, testDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "    \n",
    "if not checkFile3:\n",
    "    encodedSpeakersFlat = [speaker for dialogue in encodedSpeakersDev for speaker in dialogue]\n",
    "    oheEncodedSpeakersFlat = torch.FloatTensor([[1, 0] if x == 0 else [0, 1] for x in encodedSpeakersFlat])\n",
    "    \n",
    "    devDataset = ContextDataset(X_dev, rangesDev, oheEncodedSpeakersFlat)\n",
    "    start_time = time.time()\n",
    "    devContext, _, _, _, _, _ = ContextEncoding(file_path3, devDataset)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode test text\")\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3:\n",
    "    with open(file_path1[0], 'rb') as file1:\n",
    "        trainContext = pickle.load(file1)\n",
    "    with open(file_path1[1], 'rb') as file1:\n",
    "        features, edge_index, \\\n",
    "        edge_norm, edge_type, edge_index_lengths = pickle.load(file1)     \n",
    "    with open(file_path2[0], 'rb') as file2:\n",
    "        testContext = pickle.load(file2)\n",
    "    with open(file_path3[0], 'rb') as file3:\n",
    "        devContext = pickle.load(file3)\n",
    "\n",
    "# Confirm the shape of trainContext and testContext\n",
    "print(f\"trainContext shape: {trainContext.shape}\")\n",
    "print(f\"testContext shape: {testContext.shape}\")\n",
    "print(f\"devContext shape: {devContext.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "509eb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainContext[2])\n",
    "# print(features[0].shape)\n",
    "# print(edge_index[0].shape)\n",
    "# print(edge_norm[0])\n",
    "# print(edge_type)\n",
    "# print(edge_index_lengths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e46ec",
   "metadata": {},
   "source": [
    "The original code with no regards to memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "552d0b2c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def ContextEncoding(file_path, X_set, rangesSet):\n",
    "#     listContext = []\n",
    "#     for dID in range(len(rangesSet)):\n",
    "#         startIdx, endIdx = rangesSet[dID]\n",
    "#         textf = torch.FloatTensor([X_set[\"sequence\"][startIdx: endIdx+1].tolist()])\n",
    "#     #     qmask = encodedSpeakersTrain[dID]\n",
    "#         umask = torch.FloatTensor([[1]*len(textf)])\n",
    "#         U = cnn_feat_extractor(textf, umask)\n",
    "#         emotions, hidden = lstm(U)\n",
    "#         listContext.append(emotions)\n",
    "    \n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(listContext, file)\n",
    "    \n",
    "#     return listContext\n",
    "\n",
    "# file_path1 = f'embed/u_prime_CNNBiLSTM_train.pkl'\n",
    "# file_path2= f'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "\n",
    "# checkFile1 = os.path.isfile(file_path1)\n",
    "# checkFile2 = os.path.isfile(file_path2)\n",
    "\n",
    "# if not (checkFile1 or checkFile2):\n",
    "#     start_time = time.time()\n",
    "#     trainContext = ContextEncoding(file_path1, X_train, rangesTrain)\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(\"It took \", elapsed_time, \" seconds to encode train text\")\n",
    "#     with open(file_path1, 'wb') as file:\n",
    "#             pickle.dump(trainContext, file)\n",
    "#     file.close()\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     testContext = ContextEncoding(file_path2, X_test, rangesTest)\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(\"It took \", elapsed_time, \" seconds to encode test text\")\n",
    "#     with open(file_path2, 'wb') as file:\n",
    "#         pickle.dump(testContext, file)\n",
    "#     file.close()\n",
    "# else:\n",
    "#     file1 = open(file_path1, \"rb\")\n",
    "#     trainContext = pickle.load(file1)\n",
    "#     file2 = open(file_path2, \"rb\")\n",
    "#     testContext = pickle.load(file2)\n",
    "#     file1.close()\n",
    "#     file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad6a0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainContext[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a19cb",
   "metadata": {},
   "source": [
    "Unsupervised visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b5f1de43",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Concatenate all the tensors representing individual utterances\n",
    "# concatenated_tensors = []\n",
    "# for dialogue_tensor in updated_representations:\n",
    "#     concatenated_tensors.extend(dialogue_tensor)\n",
    "\n",
    "# # Convert the concatenated list of tensors into a single tensor\n",
    "# tensor_utterances = torch.stack(concatenated_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "12995d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'anger',\n",
       " 1: 'disgust',\n",
       " 2: 'fear',\n",
       " 3: 'joy',\n",
       " 4: 'neutral',\n",
       " 5: 'sadness',\n",
       " 6: 'surprise'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "23c553ae",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# list(labelDecoder.values())\n",
    "# num_instance=len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3d031c27",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Calculate the counts for each unique label\n",
    "# unique_labels, label_counts = np.unique(list(X_train[\"Emotion\"][:num_instance]), return_counts=True)\n",
    "\n",
    "# # Print the counts for each unique label\n",
    "# for label, count in zip(unique_labels, label_counts):\n",
    "#     print(f\"{labelDecoder[label]}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c07b8f",
   "metadata": {},
   "source": [
    "<h4> Visualize utterance embeddnig (u') with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6aede38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fe10df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if key:\n",
    "    # Assuming trainContext, X_train, and labelDecoder are already defined\n",
    "    num_instance = len(X_train[\"Emotion\"])\n",
    "\n",
    "    # Calculate the counts for each unique label\n",
    "    unique_labels, label_counts = np.unique(list(X_train[\"Emotion\"][:num_instance]), return_counts=True)\n",
    "\n",
    "    # Print the counts for each unique label\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        print(f\"{labelDecoder[label]}: {count} occurrences\")\n",
    "\n",
    "    encodedFeaturesFlat = [speaker for dialogue in encodedSpeakersTrain for speaker in dialogue]\n",
    "    trainContext = trainContext.squeeze(1)\n",
    "\n",
    "    # Convert the tensor to a numpy array for use with sklearn\n",
    "    trainContext_np = trainContext.detach().numpy()\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    trainContext_pca = pca.fit_transform(trainContext_np)\n",
    "\n",
    "    # Perform t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    trainContext_tsne = tsne.fit_transform(trainContext_np)\n",
    "\n",
    "    # Plot PCA results with color-coded labels\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "        plt.scatter(trainContext_pca[indices, 0], trainContext_pca[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "    plt.title('PCA of trainContext with Color-Coded Labels')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot t-SNE results with color-coded labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "        plt.scatter(trainContext_tsne[indices, 0], trainContext_tsne[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "    plt.title('t-SNE of trainContext with Color-Coded Labels')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78804eeb",
   "metadata": {},
   "source": [
    "<h4>Visualize node features (pre-h') with T-SNE and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2c3f3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if key:\n",
    "    # Assuming trainContext, X_train, and labelDecoder are already defined\n",
    "    num_instance = len(X_train[\"Emotion\"])\n",
    "\n",
    "    # Calculate the counts for each unique label\n",
    "    unique_labels, label_counts = np.unique(list(X_train[\"Emotion\"][:num_instance]), return_counts=True)\n",
    "\n",
    "    # Print the counts for each unique label\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        print(f\"{labelDecoder[label]}: {count} occurrences\")\n",
    "\n",
    "    flattened_features = torch.cat(features, dim=0)\n",
    "\n",
    "    # Convert the tensor to a numpy array for use with sklearn\n",
    "    flattened_features_np = flattened_features.detach().numpy()\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    flattened_features_np_pca = pca.fit_transform(flattened_features_np)\n",
    "\n",
    "    # Perform t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    flattened_features_tsne = tsne.fit_transform(flattened_features_np)\n",
    "\n",
    "    # Plot PCA results with color-coded labels\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "        plt.scatter(flattened_features_np_pca[indices, 0], flattened_features_np_pca[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "    plt.title('PCA of trainContext with Color-Coded Labels')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot t-SNE results with color-coded labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(X_train[\"Emotion\"][:num_instance] == label)[0]\n",
    "        plt.scatter(flattened_features_tsne[indices, 0], flattened_features_tsne[indices, 1], label=labelDecoder[label], alpha=0.7)\n",
    "    plt.title('t-SNE of trainContext with Color-Coded Labels')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3d430fa6",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# labels = torch.tensor(X_train[\"Emotion\"][:num_instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f1043832",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# label_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aed63efa",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# runTSNE = 1\n",
    "# if runTSNE:\n",
    "#     from sklearn.manifold import TSNE\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     # List of perplexity values to loop over\n",
    "#     perplexity_values = [50, 100]\n",
    "\n",
    "#     # Loop over each perplexity value\n",
    "#     for perplexity in perplexity_values:\n",
    "#         # Initialize t-SNE with the current perplexity value\n",
    "#         tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "#         # Fit and transform the data using t-SNE\n",
    "#         h_prime_tsne = tsne.fit_transform(tensor_utterances[:num_instance].detach().numpy())\n",
    "\n",
    "#         # Plot the node embeddings with different colors for each label\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         for label, emotion in zip(range(len(label_encoder)), label_encoder):\n",
    "#             indices = (labels == label).nonzero().squeeze()\n",
    "#             plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "#         plt.title(f'Utterance Embeddings (Train) Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "#         plt.xlabel('Dimension 1', color=\"white\")\n",
    "#         plt.ylabel('Dimension 2', color=\"white\")\n",
    "#         plt.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e2170c",
   "metadata": {},
   "source": [
    "<h4> Visualize utterance embedding (u') with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "91e6dc36",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(tensor_utterances[:num_instance].detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(labels):\n",
    "#     indices = labels == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "# plt.title('PCA Visualization of Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
