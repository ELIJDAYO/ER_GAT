{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c64aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import torch.nn.init as init\n",
    "import dgl\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f920f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8aa9fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        torch.manual_seed(42)\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "        \n",
    "        # Edge type embeddings\n",
    "        torch.manual_seed(42)\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "        \n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of neighbor node representations\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).mean(dim=2)\n",
    "\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores\n",
    "\n",
    "class EGATConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_node_feats,\n",
    "                 in_edge_feats,\n",
    "                 out_node_feats,\n",
    "                 out_edge_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 **kw_args):\n",
    "\n",
    "        super().__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._out_node_feats = out_node_feats\n",
    "        self._out_edge_feats = out_edge_feats\n",
    "        \n",
    "        self.fc_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=bias)\n",
    "        self.fc_ni = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_nj = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        \n",
    "        # Attention parameter\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.manual_seed(42)\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.xavier_normal_(self.fc_node.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "        init.xavier_normal_(self.attn, gain=gain)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "        with graph.local_scope():\n",
    "            graph.edata['f'] = efeats\n",
    "            graph.ndata['h'] = nfeats\n",
    "            \n",
    "            f_ni = self.fc_ni(nfeats)\n",
    "            f_nj = self.fc_nj(nfeats)\n",
    "            f_fij = self.fc_fij(efeats)\n",
    "            graph.srcdata.update({'f_ni' : f_ni})\n",
    "            graph.dstdata.update({'f_nj' : f_nj})\n",
    "            \n",
    "            graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "            f_out = graph.edata.pop('f_tmp') + f_fij\n",
    "            \n",
    "            if self.bias is not None:\n",
    "                f_out += self.bias\n",
    "            f_out = nn.functional.leaky_relu(f_out)\n",
    "            f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "            \n",
    "            e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.edata['a'] = edge_softmax(graph, e)\n",
    "            graph.ndata['h_out'] = self.fc_node(nfeats).view(-1, self._num_heads, self._out_node_feats)\n",
    "            \n",
    "            graph.update_all(fn.u_mul_e('h_out', 'a', 'm'), fn.sum('m', 'h_out'))\n",
    "\n",
    "            h_out = graph.ndata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "            if get_attention:\n",
    "                return h_out, f_out, graph.edata.pop('a')\n",
    "            else:\n",
    "                return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7adf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ohe(edge_types):\n",
    "    one_hot_encoding = []\n",
    "    for edge_type in edge_types:\n",
    "        if edge_type == 0:\n",
    "            one_hot_encoding.append([1., 0., 0.])\n",
    "        elif edge_type == 1:\n",
    "            one_hot_encoding.append([0., 1., 0.])\n",
    "        elif edge_type == 2:\n",
    "            one_hot_encoding.append([0., 0., 1.])\n",
    "    return torch.tensor(one_hot_encoding)\n",
    "\n",
    "def get_inferred_edgetypes_GAT(dialog, edge_types):\n",
    "    inferred_edge_types = []\n",
    "    inferred_edge_indices = []\n",
    "    for target_node in dialog.values():\n",
    "        if len(target_node) == 1:\n",
    "            inferred_edge_types.append(0)\n",
    "            inferred_edge_indices.append(0)\n",
    "        else:\n",
    "            edge_index = target_node[0][0]\n",
    "            highest_attention = target_node[0][1]\n",
    "            for src_node in target_node[1:]:\n",
    "                if highest_attention < src_node[1]:\n",
    "                    highest_attention = src_node[1]\n",
    "                    edge_index = src_node[0]\n",
    "            inferred_edge_indices.append(edge_index)\n",
    "            inferred_edge_types.append(edge_types[edge_index].tolist())\n",
    "    return inferred_edge_indices, inferred_edge_types\n",
    "\n",
    "def get_inferred_edgetypes_EGAT(edges_target_nodes, sample_edge_types, size_dialog, dialog_id):\n",
    "    inferred_edge_types = []\n",
    "    for target_idx in range(size_dialog):\n",
    "        num_edges = len(edges_target_nodes[target_idx])\n",
    "        if num_edges == 1:\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "        else:\n",
    "            highest_attn_score = max(edges_target_nodes[target_idx][0][1])\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            for sample_edge in range(1, num_edges):\n",
    "                cur_highest_attn_score = max(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                if cur_highest_attn_score > highest_attn_score:\n",
    "                    highest_attn_score = cur_highest_attn_score\n",
    "                    edgetype_idx = np.argmax(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                    edge_idx = edges_target_nodes[target_idx][sample_edge][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "    return inferred_edge_types\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7639e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 0\n",
    "        while (val <= 3) and (i+val <= end_idx):\n",
    "            target_idx = i+val\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "            val = val+1\n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "    return adjacency_list_dict\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    for i in range(dialogs_len):\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "        for j in range(dialog_len):\n",
    "            src_node = dialog_nodes_pairs[j]\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k]\n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))\n",
    "    return whole_edge_type_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddb394",
   "metadata": {},
   "source": [
    "<h3> Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c424c",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d8a8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "rangesTrain = []\n",
    "\n",
    "if not checkFile:\n",
    "    print(\"Run first the contextEncoder1 or 2 to generate this file\")\n",
    "else:\n",
    "    with open('data/dump/speaker_encoder_train.pkl', \"rb\") as file:\n",
    "        encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/adjListTrain.pkl\")\n",
    "adjacencyListTrain = []\n",
    "\n",
    "if key:\n",
    "    adjacencyListTrain = get_all_adjacency_list(rangesTrain)\n",
    "else:\n",
    "    with open('data/dump/adjListTrain', \"rb\") as file:\n",
    "        adjacencyListTrain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "771b8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_CNNBiLSTM_train.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    contextualEmbeddingsTrain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6055971",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain)\n",
    "edgeTypesTrain = get_all_edge_type_list(edgeIndicesTrain, encodedSpeakersTrain)\n",
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871b0b4",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de3baa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "rangesTest = []\n",
    "\n",
    "if not checkFile:\n",
    "    print(\"Run first the contextEncoder2 to generate this file\")\n",
    "else:\n",
    "    with open('data/dump/speaker_encoder_test.pkl', \"rb\") as file:\n",
    "        encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/adjListTest.pkl\")\n",
    "adjacencyListTest = []\n",
    "\n",
    "if key:\n",
    "    adjacencyListTest = get_all_adjacency_list(rangesTest)\n",
    "else:\n",
    "    with open('data/dump/adjListTest', \"rb\") as file:\n",
    "        adjacencyListTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0b9fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    contextualEmbeddingsTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29deca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTest = get_all_adjacency_list(rangesTest)\n",
    "edgeTypesTest = get_all_edge_type_list(edgeIndicesTest, encodedSpeakersTest)\n",
    "edgeIndicesTest = get_all_adjacency_list(rangesTest, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e5796",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0634150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO repeat the one above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36c94f",
   "metadata": {},
   "source": [
    "<h3> Get GAT output from each set of data (train, test, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f024704",
   "metadata": {},
   "source": [
    "<h4> Instantiating the GAT (1st implementation) for 1 sample train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cacafa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in_features = len(contextualEmbeddingsTrain[0][0])\n",
    "num_out_features = len(contextualEmbeddingsTrain[0][0])\n",
    "num_heads = 4\n",
    "num_edge_types = 3\n",
    "gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98a8775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_prime shape:  torch.Size([14, 50]) attention_coef shape:  torch.Size([14, 300, 50])\n"
     ]
    }
   ],
   "source": [
    "i = 0  # dialogue id\n",
    "relationalEmbedding, attentionCoef = gat_layer((contextualEmbeddingsTrain[i], edgeIndicesTrain[i]), edgeTypesTrain[i])\n",
    "print(\"h_prime shape: \", relationalEmbedding.shape, \"attention_coef shape: \", attentionCoef.shape)\n",
    "\n",
    "targetNodes = edgeIndicesTrain[i][1].tolist()\n",
    "\n",
    "sample = {}\n",
    "sampleEdgetypes = []\n",
    "\n",
    "for target_i in sorted(set(targetNodes)):\n",
    "    sample[target_i] = []\n",
    "\n",
    "for targetNode, idx in zip(targetNodes, range(len(targetNodes))):\n",
    "    sample[targetNode].append([idx, relationalEmbedding[targetNode][idx].tolist()])\n",
    "\n",
    "listEdgeIdxTrain, inferredEdgeTypes = get_inferred_edgetypes_GAT(sample, edgeTypesTrain[i])\n",
    "sampleEdgetypes.append(inferredEdgeTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc9c28c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/dump/label_decoder.pkl', 'rb')\n",
    "label_decoder = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "label_decoder = list(label_decoder.values())\n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ca6003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/labels_train.pkl\")\n",
    "\n",
    "if checkFile is False:\n",
    "    print(\"Please run the contextEncoder2 notebook to save the label file\")\n",
    "else:\n",
    "    file = open('data/dump/labels_train.pkl', 'rb')\n",
    "    y_train = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6f3e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/labels_test.pkl\")\n",
    "\n",
    "if checkFile is False:\n",
    "    print(\"Please run the contextEncoder2 notebook to save the label file\")\n",
    "else:\n",
    "    file = open('data/dump/labels_test.pkl', 'rb')\n",
    "    y_test = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223125",
   "metadata": {},
   "source": [
    "<h5>Unsupervised Visualizarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1a41a39",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming h_prime contains the node embeddings\n",
    "# utt_size = 13\n",
    "# labels = torch.tensor(y_train[:utt_size + 1])\n",
    "\n",
    "# cherrypicked_nodes = []\n",
    "# for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#     cherrypicked_nodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "# cherrypicked_nodes = torch.tensor(cherrypicked_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47a97231",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# h_prime_np = cherrypicked_nodes.detach().numpy()\n",
    "\n",
    "# # Perform dimensionality reduction using t-SNE\n",
    "# tsne = TSNE(n_components=3, perplexity=5, random_state=42)\n",
    "# h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# # Plot the node embeddings with different colors for each label\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#     indices = (labels == label).nonzero().squeeze()\n",
    "#     plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "# plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "# plt.xlabel('Dimension 1', color=\"white\")\n",
    "# plt.ylabel('Dimension 2', color=\"white\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8043a6a",
   "metadata": {},
   "source": [
    "<h4> Now get new representations of all train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "588776d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "\n",
    "def get_GAT_representation(filePath, contextualEmbeddings, edgeIndices, edgeTypes):\n",
    "#     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-GAT_train.pkl\") #replace it with key when deployed\n",
    "    if key:\n",
    "        print(\"Start of getting output of 1st GAT\")\n",
    "        allInferredEdgetypes = []\n",
    "        listAllEdgeIdx = []\n",
    "        cherrypickedNodes = []\n",
    "        for dialog, dialog_id in zip(contextualEmbeddings, range(len(contextualEmbeddings))):\n",
    "            h_prime, attention_coef = gat_layer((dialog, edgeIndices[dialog_id]), edgeTypes[dialog_id])\n",
    "            target_nodes = edgeIndices[dialog_id][1].tolist() # first idx represents dialogue id\n",
    "\n",
    "            sample_edgetypes = {}\n",
    "            for i in set(target_nodes):\n",
    "                sample_edgetypes[i] = []\n",
    "\n",
    "            for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "                sample_edgetypes[target_node].append([edge_idx, h_prime[target_node][edge_idx].tolist()])\n",
    "\n",
    "            list_edge_idx, inferred_edgetypes = get_inferred_edgetypes_GAT(sample_edgetypes,  edgeTypes[dialog_id])\n",
    "            listAllEdgeIdx.append(list_edge_idx)\n",
    "            allInferredEdgetypes.append(inferred_edgetypes)\n",
    "\n",
    "            for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "                cherrypickedNodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "\n",
    "        cherrypickedNodes = torch.tensor(cherrypickedNodes)\n",
    "        cherrypickedNodes.shape\n",
    "        print(\"End of getting output of 1st GAT\")\n",
    "\n",
    "        pickle.dump([cherrypickedNodes, allInferredEdgetypes],\n",
    "                    open(filePath, 'wb'))\n",
    "\n",
    "    else:\n",
    "        file = open(filePath, 'rb')\n",
    "        cherrypickedNodes, allInferredEdgetypes = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return cherrypickedNodes, allInferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2ef42f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of getting output of 1st GAT\n",
      "End of getting output of 1st GAT\n",
      "Start of getting output of 1st GAT\n",
      "End of getting output of 1st GAT\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "cherrypickedNodesTrain, allInferredEdgetypesTrain = get_GAT_representation(\n",
    "                                                    \"embed/h_prime_CNNBiLSTM-GAT_train.pkl\",\n",
    "                                                    contextualEmbeddingsTrain,\n",
    "                                                    edgeIndicesTrain,\n",
    "                                                    edgeTypesTrain)\n",
    "# only save the pickle data for test and validation\n",
    "_, _ = get_GAT_representation(\"embed/h_prime_CNNBiLSTM-GAT_test.pkl\",\n",
    "                        contextualEmbeddingsTest,\n",
    "                        edgeIndicesTest,\n",
    "                        edgeTypesTest)\n",
    "# TODO add valid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de95f55",
   "metadata": {},
   "source": [
    "<h5> Visualize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e56aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(y_train)\n",
    "h_prime_np = cherrypickedNodesTrain.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "548c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcd5bd42",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caea68",
   "metadata": {},
   "source": [
    "<h4> Analyze the edgetypes of all train nodes in the context of a dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c236b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_inferred_edgetypes` and `y_train` are defined\n",
    "df_eda = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(allInferredEdgetypesTrain),\n",
    "     'label': y_train,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46b7f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab Result:\n",
      "label        0    1    2     3     4    5    6\n",
      "edgetype                                      \n",
      "0          407  117   97   941  2218  277  591\n",
      "2         1093  247  241  1371  3742  599  899\n",
      "\n",
      "The P-Value of the Chi-Squared Test is: 6.372109133776597e-20\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "# Assuming `df_eda` and `CrosstabResult` are defined\n",
    "CrosstabResult = pd.crosstab(index=df_eda['edgetype'], columns=df_eda['label'])\n",
    "\n",
    "print(\"Crosstab Result:\")\n",
    "print(CrosstabResult)\n",
    "print()\n",
    "\n",
    "# Performing Chi-squared test\n",
    "ChiSqResult = chi2_contingency(CrosstabResult)\n",
    "\n",
    "# P-Value is the Probability of H0 being True\n",
    "# If P-Value > 0.05 then only we Accept the assumption(H0)\n",
    "# H0: The variables are not correlated with each other.\n",
    "\n",
    "print('The P-Value of the Chi-Squared Test is:', ChiSqResult[1])\n",
    "\n",
    "if ChiSqResult[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb0a7",
   "metadata": {},
   "source": [
    "<h3> Get EGAT output from each set of data (train, test, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60061c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "egat = EGATConv(in_node_feats=len(contextualEmbeddingsTrain[0][0]),\n",
    "                    in_edge_feats=3,\n",
    "                    out_node_feats=len(contextualEmbeddingsTrain[0][0]),\n",
    "                    out_edge_feats=3,\n",
    "                    num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc9167ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "def get_EGAT_representations(filePath, contextualEmbeddings, edgeIndices, edgeTypes):\n",
    "#     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-EGAT_train.pkl\")\n",
    "    if key:\n",
    "        print(\"Start of getting output of 2nd GAT\")\n",
    "        inferredEdgetypes = []\n",
    "        allNodeFeats = []\n",
    "\n",
    "        # Iterate over each dialogue\n",
    "        for dialog_id in range(len(contextualEmbeddings)):\n",
    "            # Create a DGL graph\n",
    "            graph = dgl.graph((edgeIndices[dialog_id][0], edgeIndices[dialog_id][1]))\n",
    "\n",
    "            # Get one-hot encoded edge features\n",
    "            edge_feats = get_ohe(edgeTypes[dialog_id])\n",
    "\n",
    "            # Get outputs from the second GAT layer\n",
    "            egat_output = egat(graph, contextualEmbeddings[dialog_id], edge_feats)\n",
    "            new_node_feats, new_edge_feats = egat_output\n",
    "\n",
    "            # Compute mean edge features\n",
    "            mean_edge_feats = new_edge_feats.mean(dim=1)\n",
    "            allNodeFeats.append(new_node_feats.mean(dim=1).tolist())\n",
    "\n",
    "            # Prepare edge features for inference\n",
    "            target_nodes = edgeIndices[dialog_id][1].tolist()\n",
    "            sample_edgetypes = {}\n",
    "            for i in set(target_nodes):\n",
    "                sample_edgetypes[i] = []\n",
    "            for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "                sample_edgetypes[target_node].append([edge_idx, \n",
    "                                                      mean_edge_feats[edge_idx].tolist()])\n",
    "\n",
    "            # Infer edge types\n",
    "            sample_edgetypes = get_inferred_edgetypes_EGAT(sample_edgetypes, edgeTypes[dialog_id], \n",
    "                                                           len(contextualEmbeddings[dialog_id]), \n",
    "                                                           dialog_id)\n",
    "            inferredEdgetypes.append(sample_edgetypes)\n",
    "\n",
    "        # Flatten and convert node features to tensor\n",
    "        allNodeFeats = torch.tensor(flatten_extend(allNodeFeats))\n",
    "\n",
    "        print(\"End of getting output of 2nd GAT\")\n",
    "\n",
    "        # Save the data to a pickle file\n",
    "        pickle.dump([allNodeFeats, inferredEdgetypes], open(filePath, 'wb'))\n",
    "    else:\n",
    "        # Load data from the existing pickle file\n",
    "        file = open(filePath, 'rb')\n",
    "        all_node_feats, inferredEdgetypes = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return allNodeFeats, inferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "938645cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of getting output of 2nd GAT\n",
      "End of getting output of 2nd GAT\n",
      "Start of getting output of 2nd GAT\n",
      "End of getting output of 2nd GAT\n"
     ]
    }
   ],
   "source": [
    "allNodeFeatsTrain, inferredEdgetypesTrain = get_EGAT_representations(\n",
    "                                        \"embed/h_prime_CNNBiLSTM-EGAT_train.pkl\",\n",
    "                                        contextualEmbeddingsTrain,\n",
    "                                        edgeIndicesTrain,\n",
    "                                        edgeTypesTrain\n",
    "                                )\n",
    "_, _ = get_EGAT_representations(\n",
    "        \"embed/h_prime_CNNBiLSTM-EGAT_test.pkl\",\n",
    "        contextualEmbeddingsTest,\n",
    "        edgeIndicesTest,\n",
    "        edgeTypesTest\n",
    ")\n",
    "#TODO do for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d83d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda2 = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(inferredEdgetypesTrain),\n",
    "     'label': y_train,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf36e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab Result:\n",
      " label        0    1    2     3     4    5     6\n",
      "edgetype                                       \n",
      "0         1295  305  288  1871  4956  739  1226\n",
      "1          205   59   50   441  1004  137   264\n",
      "The P-Value of the ChiSq Test is: 0.0012723314350347509\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from your data (df_eda2)\n",
    "# Assuming df_eda2 is already defined\n",
    "\n",
    "# Crosstabulation\n",
    "CrosstabResult2 = pd.crosstab(index=df_eda2['edgetype'], columns=df_eda2['label'])\n",
    "print(\"Crosstab Result:\\n\", CrosstabResult2)\n",
    "\n",
    "# Performing Chi-squared test\n",
    "ChiSqResult2 = chi2_contingency(CrosstabResult2)\n",
    "\n",
    "# Print the p-value of the Chi-squared test\n",
    "print('The P-Value of the ChiSq Test is:', ChiSqResult2[1])\n",
    "\n",
    "# Interpret the p-value\n",
    "if ChiSqResult2[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213c68a",
   "metadata": {},
   "source": [
    "Testing on 1 dialog data before scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84641d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Node Features Shape: torch.Size([14, 4, 300])\n",
      "New Edge Features Shape: torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "dialog_id = 0\n",
    "\n",
    "# Create a DGL graph\n",
    "graph = dgl.graph((edgeIndicesTrain[dialog_id][0], edgeIndicesTrain[dialog_id][1]))\n",
    "\n",
    "# Obtain one-hot encoded edge features\n",
    "edge_feats = get_ohe(edgeTypesTrain[dialog_id])\n",
    "\n",
    "# Pass the graph, node representations, and edge features through the EGAT model\n",
    "newNodeFeats, newEdgeFeats = egat(graph, contextualEmbeddingsTrain[dialog_id], edge_feats)\n",
    "\n",
    "# Print the shapes of the new node and edge features\n",
    "print(\"New Node Features Shape:\", newNodeFeats.shape)\n",
    "print(\"New Edge Features Shape:\", newEdgeFeats.shape)\n",
    "\n",
    "# Calculate the mean of node features along the second dimension (number of nodes)\n",
    "h_prime_mean = newNodeFeats.mean(dim=1)\n",
    "\n",
    "# Assuming you want to select only a subset of labels for visualization\n",
    "utt_size = 13\n",
    "labels = torch.tensor(y_train[:utt_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0bbcf4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "h_prime_np = h_prime_mean.detach().numpy()\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# Plot the node embeddings with different colors for each label\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "    indices = (labels == label).nonzero().squeeze()\n",
    "    plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "plt.xlabel('Dimension 1', color=\"white\")\n",
    "plt.ylabel('Dimension 2', color=\"white\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e75975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "851a250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # Convert tensor to numpy array\n",
    "    h_prime_np = allNodeFeatsTrain.detach().numpy()\n",
    "    labels = torch.tensor(y_train)\n",
    "    \n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "        print(h_prime_tsne.shape)\n",
    "        \n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
