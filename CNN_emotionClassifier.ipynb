{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from graph_context_dataset import FeatureEngineeredDataset\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import random\n",
    "from model import FCClassifier, MyNetwork, DATASET_PATH, MatchingAttention\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b22dd",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "\n",
    " - dataset_original\n",
    " - dataset_drop_noise\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1470c3",
   "metadata": {},
   "source": [
    "<h3> Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68406d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a4fb045",
   "metadata": {
    "code_folding": [
     0,
     9,
     18,
     21,
     24,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# class FCLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(FCLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# class ActivationLayer(nn.Module):\n",
    "#     def __init__(self, activation_fn):\n",
    "#         super(ActivationLayer, self).__init__()\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.activation_fn(x)\n",
    "#         return x\n",
    "\n",
    "# def tanh(x):\n",
    "#     return torch.tanh(x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return torch.sigmoid(x)\n",
    "# # loss function and its derivative\n",
    "# def mse(y_true, y_pred):\n",
    "#     return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "# def mse_prime(y_true, y_pred):\n",
    "#     return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "246bf76e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oversample_data(X_train, Y_train, num_classes):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            # Calculate the number of instances to oversample for this class\n",
    "            num_to_oversample = max_class_count - len(indices)\n",
    "            # Randomly select instances with replacement to oversample\n",
    "            oversampled_indices = np.random.choice(indices, size=num_to_oversample, replace=True)\n",
    "            # Append the oversampled instances to the original data\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return torch.tensor(X_train), torch.tensor(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8349606b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    if not tensor_list:\n",
    "        raise ValueError(\"The tensor list is empty\")\n",
    "\n",
    "    feature_dim = tensor_list[0].shape[1]\n",
    "    for tensor in tensor_list:\n",
    "        if tensor.shape[1] != feature_dim:\n",
    "            raise ValueError(\"All tensors must have the same feature dimension\")\n",
    "    \n",
    "    concatenated_tensor = torch.cat(tensor_list, dim=0)\n",
    "    \n",
    "    return concatenated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503aae",
   "metadata": {},
   "source": [
    "<h4> Import labels and label decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/labels_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_dev.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_val = pickle.load(file)\n",
    "y_val = torch.tensor(y_val)\n",
    "    \n",
    "file_path = 'data/dump/' + dataset_path + '/label_decoder.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3e716",
   "metadata": {},
   "source": [
    "<h4> Import the CNNBiLSTM base-node outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c53ba",
   "metadata": {},
   "source": [
    "first we disregard the u' and directly train the h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "396cc340",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_CNNBiLSTM_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_DGCN_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv2_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_RGAT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_EGAT_train.pkl\",\n",
    "]\n",
    "\n",
    "test_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_CNNBiLSTM_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_DGCN_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv2_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_RGAT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_EGAT_test.pkl\",\n",
    "]\n",
    "\n",
    "val_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_CNNBiLSTM_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_DGCN_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv2_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_RGAT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_EGAT_dev.pkl\",\n",
    "]\n",
    "\n",
    "dictKey = {\n",
    "    0 : 'cnn',\n",
    "    1 : 'cnn-select-few',\n",
    "    2 : 'cnn-select-mod',\n",
    "    3 : 'cnn-select-more',\n",
    "    4 : 'dgcn',\n",
    "    5 : 'dgcn-select',\n",
    "    6 : 'gatv1',\n",
    "    7 : 'gatv1-select',\n",
    "    8 : 'gatv1-edge',\n",
    "    9 : 'gatv1-edge-select',\n",
    "    10 : 'gatv2-edge',\n",
    "    11 : 'gatv2-edge-select',\n",
    "    12 : 'rgat',\n",
    "    13 : 'rgat-select',\n",
    "    14 : 'egat',\n",
    "    15 : 'egat-select',\n",
    "    16 : 'cnn-select-mod-dgcn',\n",
    "    17 : 'cnn-select-mod-gatv1',\n",
    "    18 : 'cnn-select-mod-gatv1-edge',\n",
    "    19 : 'cnn-select-mod-gatv2-edge',\n",
    "    20 : 'cnn-select-mod-rgat',\n",
    "    21 : 'cnn-select-mod-egat',\n",
    "}\n",
    "selected_combination = [0,2,3,4,10,12,14]\n",
    "# selected_combination = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff1721",
   "metadata": {},
   "source": [
    "<h4> Getting CNNlstm and GAT outputs for all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2c366b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFeaturesList[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e3426c4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  2160\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  577\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n",
      "<class 'list'>  instance of list:  270\n"
     ]
    }
   ],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    return torch.cat(tensor_list, dim=0)\n",
    "\n",
    "def import_h_prime(file_paths):\n",
    "    featuresList = []\n",
    "    attList = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "#             print(\"Check... \", len(data))\n",
    "            if isinstance(data, list):\n",
    "                print(type(data), \" instance of list: \", len(data))\n",
    "                featuresList.append(concatenate_tensors(data))\n",
    "            else:\n",
    "                print(type(data), \" instance of tensor, \", data.shape)\n",
    "                featuresList.append(data)\n",
    "                \n",
    "    return featuresList\n",
    "    \n",
    "trainFeaturesList = import_h_prime(train_file_paths)\n",
    "\n",
    "testFeaturesList = import_h_prime(test_file_paths)\n",
    "\n",
    "valFeaturesList = import_h_prime(val_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dbfa6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeaturesList[0] = trainFeaturesList[0].squeeze(1)\n",
    "testFeaturesList[0] = testFeaturesList[0].squeeze(1)\n",
    "valFeaturesList[0] = valFeaturesList[0].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95b9fbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 200])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeaturesList[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "33882e42",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getNodalAttn():\n",
    "    nodalAttList = []\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    return nodalAttList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d84c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodalAttn = getNodalAttn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e174164",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "479a3b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a070ce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a43fa315",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "<h3> Feature Selection and creating data combination for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Define select feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6501b577",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_norm_features(encoded_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(torch.tensor(encoded_features).clone().detach().requires_grad_(True))\n",
    "    return torch.tensor(features_scaled)\n",
    "\n",
    "def get_selected_features(encoded_features, labels, top_n):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    print(encoded_features.shape)\n",
    "    features_scaled = scaler.fit_transform(encoded_features.detach().numpy())\n",
    "\n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        # Create a binary mask indicating instances belonging to the current class\n",
    "        mask = (labels == label)\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=top_n) \n",
    "        selector.fit(features_scaled, mask)  \n",
    "\n",
    "        top_features_indices = np.argsort(selector.scores_)[-top_n:]\n",
    "        scores = selector.scores_[top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "\n",
    "    selected_features = encoded_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b70ac5dd",
   "metadata": {
    "code_folding": [
     0,
     18,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Function to train the autoencoder\n",
    "def train_autoencoder(encoded_features, hidden_dim=100, num_epochs=20, lr=0.001):\n",
    "    input_dim = encoded_features.shape[1]\n",
    "    autoencoder = Autoencoder(input_dim, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    X_tensor = torch.tensor(encoded_features, dtype=torch.float32)\n",
    "    train_loader = torch.utils.data.DataLoader(X_tensor, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Autoencoder\", unit=\"epoch\"):\n",
    "        total_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs = data\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = autoencoder(inputs)\n",
    "            loss = criterion(decoded, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_features, _ = autoencoder(torch.tensor(encoded_features, dtype=torch.float32))\n",
    "    \n",
    "    return encoded_features, autoencoder\n",
    "\n",
    "def get_selected_features_autoencoder(autoencoder, encoded_features, labels, top_n=100):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "\n",
    "    reduced_features = autoencoder.encoder(torch.tensor(encoded_features, dtype=torch.float32)).detach().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(reduced_features)\n",
    "    \n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        mask = (labels == label)\n",
    "        selector = chi2(features_scaled, mask)\n",
    "        top_features_indices = np.argsort(selector[0])[-top_n:]\n",
    "        scores = selector[0][top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "    selected_features = reduced_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46d79a94",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "efccc17d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca73f7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Now prepare the data that will be ued to train the classifier, there are 20 combinations. And pick top 7 combinations yielding top F1 weighted-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9e2057f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12840, 1, 200])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "61905fb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12840, 200])\n",
      "torch.Size([12840, 200])\n",
      "torch.Size([12840, 200])\n",
      "torch.Size([12840, 64])\n",
      "torch.Size([12840, 64])\n",
      "torch.Size([12840, 64])\n",
      "torch.Size([12840, 64])\n",
      "torch.Size([12840, 64])\n",
      "torch.Size([12840, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_12960\\2363731650.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features_scaled = scaler.fit_transform(torch.tensor(encoded_features).detach().numpy())\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_12960\\2363731650.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features_scaled = scaler.fit_transform(torch.tensor(encoded_features).detach().numpy())\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_12960\\2363731650.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features_scaled = scaler.fit_transform(torch.tensor(encoded_features).detach().numpy())\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_12960\\61924629.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  trainList_tensors = [torch.tensor(item) for item in trainList]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_12960\\61924629.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  testList_tensors = [torch.tensor(item) for item in testList]\n",
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_12960\\61924629.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  valList_tensors = [torch.tensor(item) for item in valList]\n"
     ]
    }
   ],
   "source": [
    "trainList = []\n",
    "testList = []\n",
    "valList = []\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/trainList.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/testList.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/valList.pkl\"\n",
    "\n",
    "\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3: \n",
    "    with open(file_path1, \"rb\") as file:\n",
    "        trainList = pickle.load(file)\n",
    "    with open(file_path2, \"rb\") as file:\n",
    "        testList = pickle.load(file)\n",
    "    with open(file_path3, \"rb\") as file:\n",
    "        valList = pickle.load(file)\n",
    "else:\n",
    "#     trainFeaturesList.append(data)\n",
    "    #1\n",
    "    trainList.append(trainFeaturesList[0])\n",
    "    testList.append(testFeaturesList[0])\n",
    "    valList.append(valFeaturesList[0])\n",
    "    #2\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 16)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #3\n",
    "    selectedTrainFeatures1b, indicesFeatures1b = get_selected_features(trainFeaturesList[0], y_train, 32)\n",
    "    selectedTestFeatures1b = testFeaturesList[0][:, indicesFeatures1b]\n",
    "    selectedValFeatures1b = valFeaturesList[0][:, indicesFeatures1b]\n",
    "    trainList.append(selectedTrainFeatures1b)\n",
    "    testList.append(selectedTestFeatures1b)\n",
    "    valList.append(selectedValFeatures1b)\n",
    "    #4\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 64)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #5\n",
    "    trainList.append(trainFeaturesList[1])\n",
    "    testList.append(testFeaturesList[1])\n",
    "    valList.append(valFeaturesList[1])\n",
    "    #6\n",
    "    selectedTrainFeatures2, indicesFeatures2 = get_selected_features(trainFeaturesList[1], y_train, 12)\n",
    "    selectedTestFeatures2 = testFeaturesList[1][:, indicesFeatures2]\n",
    "    selectedValFeatures2 = valFeaturesList[1][:, indicesFeatures2]\n",
    "    trainList.append(selectedTrainFeatures2)\n",
    "    testList.append(selectedTestFeatures2)\n",
    "    valList.append(selectedValFeatures2)\n",
    "    #7\n",
    "    trainList.append(trainFeaturesList[2])\n",
    "    testList.append(testFeaturesList[2])\n",
    "    valList.append(valFeaturesList[2])\n",
    "    #8\n",
    "    selectedTrainFeatures3, indicesFeatures3 = get_selected_features(trainFeaturesList[2], y_train, 12)\n",
    "    selectedTestFeatures3 = testFeaturesList[2][:, indicesFeatures3]\n",
    "    selectedValFeatures3 = valFeaturesList[2][:, indicesFeatures3]\n",
    "    trainList.append(selectedTrainFeatures3)\n",
    "    testList.append(selectedTestFeatures3)\n",
    "    valList.append(selectedValFeatures3)\n",
    "    #9\n",
    "    trainList.append(trainFeaturesList[3])\n",
    "    testList.append(testFeaturesList[3])\n",
    "    valList.append(valFeaturesList[3])\n",
    "    #10\n",
    "    selectedTrainFeatures4, indicesFeatures4 = get_selected_features(trainFeaturesList[3], y_train, 12)\n",
    "    selectedTestFeatures4 = testFeaturesList[3][:, indicesFeatures4]\n",
    "    selectedValFeatures4 = valFeaturesList[3][:, indicesFeatures4]\n",
    "    trainList.append(selectedTrainFeatures4)\n",
    "    testList.append(selectedTestFeatures4)\n",
    "    valList.append(selectedValFeatures4)\n",
    "    #11\n",
    "    trainList.append(trainFeaturesList[4])\n",
    "    testList.append(testFeaturesList[4])\n",
    "    valList.append(valFeaturesList[4])\n",
    "    #12\n",
    "    selectedTrainFeatures5, indicesFeatures5 = get_selected_features(trainFeaturesList[4], y_train, 12)\n",
    "    selectedTestFeatures5 = testFeaturesList[4][:, indicesFeatures5]\n",
    "    selectedValFeatures5 = valFeaturesList[4][:, indicesFeatures5]\n",
    "    trainList.append(selectedTrainFeatures5)\n",
    "    testList.append(selectedTestFeatures5)\n",
    "    valList.append(selectedValFeatures5)\n",
    "    #13\n",
    "    trainList.append(trainFeaturesList[5])\n",
    "    testList.append(testFeaturesList[5])\n",
    "    valList.append(valFeaturesList[5])\n",
    "    #14\n",
    "    selectedTrainFeatures6, indicesFeatures6 = get_selected_features(trainFeaturesList[5], y_train, 12)\n",
    "    selectedTestFeatures6 = testFeaturesList[5][:, indicesFeatures6]\n",
    "    selectedValFeatures6 = valFeaturesList[5][:, indicesFeatures6]\n",
    "    trainList.append(selectedTrainFeatures6)\n",
    "    testList.append(selectedTestFeatures6)\n",
    "    valList.append(selectedValFeatures6)\n",
    "    #15\n",
    "    trainList.append(trainFeaturesList[6])\n",
    "    testList.append(testFeaturesList[6])\n",
    "    valList.append(valFeaturesList[6])\n",
    "    #16\n",
    "    selectedTrainFeatures7, indicesFeatures7 = get_selected_features(trainFeaturesList[6], y_train, 12)\n",
    "    selectedTestFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    selectedValFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    trainList.append(selectedTrainFeatures7)\n",
    "    testList.append(selectedTestFeatures7)\n",
    "    valList.append(selectedValFeatures7)\n",
    "    selectedNormTrainFeatures1 = get_norm_features(selectedTrainFeatures1b)\n",
    "    selectedNormTestFeatures1 = get_norm_features(selectedTestFeatures1b)\n",
    "    selectedNormValFeatures1 = get_norm_features(selectedValFeatures1b)\n",
    "\n",
    "    #17\n",
    "    trainNormFeatures2 = get_norm_features(trainFeaturesList[1].detach().numpy())\n",
    "    testNormFeatures2 = get_norm_features(testFeaturesList[1].detach().numpy())\n",
    "    valNormFeatures2 = get_norm_features(valFeaturesList[1].detach().numpy())\n",
    "    concatenatedTrainFeatures2 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures2), dim=1)\n",
    "    concatenatedTestFeatures2 = torch.cat((selectedNormTestFeatures1, testNormFeatures2), dim=1)\n",
    "    concatenatedValFeatures2 = torch.cat((selectedNormValFeatures1, valNormFeatures2), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures2)\n",
    "    testList.append(concatenatedTestFeatures2)\n",
    "    valList.append(concatenatedValFeatures2)\n",
    "    #18\n",
    "    trainNormFeatures3 = get_norm_features(trainFeaturesList[2].detach().numpy())\n",
    "    testNormFeatures3 = get_norm_features(testFeaturesList[2].detach().numpy())\n",
    "    valNormFeatures3 = get_norm_features(valFeaturesList[2].detach().numpy())\n",
    "    concatenatedTrainFeatures3 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures3), dim=1)\n",
    "    concatenatedTestFeatures3 = torch.cat((selectedNormTestFeatures1, testNormFeatures3), dim=1)\n",
    "    concatenatedValFeatures3 = torch.cat((selectedNormValFeatures1, valNormFeatures3), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures3)\n",
    "    testList.append(concatenatedTestFeatures3)\n",
    "    valList.append(concatenatedValFeatures3)\n",
    "    #19\n",
    "    trainNormFeatures4 = get_norm_features(trainFeaturesList[3].detach().numpy())\n",
    "    testNormFeatures4 = get_norm_features(testFeaturesList[3].detach().numpy())\n",
    "    valNormFeatures4 = get_norm_features(valFeaturesList[3].detach().numpy())\n",
    "    concatenatedTrainFeatures4 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures4), dim=1)\n",
    "    concatenatedTestFeatures4 = torch.cat((selectedNormTestFeatures1, testNormFeatures4), dim=1)\n",
    "    concatenatedValFeatures4 = torch.cat((selectedNormValFeatures1, valNormFeatures4), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures4)\n",
    "    testList.append(concatenatedTestFeatures4)\n",
    "    valList.append(concatenatedValFeatures4)\n",
    "    #20\n",
    "    trainNormFeatures5 = get_norm_features(trainFeaturesList[4].detach().numpy())\n",
    "    testNormFeatures5 = get_norm_features(testFeaturesList[4].detach().numpy())\n",
    "    valNormFeatures5 = get_norm_features(valFeaturesList[4].detach().numpy())\n",
    "    concatenatedTrainFeatures5 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures5), dim=1)\n",
    "    concatenatedTestFeatures5 = torch.cat((selectedNormTestFeatures1, testNormFeatures5), dim=1)\n",
    "    concatenatedValFeatures5 = torch.cat((selectedNormValFeatures1, valNormFeatures5), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures5)\n",
    "    testList.append(concatenatedTestFeatures5)\n",
    "    valList.append(concatenatedValFeatures5)\n",
    "\n",
    "    #21\n",
    "    trainNormFeatures6 = get_norm_features(trainFeaturesList[5].detach().numpy())\n",
    "    testNormFeatures6 = get_norm_features(testFeaturesList[5].detach().numpy())\n",
    "    valNormFeatures6 = get_norm_features(valFeaturesList[5].detach().numpy())\n",
    "    concatenatedTrainFeatures6 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures6), dim=1)\n",
    "    concatenatedTestFeatures6 = torch.cat((selectedNormTestFeatures1, testNormFeatures6), dim=1)\n",
    "    concatenatedValFeatures6 = torch.cat((selectedNormValFeatures1, valNormFeatures6), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures6)\n",
    "    testList.append(concatenatedTestFeatures6)\n",
    "    valList.append(concatenatedValFeatures6)\n",
    "\n",
    "    #22\n",
    "    trainNormFeatures7 = get_norm_features(trainFeaturesList[6].detach().numpy())\n",
    "    testNormFeatures7 = get_norm_features(testFeaturesList[6].detach().numpy())\n",
    "    valNormFeatures7 = get_norm_features(valFeaturesList[6].detach().numpy())\n",
    "    concatenatedTrainFeatures7 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures7), dim=1)\n",
    "    concatenatedTestFeatures7 = torch.cat((selectedNormTestFeatures1, testNormFeatures7), dim=1)\n",
    "    concatenatedValFeatures7 = torch.cat((selectedNormValFeatures1, valNormFeatures7), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures7)\n",
    "    testList.append(concatenatedTestFeatures7)\n",
    "    valList.append(concatenatedValFeatures7)\n",
    "\n",
    "    trainList_tensors = [torch.tensor(item) for item in trainList]\n",
    "    testList_tensors = [torch.tensor(item) for item in testList]\n",
    "    valList_tensors = [torch.tensor(item) for item in valList]\n",
    "\n",
    "    with open(file_path1, 'wb') as file:\n",
    "        pickle.dump(trainList_tensors, file)\n",
    "    with open(file_path2, 'wb') as file:\n",
    "        pickle.dump(testList_tensors, file)\n",
    "    with open(file_path3, 'wb') as file:\n",
    "        pickle.dump(valList_tensors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59343465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8647e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# for item in trainFeaturesList:\n",
    "#     print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ebb893e7",
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "def to_tensor(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return torch.tensor(data)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")\n",
    "        \n",
    "def prep_data(features, labels, isOversample):\n",
    "    num_classes = 7\n",
    "\n",
    "    if isOversample:\n",
    "        X_set, Y_set = oversample_data(features, labels, num_classes)\n",
    "    else:\n",
    "        X_set, Y_set = features, labels\n",
    "\n",
    "    if isinstance(X_set, torch.Tensor):\n",
    "        X_tensor = X_set.float()\n",
    "    else:\n",
    "        X_tensor = torch.tensor(X_set, dtype=torch.float32)\n",
    "    \n",
    "    if isinstance(Y_set, torch.Tensor):\n",
    "        Y_tensor = Y_set.long()\n",
    "    else:\n",
    "        Y_tensor = torch.tensor(Y_set, dtype=torch.long)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(Y_set, return_counts=True)\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4bff2",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71c886",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def model_train1(X_set, Y_set, num_epochs=20, batch_size=32, loss_difference_threshold=0.01, \n",
    "#                  hidden_dims=[256, 128], dropout_rate=0.5, lr=0.0001, \n",
    "#                  optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, matchAtt\n",
    "#                  ):\n",
    "#     output_dim = 7  # Number of classes\n",
    "#     model = MyNetwork(len(X_set[0]), hidden_dims, output_dim, dropout_rate)\n",
    "#     criterion = criterion_class()\n",
    "#     optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "#     loss_history = []\n",
    "#     accuracy_history = []\n",
    "# #     print_interval = 1  # Print tqdm every epoch\n",
    "#     previous_loss = float('inf')\n",
    "\n",
    "#     # Create dataset and dataloader\n",
    "#     dataset = TensorDataset(X_set, Y_set)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     epoch_num = num_epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         correct_predictions = 0\n",
    "#         total_instances = 0\n",
    "#         with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "#             for inputs, labels in dataloader:\n",
    "#                 inputs = inputs.float()  # Ensure inputs are float32\n",
    "#                 labels = labels.long()   # Ensure labels are long\n",
    "#                 optimizer.zero_grad()\n",
    "# #                 TODO\n",
    "#                 outputs = model(inputs)\n",
    "#                 outputs = outputs.squeeze()\n",
    "#                 labels = labels.squeeze()\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # Check for NaN loss values\n",
    "#                 if torch.isnan(loss):\n",
    "#                     print(\"NaN loss encountered. Skipping this batch.\")\n",
    "#                     break\n",
    "                \n",
    "#                 # Apply gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, dim=1)\n",
    "#                 correct_predictions += (predicted == labels).sum().item()\n",
    "#                 total_instances += labels.size(0)\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#         epoch_loss = total_loss / total_instances\n",
    "#         epoch_accuracy = correct_predictions / total_instances\n",
    "#         loss_history.append(epoch_loss)\n",
    "#         accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "#         if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "#             epoch_num = epoch\n",
    "#             break\n",
    "\n",
    "#         previous_loss = epoch_loss\n",
    "\n",
    "#     return model, epoch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a0827f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X_set, Y_set):\n",
    "    indices = np.arange(len(X_set))\n",
    "    np.random.shuffle(indices)\n",
    "    return X_set[indices], Y_set[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5237e4fd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train1(X_set=None, Y_set=None, num_epochs=50, loss_difference_threshold=0.0001, \n",
    "                 hidden_dims=[64, 32], dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = MyNetwork(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=7, dropout_rate=0.5)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "                if nodalAtt:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                else:\n",
    "                    loss.backward(retain_graph=False)\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "        if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "            epoch_num = epoch\n",
    "            break\n",
    "\n",
    "        previous_loss = epoch_loss\n",
    "\n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae7cd0",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9a03e2cd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train2(X_set=None, Y_set=None, num_epochs=20, loss_difference_threshold=0.001, \n",
    "                 hidden_dims=128, dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    output_dim = 7  # Number of classes\n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = FCClassifier(input_dim, hidden_dims, output_dim, dropout_rate)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "#                 if nodalAtt:\n",
    "                loss.backward(retain_graph=True)\n",
    "#                 else:\n",
    "#                     loss.backward(retain_graph=False)\n",
    "                    \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(log_prob, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "                \n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "        \n",
    "        if epoch > 0:\n",
    "            loss_diff = abs(epoch_loss - previous_loss)\n",
    "            if loss_diff < loss_difference_threshold:\n",
    "                print(f\"Training stopped early at epoch {epoch+1}.\")\n",
    "                print(f\"Loss difference ({loss_diff}) is below the threshold ({loss_difference_threshold}).\")\n",
    "                epoch_num = epoch + 1\n",
    "                break\n",
    "        \n",
    "        previous_loss = epoch_loss\n",
    "    \n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b435c98b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_emotions(model=None, X_set=None, Y_set=None, typeSet=None, \n",
    "                      isSimpleFC=False, i_dict=None,\n",
    "                      nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize empty lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    i = 0\n",
    "    # Iterate over the given ranges\n",
    "    for start, end in ranges:\n",
    "        X_batch = X_set[start:end+1].float()\n",
    "        Y_batch = Y_set[start:end+1].long()\n",
    "\n",
    "#         if X_batch.dtype != torch.float32:\n",
    "#             X_batch = X_batch.float()\n",
    "\n",
    "        # Use no_grad to save memory and computations\n",
    "        with torch.no_grad():\n",
    "            inputs = X_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            labels = Y_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            last_idx = seq_len[i][0]\n",
    "            \n",
    "            umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "            outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "\n",
    "            outputs = outputs.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "            log_prob = F.log_softmax(outputs, 1)\n",
    "\n",
    "            _, predicted = torch.max(log_prob, 1)\n",
    "\n",
    "            # Append the predictions and labels to the lists\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            i = i+1\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_decoder.values(), output_dict=True, zero_division=0)\n",
    "\n",
    "    # Calculate the required metrics\n",
    "    accuracy = report['accuracy']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    f1_micro = report.get('micro avg', {}).get('f1-score', accuracy)\n",
    "    f1_macro = report.get('macro avg', {}).get('f1-score', 0.0) \n",
    "\n",
    "    if typeSet == \"validation\":\n",
    "        print(\"Classified: \", dictKey[i_dict])\n",
    "    \n",
    "    return dictKey[i_dict], typeSet, isSimpleFC, accuracy, recall, weighted_f1, f1_micro, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35df7e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for trainSet, testSet, valSet, _, _, _ in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "#     print(i, type(trainSet))\n",
    "#     if isinstance(trainSet, list):\n",
    "#         print(type(trainSet[0]))\n",
    "#         sample = trainSet[0]\n",
    "#         print(sample.shape)\n",
    "#     else:\n",
    "#         print(trainSet.squeeze(0).shape)\n",
    "#     i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d9231db5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getGraphComponents(file_path):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    if checkFile:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths = pickle.load(file)  \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3a83d29f",
   "metadata": {
    "code_folding": [
     4,
     7,
     10
    ]
   },
   "outputs": [],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl'\n",
    "\n",
    "train_umask, train_seq_lengths, train_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path1)\n",
    "\n",
    "test_umask, test_seq_lengths, test_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path2)\n",
    "\n",
    "val_umask, val_seq_lengths, val_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1984b",
   "metadata": {},
   "source": [
    "UpdateJune 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "741daeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160 2160\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_umask), len(train_seq_lengths))\n",
    "print(train_seq_lengths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "163d2e11",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getSpeakersAndRanges(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        encodedSpeakers, ranges = pickle.load(file)\n",
    "    file.close()\n",
    "    return encodedSpeakers, ranges\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\"\n",
    "\n",
    "encodedSpeakersTrain, rangesTrain = getSpeakersAndRanges(file_path1)\n",
    "encodedSpeakersTest, rangesTest = getSpeakersAndRanges(file_path2)\n",
    "encodedSpeakersDev, rangesDev = getSpeakersAndRanges(file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60304bfd",
   "metadata": {},
   "source": [
    "Sample run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f1571",
   "metadata": {},
   "source": [
    "Verifying the attention in batch before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9385da0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class MatchingAttention(nn.Module):\n",
    "#     def __init__(self, mem_dim, cand_dim, alpha_dim, att_type='general2'):\n",
    "#         super(MatchingAttention, self).__init__()\n",
    "#         self.mem_dim = mem_dim\n",
    "#         self.cand_dim = cand_dim\n",
    "#         self.alpha_dim = alpha_dim\n",
    "#         self.att_type = att_type\n",
    "\n",
    "#         if self.att_type == 'general2':\n",
    "#             self.transform = nn.Linear(self.mem_dim, self.cand_dim * self.alpha_dim)\n",
    "\n",
    "#     def forward(self, M, x, mask):\n",
    "#         M_ = M.permute(1, 2, 0)  # (batch, mem_dim, seq_len)\n",
    "#         x_ = self.transform(x).unsqueeze(1)  # (batch, 1, cand_dim * alpha_dim)\n",
    "#         mask_ = mask.unsqueeze(2).repeat(1, 1, self.mem_dim).transpose(1, 2)  # (batch, mem_dim, seq_len)\n",
    "        \n",
    "#         M_ = M_ * mask_\n",
    "#         alpha = torch.bmm(x_, M_)  # (batch, 1, seq_len)\n",
    "        \n",
    "#         alpha = F.softmax(alpha, dim=-1)  # Apply softmax to get attention weights\n",
    "#         attended = torch.bmm(alpha, M.permute(1, 0, 2))  # (batch, 1, mem_dim)\n",
    "        \n",
    "#         return attended.squeeze(1), alpha\n",
    "    \n",
    "# def attentive_node_features(emotions, seq_lengths, umask, matchatt_layer):\n",
    "#     max_len = max(seq_lengths)\n",
    "#     batch_size = len(seq_lengths)\n",
    "#     mem_dim = emotions.size(1)\n",
    "\n",
    "#     padded_emotions = []\n",
    "#     for i in range(batch_size):\n",
    "#         length = seq_lengths[i]\n",
    "#         # Assuming emotions is already a 2D tensor of shape (seq_len, mem_dim)\n",
    "#         padded_emotion = F.pad(emotions[:length], (0, 0, 0, max_len - length), \"constant\", 0)\n",
    "#         padded_emotions.append(padded_emotion)\n",
    "\n",
    "#     emotions_padded = torch.stack(padded_emotions, dim=1)  # (max_len, batch_size, mem_dim)\n",
    "    \n",
    "#     att_emotions = []\n",
    "#     alpha_list = []\n",
    "#     for t in range(max_len):\n",
    "#         att_em, alpha = matchatt_layer(emotions_padded, emotions_padded[t], umask)\n",
    "#         att_emotions.append(att_em)\n",
    "#         alpha_list.append(alpha)\n",
    "\n",
    "#     att_emotions = torch.stack(att_emotions, dim=0)  # (max_len, batch_size, mem_dim)\n",
    "\n",
    "#     # Remove the singleton dimension for batch size 1\n",
    "#     att_emotions = att_emotions.squeeze(1)  # (seq_len, mem_dim)\n",
    "\n",
    "#     return att_emotions, alpha_list\n",
    "    \n",
    "# class MyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "#         super(MyNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dims[0])  # Adjust input_dim to match flattened shape\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=64, cand_dim=64, alpha_dim=1, att_type='general2')\n",
    "\n",
    "#     def forward(self, x, nodalAtt, seq_lengths, umask):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "        \n",
    "#         x = self.fc1(att_emotions)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# class FCClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "#         super(FCClassifier, self).__init__()\n",
    "#         self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=input_dim, cand_dim=input_dim, alpha_dim=1, att_type='general2')\n",
    "#     def forward(self, x=None, nodalAtt=None, seq_lengths=None, umask=None, no_cuda=True):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "            \n",
    "#         x = F.relu(self.linear1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.linear2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef46abb",
   "metadata": {},
   "source": [
    "part a test case (uncomment the top part to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dc9f6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# seq_lengths = [14]  # Sequence length matches the input tensor\n",
    "# umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "# inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "# # Initialize MatchingAttention\n",
    "# matchatt_layer = MatchingAttention(mem_dim=inputs.shape[1], cand_dim=inputs.shape[1], alpha_dim=1, att_type='general2')\n",
    "\n",
    "# # Call attentive_node_features\n",
    "# att_emotions, alpha_list = attentive_node_features(inputs, seq_lengths, umask, matchatt_layer)\n",
    "\n",
    "# # Print shapes for verification\n",
    "# print(\"att_emotions shape:\", att_emotions.shape)  # Should be (seq_len, mem_dim)\n",
    "# print(\"alpha_list shape:\", len(alpha_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "69e91b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [14]\n",
    "umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "model = MyNetwork(input_dim=inputs.shape[1], hidden_dims=[128, 30], output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  #.Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea58147",
   "metadata": {},
   "source": [
    "part b test case (uncomment the part and the function to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0fcc9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 7])\n"
     ]
    }
   ],
   "source": [
    "model = FCClassifier(input_dim=inputs.shape[1], hidden_dim=64, output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  # Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e24dbc",
   "metadata": {},
   "source": [
    "end of sample experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b7940",
   "metadata": {},
   "source": [
    "Actual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db90480",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c6a93e4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curerntly at bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.00049613937503451) is below the threshold (0.001).\n",
      "Curerntly at bert-select-few\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0005731455819257825) is below the threshold (0.001).\n",
      "Curerntly at bert-select-more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.00021303602003974742) is below the threshold (0.001).\n",
      "Curerntly at dgcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 4.\n",
      "Loss difference (4.615111050204623e-05) is below the threshold (0.001).\n",
      "Curerntly at dgcn-select\n",
      "This is skipped\n",
      "Curerntly at gatv1\n",
      "This is skipped\n",
      "Curerntly at gatv1-select\n",
      "This is skipped\n",
      "Curerntly at gatv1-edge\n",
      "This is skipped\n",
      "Curerntly at gatv1-edge-select\n",
      "This is skipped\n",
      "Curerntly at gatv2-edge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0006604176954688179) is below the threshold (0.001).\n",
      "Curerntly at gatv2-edge-select\n",
      "This is skipped\n",
      "Curerntly at rgat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 4.\n",
      "Loss difference (4.509611757375076e-05) is below the threshold (0.001).\n",
      "Curerntly at rgat-select\n",
      "This is skipped\n",
      "Curerntly at egat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0008510792088285979) is below the threshold (0.001).\n",
      "Curerntly at egat-select\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-dgcn\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv1\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv1-edge\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-gatv2-edge\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-rgat\n",
      "This is skipped\n",
      "Curerntly at bert-select-mod-egat\n",
      "This is skipped\n"
     ]
    }
   ],
   "source": [
    "dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        df_results_sorted = pickle.load(file)\n",
    "else:\n",
    "    results = []\n",
    "    num_epochs = 30\n",
    "    i = 0\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor = to_tensor(trainSet)\n",
    "        Y_tensor = to_tensor(y_train)\n",
    "\n",
    "        print(\"Curerntly at\", dictKey[i])\n",
    "#         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#                                    isSimpleFC=False, i_dict=i, \n",
    "#                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#                                    ranges=rangesTrain )\n",
    "#         results.append(result1)\n",
    "        if i in selected_combination:\n",
    "            model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "                                     umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "                                   seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "            X_tensor = to_tensor(testSet)\n",
    "            Y_tensor = to_tensor(y_test)\n",
    "            result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "                                       isSimpleFC=True, i_dict=i, \n",
    "                                       nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "                                       ranges=rangesTest)\n",
    "    #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "    #                                    isSimpleFC=True, i_dict=i, \n",
    "    #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "    #                                    ranges=rangesTrain)\n",
    "            results.append(result2)\n",
    "        else:\n",
    "            print(\"This is skipped\")\n",
    "        i = i+1\n",
    "\n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "    df_results = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2013cdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rgat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>egat</td>\n",
       "      <td>test</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.09201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_combination typeSet  isSimpleFC  Accuracy  Recall  Weighted-F1  \\\n",
       "0              bert    test        True     0.475   0.475     0.305932   \n",
       "1   bert-select-mod    test        True     0.475   0.475     0.305932   \n",
       "2  bert-select-more    test        True     0.475   0.475     0.305932   \n",
       "3              dgcn    test        True     0.475   0.475     0.305932   \n",
       "4        gatv2-edge    test        True     0.475   0.475     0.305932   \n",
       "5              rgat    test        True     0.475   0.475     0.305932   \n",
       "6              egat    test        True     0.475   0.475     0.305932   \n",
       "\n",
       "   F1-micro  F1-macro  \n",
       "0     0.475   0.09201  \n",
       "1     0.475   0.09201  \n",
       "2     0.475   0.09201  \n",
       "3     0.475   0.09201  \n",
       "4     0.475   0.09201  \n",
       "5     0.475   0.09201  \n",
       "6     0.475   0.09201  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4ed51",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e36b6",
   "metadata": {},
   "source": [
    "<h4> Select top 10 unique data combinations then tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36973bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 9\n",
    "counter = 0\n",
    "combination1 = []\n",
    "combination2 = []\n",
    "seen_combinations = set()\n",
    "\n",
    "for idx, row in df_results_sorted.iterrows():\n",
    "    if counter >= max_iterations:\n",
    "        break\n",
    "\n",
    "    if row['data_combination'] in seen_combinations:\n",
    "        continue\n",
    "\n",
    "    if row['isSimpleFC']:\n",
    "        combination2.append(row['data_combination'])\n",
    "    else:\n",
    "        combination1.append(row['data_combination'])\n",
    "\n",
    "    seen_combinations.add(row['data_combination'])\n",
    "    counter += 1\n",
    "\n",
    "# Display the results\n",
    "print(\"Combination 1 (isSimpleFC=False):\", combination1)\n",
    "print(\"Combination 2 (isSimpleFC=True):\", combination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54513411",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1 = [key for key, value in dictKey.items() if value in combination1]\n",
    "indices2 = [key for key, value in dictKey.items() if value in combination2]\n",
    "\n",
    "print(\"Indices for isSimpleFC=False:\", indices1)\n",
    "print(\"Indices for isSimpleFC=True:\", indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedTrainDeepList = [trainList[i] for i in indices1]\n",
    "selectedTestDeepList = [testList[i] for i in indices1]\n",
    "selectedValDeepList = [valList[i] for i in indices1]\n",
    "\n",
    "len(selectedTrainDeepList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccae3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainSet in selectedTrainList:\n",
    "#     print(type(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedTrainList = [trainList[i] for i in indices2]\n",
    "selectedTestList = [testList[i] for i in indices2]\n",
    "selectedValList = [valList[i] for i in indices2]\n",
    "\n",
    "len(selectedTrainList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c266050",
   "metadata": {},
   "source": [
    "<h4> Tuning using random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da2dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should call both model_train1 and 2\n",
    "\n",
    "def objective_func(X_train, X_test, X_val, y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,\n",
    "                  train_umask, test_umask, val_umask,\n",
    "                  train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                  rangesTrain, rangesTest, rangesDev, nodalAtt):\n",
    "    results = []\n",
    "    hyperparams_string = (\n",
    "        f'num_epochs={hyperparams[\"num_epochs\"]} '\n",
    "        f'loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]} '\n",
    "        f'hidden_dims={hyperparams[\"hidden_dims\"]} '\n",
    "        f'dropout_rate={hyperparams[\"dropout_rate\"]} '\n",
    "        f'learning_rate={hyperparams[\"learning_rate\"]} '\n",
    "        f'optimizers={hyperparams[\"optimizers\"]} '\n",
    "        f'criteria={hyperparams[\"criteria\"]}'\n",
    "    )    \n",
    "    print(hyperparams_string)\n",
    "            \n",
    "    X_train_tensor = to_tensor(X_train)\n",
    "    y_train_tensor = to_tensor(y_train).long()\n",
    "    X_val_tensor = to_tensor(X_val)\n",
    "    y_val_tensor = to_tensor(y_val).long()\n",
    "    X_test_tensor = to_tensor(X_test)\n",
    "    y_test_tensor = to_tensor(y_test).long()\n",
    "# train\n",
    "    start_time = time.time()\n",
    "    if isSimpleFC:\n",
    "        model, num_epoch = model_train2(X_set=X_train_tensor, Y_set=y_train_tensor, \n",
    "                            num_epochs=hyperparams[\"num_epochs\"], loss_difference_threshold=hyperparams[\"loss_difference_threshold\"], \n",
    "                            hidden_dims=hyperparams[\"hidden_dims\"], dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "                            lr=hyperparams[\"learning_rate\"], optimizer_class=hyperparams[\"optimizers\"], \n",
    "                            criterion_class=hyperparams[\"criteria\"],\n",
    "                            umask=train_umask, seq_len=train_seq_lengths, ranges=rangesTrain, nodalAtt=nodalAtt)        \n",
    "#     else:\n",
    "#         model, num_epoch = model_train1(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "#                             hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "#                             hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "#                             hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "# val\n",
    "    result = classify_emotions(model=model, X_set=X_val_tensor, Y_set=y_val_tensor, typeSet='validation', \n",
    "                               isSimpleFC=isSimpleFC, i_dict=i_dict,\n",
    "                               nodalAtt=nodalAtt, umask=val_umask, seq_len=val_seq_lengths, ranges=rangesDev)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    result = list(result)\n",
    "    hyperparams_string = f'num_epochs={hyperparams[\"num_epochs\"]}-loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]}-hidden_dims={hyperparams[\"hidden_dims\"]}-dropout_rate={hyperparams[\"dropout_rate\"]}-learning_rate={hyperparams[\"learning_rate\"]}-optimizers={hyperparams[\"optimizers\"]}-criteria={hyperparams[\"criteria\"]}'\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "# test\n",
    "#     result = classify_emotions(model, X_test_tensor, y_test_tensor, \\\n",
    "#                                'test', isSimpleFC, i_dict)\n",
    "    \n",
    "#     result = list(result)\n",
    "#     result.append(elapsed_time)\n",
    "#     result.append(hyperparams_string)\n",
    "#     result.append(num_epoch)\n",
    "#     results.append(result)\n",
    "    \n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='data_combination', ascending=False)\n",
    "    \n",
    "    return df_results_sorted\n",
    "\n",
    "\n",
    "# def objective_func(X_train, X_test, X_val, \n",
    "#                y_train, y_test, y_val, hyperparams, i_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f998054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X_train=None, X_test=None, X_val=None, \n",
    "                  y_train=None, y_test=None, y_val=None, \n",
    "                  param_grid=None, isSimpleFC=True, i_dict=None,\n",
    "                  train_umask=None, test_umask=None, val_umask=None,\n",
    "                  train_seq_lengths=None, test_seq_lengths=None, val_seq_lengths=None,\n",
    "                  rangesTrain=None, rangesTest=None, rangesDev=None, nodalAtt=False\n",
    "                 ):\n",
    "    sub_total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    MAX_EVALS = 5\n",
    "    for i in range(MAX_EVALS):\n",
    "        hyperparams = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        try:\n",
    "            new_results = objective_func(X_train, X_test, X_val,  y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,  \n",
    "                                        train_umask, test_umask, val_umask,\n",
    "                                        train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                        rangesTrain, rangesTest, rangesDev, nodalAtt)\n",
    "            sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparams {hyperparams}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    return sub_total_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7295744e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [[256, 128], [128, 64], [64, 32]],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}\n",
    "param_grid2 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [128, 256, 512],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcf2b3",
   "metadata": {},
   "source": [
    "<h5> First find the best hyperparameter combination for the DeepClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "163a1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamTuning(X_trainSet, X_testSet, X_valSet, y_train, y_test, y_val, isSimpleFC, param_grid, indices,\n",
    "                    train_umask, test_umask, val_umask,\n",
    "                    train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                    rangesTrain, rangesTest, rangesDev, nodalAttn):\n",
    "    \n",
    "    total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    \n",
    "    for i in indices:\n",
    "        print(\"============\", dictKey[i], \"============\")\n",
    "        X_train = X_trainSet[i]\n",
    "        X_test = X_testSet[i]\n",
    "        X_val = X_valSet[i]\n",
    "\n",
    "        sub_total_results = random_search(X_train, X_test, X_val, y_train, y_test, y_val,\n",
    "                     param_grid, isSimpleFC, i,\n",
    "                     train_umask, test_umask, val_umask,\n",
    "                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                     rangesTrain, rangesTest, rangesDev, nodalAttn[i])\n",
    "\n",
    "        total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be82a8",
   "metadata": {},
   "source": [
    "Uncomment below next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e1fd4d3",
   "metadata": {
    "code_folding": [
     3,
     6
    ]
   },
   "outputs": [],
   "source": [
    "# file_path = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/results/deep_classifier_tuned_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         total_results1_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     total_results1 = hyperparamTuning(selectedTrainDeepList, selectedTestDeepList, selectedValDeepList, \\\n",
    "#                                  y_train, y_test, y_val, False, param_grid1, indices1)\n",
    "    \n",
    "#     total_results1_sorted = total_results1.sort_values(by='Weighted-F1', ascending=False)\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(total_results1_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90c5d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "# pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "# pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "\n",
    "# total_results1_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1ea75ad",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "# dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# file_path = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         df_results_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     results = []\n",
    "#     num_epochs = 30\n",
    "#     i = 0\n",
    "#     for trainSet, testSet, valSet in dataLoader:\n",
    "#         if isinstance(trainSet, list):\n",
    "#             trainSet = trainSet[0].squeeze(0)\n",
    "#             testSet = testSet[0].squeeze(0)\n",
    "#             valSet = valSet[0].squeeze(0)\n",
    "#         else:\n",
    "#             trainSet = trainSet.squeeze(0)\n",
    "#             testSet = testSet.squeeze(0)\n",
    "#             valSet = valSet.squeeze(0)\n",
    "\n",
    "#         X_tensor = to_tensor(trainSet)\n",
    "#         Y_tensor = to_tensor(y_train)\n",
    "\n",
    "#         print(\"Curerntly at\", dictKey[i])\n",
    "# #         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "# #                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "# #                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "# #         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "# #                                    isSimpleFC=False, i_dict=i, \n",
    "# #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "# #                                    ranges=rangesTrain )\n",
    "# #         results.append(result1)\n",
    "#         if i in selected_combination:\n",
    "#             model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                      umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                    seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#             X_tensor = to_tensor(testSet)\n",
    "#             Y_tensor = to_tensor(y_test)\n",
    "#             result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "#                                        isSimpleFC=True, i_dict=i, \n",
    "#                                        nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "#                                        ranges=rangesTest)\n",
    "#     #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#     #                                    isSimpleFC=True, i_dict=i, \n",
    "#     #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#     #                                    ranges=rangesTrain)\n",
    "#             results.append(result2)\n",
    "#         else:\n",
    "#             print(\"This is skipped\")\n",
    "#         i = i+1\n",
    "\n",
    "#     columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "#     df_results = pd.DataFrame(results, columns=columns)\n",
    "#     df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6d089c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ bert ============\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 6.\n",
      "Loss difference (0.0004604397788411929) is below the threshold (0.001).\n",
      "Classified:  bert\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 12.\n",
      "Loss difference (0.0007768416289831992) is below the threshold (0.001).\n",
      "Classified:  bert\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 12.\n",
      "Loss difference (4.201539766007789e-05) is below the threshold (0.0001).\n",
      "Classified:  bert\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1322761683.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 7.\n",
      "Loss difference (7.207478437212811e-05) is below the threshold (0.001).\n",
      "Classified:  bert\n",
      "============ bert-select-few ============\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (0.0003129016451328681) is below the threshold (0.001).\n",
      "Classified:  bert-select-few\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 10.\n",
      "Loss difference (2.4417912546925713e-05) is below the threshold (0.0001).\n",
      "Classified:  bert-select-few\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 21.\n",
      "Loss difference (7.753028574972176e-05) is below the threshold (0.0001).\n",
      "Classified:  bert-select-few\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 10.\n",
      "Loss difference (4.900157788181869e-05) is below the threshold (0.0001).\n",
      "Classified:  bert-select-few\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 4.\n",
      "Loss difference (0.0007062072378243855) is below the threshold (0.001).\n",
      "Classified:  bert-select-few\n",
      "============ bert-select-mod ============\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 13.\n",
      "Loss difference (0.000944984863828463) is below the threshold (0.001).\n",
      "Classified:  bert-select-mod\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 19.\n",
      "Loss difference (0.00022787867676804519) is below the threshold (0.001).\n",
      "Classified:  bert-select-mod\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 9.\n",
      "Loss difference (0.0009377919326063466) is below the threshold (0.001).\n",
      "Classified:  bert-select-mod\n",
      "============ bert-select-more ============\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 16.\n",
      "Loss difference (7.715267404867499e-05) is below the threshold (0.0001).\n",
      "Classified:  bert-select-more\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 14.\n",
      "Loss difference (3.8908676148557975e-05) is below the threshold (0.0001).\n",
      "Classified:  bert-select-more\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 27.\n",
      "Loss difference (6.968463202386421e-05) is below the threshold (0.0001).\n",
      "Classified:  bert-select-more\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.7 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-more\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 9.\n",
      "Loss difference (0.00010398213644263743) is below the threshold (0.001).\n",
      "Classified:  bert-select-more\n",
      "============ dgcn ============\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0007056516418204595) is below the threshold (0.001).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  dgcn\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (9.84977130021325e-05) is below the threshold (0.0001).\n",
      "Classified:  dgcn\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0003766950626054033) is below the threshold (0.001).\n",
      "Classified:  dgcn\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (8.09265352855193e-05) is below the threshold (0.001).\n",
      "Classified:  dgcn\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (4.847503769156436e-05) is below the threshold (0.001).\n",
      "Classified:  dgcn\n",
      "============ gatv2-edge ============\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  gatv2-edge\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 9.\n",
      "Loss difference (0.00047503670103082873) is below the threshold (0.001).\n",
      "Classified:  gatv2-edge\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (0.0009560592356500641) is below the threshold (0.001).\n",
      "Classified:  gatv2-edge\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.0009196931885039339) is below the threshold (0.001).\n",
      "Classified:  gatv2-edge\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (0.000917254950027202) is below the threshold (0.001).\n",
      "Classified:  gatv2-edge\n",
      "============ rgat ============\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=256 dropout_rate=0.5 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 4.\n",
      "Loss difference (0.0005699638172845245) is below the threshold (0.001).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  rgat\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 5.\n",
      "Loss difference (4.8530705249960615e-05) is below the threshold (0.0001).\n",
      "Classified:  rgat\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 11.\n",
      "Loss difference (9.728065137737385e-05) is below the threshold (0.0001).\n",
      "Classified:  rgat\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 20.\n",
      "Loss difference (5.554352361836212e-05) is below the threshold (0.0001).\n",
      "Classified:  rgat\n",
      "num_epochs=20 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 6.\n",
      "Loss difference (0.00044573747851767154) is below the threshold (0.001).\n",
      "Classified:  rgat\n",
      "============ egat ============\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.5 learning_rate=0.0001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 8.\n",
      "Loss difference (5.566147721813852e-05) is below the threshold (0.0001).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  egat\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  egat\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 36.\n",
      "Loss difference (7.221699225207212e-06) is below the threshold (0.0001).\n",
      "Classified:  egat\n",
      "num_epochs=40 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.7 learning_rate=0.001 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  egat\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  egat\n",
      "============ bert-select-mod-egat ============\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.3 learning_rate=0.0001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 11.\n",
      "Loss difference (4.679436484972266e-05) is below the threshold (0.0001).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edayo\\AppData\\Local\\Temp\\ipykernel_17840\\1387583240.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod-egat\n",
      "num_epochs=30 loss_difference_threshold=0.001 hidden_dims=512 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 10.\n",
      "Loss difference (0.00037798594184385026) is below the threshold (0.001).\n",
      "Classified:  bert-select-mod-egat\n",
      "num_epochs=40 loss_difference_threshold=0.001 hidden_dims=128 dropout_rate=0.3 learning_rate=0.001 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 3.\n",
      "Loss difference (0.000999482904453508) is below the threshold (0.001).\n",
      "Classified:  bert-select-mod-egat\n",
      "num_epochs=20 loss_difference_threshold=0.0001 hidden_dims=256 dropout_rate=0.5 learning_rate=1e-05 optimizers=<class 'torch.optim.sgd.SGD'> criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified:  bert-select-mod-egat\n",
      "num_epochs=30 loss_difference_threshold=0.0001 hidden_dims=512 dropout_rate=0.7 learning_rate=1e-05 optimizers=<class 'torch.optim.adam.Adam'> criteria=<class 'torch.nn.modules.loss.NLLLoss'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped early at epoch 18.\n",
      "Loss difference (2.9386904937811487e-05) is below the threshold (0.0001).\n",
      "Classified:  bert-select-mod-egat\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/CNNlstm_data_for_classifier/results/simple_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results2_sorted = pickle.load(file)\n",
    "else: \n",
    "\n",
    "    total_results2 = hyperparamTuning(trainList, testList, valList, \n",
    "                                     y_train, y_test, y_val, True, param_grid2, indices2,\n",
    "                                     train_umask, test_umask, val_umask,\n",
    "                                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                     rangesTrain, rangesTest, rangesDev, nodalAttn)\n",
    "    \n",
    "    total_results2_sorted = total_results2.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results2_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7797910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_combination</th>\n",
       "      <th>typeSet</th>\n",
       "      <th>isSimpleFC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>train_time</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>num_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.545733</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.386943</td>\n",
       "      <td>106.788389</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.573871</td>\n",
       "      <td>0.573871</td>\n",
       "      <td>0.532911</td>\n",
       "      <td>0.573871</td>\n",
       "      <td>0.342116</td>\n",
       "      <td>78.692427</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.580027</td>\n",
       "      <td>0.580027</td>\n",
       "      <td>0.529895</td>\n",
       "      <td>0.580027</td>\n",
       "      <td>0.338584</td>\n",
       "      <td>66.035404</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.579343</td>\n",
       "      <td>0.579343</td>\n",
       "      <td>0.524875</td>\n",
       "      <td>0.579343</td>\n",
       "      <td>0.330575</td>\n",
       "      <td>42.326295</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.575239</td>\n",
       "      <td>0.575239</td>\n",
       "      <td>0.518920</td>\n",
       "      <td>0.575239</td>\n",
       "      <td>0.324207</td>\n",
       "      <td>34.917133</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.567031</td>\n",
       "      <td>0.567031</td>\n",
       "      <td>0.514721</td>\n",
       "      <td>0.567031</td>\n",
       "      <td>0.334240</td>\n",
       "      <td>119.823606</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.562244</td>\n",
       "      <td>0.562244</td>\n",
       "      <td>0.512913</td>\n",
       "      <td>0.562244</td>\n",
       "      <td>0.330443</td>\n",
       "      <td>54.761099</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.509489</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.317927</td>\n",
       "      <td>57.678365</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.506511</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.319688</td>\n",
       "      <td>17.214859</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.560192</td>\n",
       "      <td>0.560192</td>\n",
       "      <td>0.506247</td>\n",
       "      <td>0.560192</td>\n",
       "      <td>0.316441</td>\n",
       "      <td>39.391744</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.556088</td>\n",
       "      <td>0.556088</td>\n",
       "      <td>0.496736</td>\n",
       "      <td>0.556088</td>\n",
       "      <td>0.304790</td>\n",
       "      <td>58.695501</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.495876</td>\n",
       "      <td>0.557456</td>\n",
       "      <td>0.304213</td>\n",
       "      <td>29.272535</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.488934</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.295853</td>\n",
       "      <td>27.606318</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.483779</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.291834</td>\n",
       "      <td>58.193229</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.545144</td>\n",
       "      <td>0.545144</td>\n",
       "      <td>0.472533</td>\n",
       "      <td>0.545144</td>\n",
       "      <td>0.279634</td>\n",
       "      <td>10.751231</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bert-select-few</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.542408</td>\n",
       "      <td>0.542408</td>\n",
       "      <td>0.470522</td>\n",
       "      <td>0.542408</td>\n",
       "      <td>0.277127</td>\n",
       "      <td>18.003559</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bert</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.468970</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.271353</td>\n",
       "      <td>21.120929</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.450068</td>\n",
       "      <td>0.450068</td>\n",
       "      <td>0.371396</td>\n",
       "      <td>0.450068</td>\n",
       "      <td>0.199968</td>\n",
       "      <td>283.000438</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.463064</td>\n",
       "      <td>0.463064</td>\n",
       "      <td>0.335138</td>\n",
       "      <td>0.463064</td>\n",
       "      <td>0.162218</td>\n",
       "      <td>34.673511</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.427497</td>\n",
       "      <td>0.427497</td>\n",
       "      <td>0.318093</td>\n",
       "      <td>0.427497</td>\n",
       "      <td>0.149349</td>\n",
       "      <td>155.835428</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.435705</td>\n",
       "      <td>0.435705</td>\n",
       "      <td>0.302911</td>\n",
       "      <td>0.435705</td>\n",
       "      <td>0.136861</td>\n",
       "      <td>179.937583</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.409029</td>\n",
       "      <td>0.409029</td>\n",
       "      <td>0.250254</td>\n",
       "      <td>0.409029</td>\n",
       "      <td>0.091724</td>\n",
       "      <td>200.597507</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.374145</td>\n",
       "      <td>0.374145</td>\n",
       "      <td>0.241646</td>\n",
       "      <td>0.374145</td>\n",
       "      <td>0.096573</td>\n",
       "      <td>32.362034</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.385773</td>\n",
       "      <td>0.385773</td>\n",
       "      <td>0.241577</td>\n",
       "      <td>0.385773</td>\n",
       "      <td>0.091492</td>\n",
       "      <td>32.823786</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.241498</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.085728</td>\n",
       "      <td>262.674026</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.393981</td>\n",
       "      <td>0.393981</td>\n",
       "      <td>0.239953</td>\n",
       "      <td>0.393981</td>\n",
       "      <td>0.086905</td>\n",
       "      <td>19.500205</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.7-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>19.388897</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rgat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>86.329958</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>41.024316</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>142.536226</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>52.789766</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>197.389990</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.5-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rgat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>25.706190</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rgat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>38.500278</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rgat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>38.707846</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rgat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>156.801465</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>41.736100</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bert-select-mod</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>32.631166</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>259.530739</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bert-select-more</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>15.252840</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gatv2-edge</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>59.206540</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>26.778166</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>41.504362</td>\n",
       "      <td>num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=&lt;class 'torch.optim.adam.Adam'&gt;-criteria=&lt;class 'torch.nn.modules.loss.CrossEntropyLoss'&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-select-mod-egat</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>74.678922</td>\n",
       "      <td>num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=1e-05-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>dgcn</td>\n",
       "      <td>validation</td>\n",
       "      <td>True</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.410397</td>\n",
       "      <td>0.083137</td>\n",
       "      <td>20.275378</td>\n",
       "      <td>num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.0001-optimizers=&lt;class 'torch.optim.sgd.SGD'&gt;-criteria=&lt;class 'torch.nn.modules.loss.NLLLoss'&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_combination     typeSet isSimpleFC  Accuracy    Recall  \\\n",
       "42                  bert  validation       True  0.584815  0.584815   \n",
       "41                  bert  validation       True  0.573871  0.573871   \n",
       "44                  bert  validation       True  0.580027  0.580027   \n",
       "26      bert-select-more  validation       True  0.579343  0.579343   \n",
       "40                  bert  validation       True  0.575239  0.575239   \n",
       "27      bert-select-more  validation       True  0.567031  0.567031   \n",
       "37       bert-select-few  validation       True  0.562244  0.562244   \n",
       "30       bert-select-mod  validation       True  0.562927  0.562927   \n",
       "35       bert-select-few  validation       True  0.557456  0.557456   \n",
       "34       bert-select-mod  validation       True  0.560192  0.560192   \n",
       "32       bert-select-mod  validation       True  0.556088  0.556088   \n",
       "38       bert-select-few  validation       True  0.557456  0.557456   \n",
       "25      bert-select-more  validation       True  0.558140  0.558140   \n",
       "33       bert-select-mod  validation       True  0.551300  0.551300   \n",
       "39       bert-select-few  validation       True  0.545144  0.545144   \n",
       "36       bert-select-few  validation       True  0.542408  0.542408   \n",
       "43                  bert  validation       True  0.546512  0.546512   \n",
       "7                   egat  validation       True  0.450068  0.450068   \n",
       "28      bert-select-more  validation       True  0.463064  0.463064   \n",
       "0   bert-select-mod-egat  validation       True  0.427497  0.427497   \n",
       "9                   egat  validation       True  0.435705  0.435705   \n",
       "4   bert-select-mod-egat  validation       True  0.409029  0.409029   \n",
       "17            gatv2-edge  validation       True  0.374145  0.374145   \n",
       "19            gatv2-edge  validation       True  0.385773  0.385773   \n",
       "8                   egat  validation       True  0.410397  0.410397   \n",
       "18            gatv2-edge  validation       True  0.393981  0.393981   \n",
       "20                  dgcn  validation       True  0.410397  0.410397   \n",
       "12                  rgat  validation       True  0.410397  0.410397   \n",
       "2   bert-select-mod-egat  validation       True  0.410397  0.410397   \n",
       "3   bert-select-mod-egat  validation       True  0.410397  0.410397   \n",
       "5                   egat  validation       True  0.410397  0.410397   \n",
       "6                   egat  validation       True  0.410397  0.410397   \n",
       "10                  rgat  validation       True  0.410397  0.410397   \n",
       "11                  rgat  validation       True  0.410397  0.410397   \n",
       "14                  rgat  validation       True  0.410397  0.410397   \n",
       "13                  rgat  validation       True  0.410397  0.410397   \n",
       "21                  dgcn  validation       True  0.410397  0.410397   \n",
       "31       bert-select-mod  validation       True  0.410397  0.410397   \n",
       "15            gatv2-edge  validation       True  0.410397  0.410397   \n",
       "29      bert-select-more  validation       True  0.410397  0.410397   \n",
       "16            gatv2-edge  validation       True  0.410397  0.410397   \n",
       "24                  dgcn  validation       True  0.410397  0.410397   \n",
       "23                  dgcn  validation       True  0.410397  0.410397   \n",
       "1   bert-select-mod-egat  validation       True  0.410397  0.410397   \n",
       "22                  dgcn  validation       True  0.410397  0.410397   \n",
       "\n",
       "    Weighted-F1  F1-micro  F1-macro  train_time  \\\n",
       "42     0.545733  0.584815  0.386943  106.788389   \n",
       "41     0.532911  0.573871  0.342116   78.692427   \n",
       "44     0.529895  0.580027  0.338584   66.035404   \n",
       "26     0.524875  0.579343  0.330575   42.326295   \n",
       "40     0.518920  0.575239  0.324207   34.917133   \n",
       "27     0.514721  0.567031  0.334240  119.823606   \n",
       "37     0.512913  0.562244  0.330443   54.761099   \n",
       "30     0.509489  0.562927  0.317927   57.678365   \n",
       "35     0.506511  0.557456  0.319688   17.214859   \n",
       "34     0.506247  0.560192  0.316441   39.391744   \n",
       "32     0.496736  0.556088  0.304790   58.695501   \n",
       "38     0.495876  0.557456  0.304213   29.272535   \n",
       "25     0.488934  0.558140  0.295853   27.606318   \n",
       "33     0.483779  0.551300  0.291834   58.193229   \n",
       "39     0.472533  0.545144  0.279634   10.751231   \n",
       "36     0.470522  0.542408  0.277127   18.003559   \n",
       "43     0.468970  0.546512  0.271353   21.120929   \n",
       "7      0.371396  0.450068  0.199968  283.000438   \n",
       "28     0.335138  0.463064  0.162218   34.673511   \n",
       "0      0.318093  0.427497  0.149349  155.835428   \n",
       "9      0.302911  0.435705  0.136861  179.937583   \n",
       "4      0.250254  0.409029  0.091724  200.597507   \n",
       "17     0.241646  0.374145  0.096573   32.362034   \n",
       "19     0.241577  0.385773  0.091492   32.823786   \n",
       "8      0.241498  0.410397  0.085728  262.674026   \n",
       "18     0.239953  0.393981  0.086905   19.500205   \n",
       "20     0.238834  0.410397  0.083137   19.388897   \n",
       "12     0.238834  0.410397  0.083137   86.329958   \n",
       "2      0.238834  0.410397  0.083137   41.024316   \n",
       "3      0.238834  0.410397  0.083137  142.536226   \n",
       "5      0.238834  0.410397  0.083137   52.789766   \n",
       "6      0.238834  0.410397  0.083137  197.389990   \n",
       "10     0.238834  0.410397  0.083137   25.706190   \n",
       "11     0.238834  0.410397  0.083137   38.500278   \n",
       "14     0.238834  0.410397  0.083137   38.707846   \n",
       "13     0.238834  0.410397  0.083137  156.801465   \n",
       "21     0.238834  0.410397  0.083137   41.736100   \n",
       "31     0.238834  0.410397  0.083137   32.631166   \n",
       "15     0.238834  0.410397  0.083137  259.530739   \n",
       "29     0.238834  0.410397  0.083137   15.252840   \n",
       "16     0.238834  0.410397  0.083137   59.206540   \n",
       "24     0.238834  0.410397  0.083137   26.778166   \n",
       "23     0.238834  0.410397  0.083137   41.504362   \n",
       "1      0.238834  0.410397  0.083137   74.678922   \n",
       "22     0.238834  0.410397  0.083137   20.275378   \n",
       "\n",
       "                                                                                                                                                                                                  hyperparams  \\\n",
       "42            num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "41  num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "44            num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "26           num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "40   num_epochs=30-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "27   num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "37            num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "30             num_epochs=40-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "35             num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "34    num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "32              num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "38  num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "25     num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "33   num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=1e-05-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "39    num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "36              num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "43     num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "7            num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "28    num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "0   num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "9             num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "4             num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=1e-05-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "17               num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "19      num_epochs=30-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "8      num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "18      num_epochs=20-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.7-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "20              num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "12   num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.7-learning_rate=1e-05-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "2              num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "3      num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "5     num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "6      num_epochs=30-loss_difference_threshold=0.0001-hidden_dims=512-dropout_rate=0.5-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "10               num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "11   num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "14               num_epochs=20-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.3-learning_rate=0.001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "13   num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "21            num_epochs=20-loss_difference_threshold=0.0001-hidden_dims=256-dropout_rate=0.3-learning_rate=1e-05-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "31               num_epochs=20-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "15              num_epochs=40-loss_difference_threshold=0.0001-hidden_dims=128-dropout_rate=0.3-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "29               num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.5-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "16              num_epochs=40-loss_difference_threshold=0.001-hidden_dims=256-dropout_rate=0.3-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "24            num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "23    num_epochs=40-loss_difference_threshold=0.001-hidden_dims=128-dropout_rate=0.7-learning_rate=0.001-optimizers=<class 'torch.optim.adam.Adam'>-criteria=<class 'torch.nn.modules.loss.CrossEntropyLoss'>   \n",
       "1                num_epochs=30-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.7-learning_rate=1e-05-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "22              num_epochs=20-loss_difference_threshold=0.001-hidden_dims=512-dropout_rate=0.5-learning_rate=0.0001-optimizers=<class 'torch.optim.sgd.SGD'>-criteria=<class 'torch.nn.modules.loss.NLLLoss'>   \n",
       "\n",
       "   num_epoch  \n",
       "42        12  \n",
       "41        20  \n",
       "44         7  \n",
       "26        14  \n",
       "40         6  \n",
       "27        27  \n",
       "37        21  \n",
       "30        13  \n",
       "35         5  \n",
       "34         9  \n",
       "32        30  \n",
       "38        10  \n",
       "25        16  \n",
       "33        20  \n",
       "39         4  \n",
       "36        10  \n",
       "43        12  \n",
       "7         36  \n",
       "28        20  \n",
       "0         11  \n",
       "9         20  \n",
       "4         18  \n",
       "17         5  \n",
       "19         5  \n",
       "8         40  \n",
       "18         3  \n",
       "20         3  \n",
       "12        11  \n",
       "2          3  \n",
       "3         20  \n",
       "5          8  \n",
       "6         30  \n",
       "10         4  \n",
       "11         5  \n",
       "14         6  \n",
       "13        20  \n",
       "21         5  \n",
       "31        19  \n",
       "15        40  \n",
       "29         9  \n",
       "16         9  \n",
       "24         3  \n",
       "23         5  \n",
       "1         10  \n",
       "22         3  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "    \n",
    "total_results2_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
