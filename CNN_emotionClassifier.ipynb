{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a857cc",
   "metadata": {},
   "source": [
    "\"FC layers referenced from https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176f72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "from sklearn.utils import class_weight\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from graph_context_dataset import FeatureEngineeredDataset\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import random\n",
    "from model import FCClassifier, MyNetwork, DATASET_PATH, MatchingAttention\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b22dd",
   "metadata": {},
   "source": [
    "Make sure to specify which dataset to use\n",
    "\n",
    " - dataset_original\n",
    " - dataset_drop_noise\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6565cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\"\n",
    "dataset_path = DATASET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1470c3",
   "metadata": {},
   "source": [
    "<h3> Declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68406d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4fb045",
   "metadata": {
    "code_folding": [
     0,
     9,
     18,
     21,
     24,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# class FCLayer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(FCLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# class ActivationLayer(nn.Module):\n",
    "#     def __init__(self, activation_fn):\n",
    "#         super(ActivationLayer, self).__init__()\n",
    "#         self.activation_fn = activation_fn\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.activation_fn(x)\n",
    "#         return x\n",
    "\n",
    "# def tanh(x):\n",
    "#     return torch.tanh(x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return torch.sigmoid(x)\n",
    "# # loss function and its derivative\n",
    "# def mse(y_true, y_pred):\n",
    "#     return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "# def mse_prime(y_true, y_pred):\n",
    "#     return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246bf76e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def oversample_data(X_train, Y_train, num_classes):\n",
    "    # Determine the class with the maximum number of instances\n",
    "    max_class_count = np.max(np.bincount(Y_train))\n",
    "    # Generate indices for oversampling each class\n",
    "    indices_list = [np.where(Y_train == i)[0] for i in range(num_classes)]\n",
    "    # Oversample minority classes to match the count of the majority class\n",
    "    for i, indices in enumerate(indices_list):\n",
    "        if len(indices) < max_class_count:\n",
    "            # Calculate the number of instances to oversample for this class\n",
    "            num_to_oversample = max_class_count - len(indices)\n",
    "            # Randomly select instances with replacement to oversample\n",
    "            oversampled_indices = np.random.choice(indices, size=num_to_oversample, replace=True)\n",
    "            # Append the oversampled instances to the original data\n",
    "            X_train = np.concatenate((X_train, X_train[oversampled_indices]), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_train[oversampled_indices]), axis=0)\n",
    "    return torch.tensor(X_train), torch.tensor(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8349606b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    if not tensor_list:\n",
    "        raise ValueError(\"The tensor list is empty\")\n",
    "\n",
    "    feature_dim = tensor_list[0].shape[1]\n",
    "    for tensor in tensor_list:\n",
    "        if tensor.shape[1] != feature_dim:\n",
    "            raise ValueError(\"All tensors must have the same feature dimension\")\n",
    "    \n",
    "    concatenated_tensor = torch.cat(tensor_list, dim=0)\n",
    "    \n",
    "    return concatenated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503aae",
   "metadata": {},
   "source": [
    "<h4> Import labels and label decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/labels_train.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_test.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_test = pickle.load(file)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/labels_dev.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    y_val = pickle.load(file)\n",
    "y_val = torch.tensor(y_val)\n",
    "    \n",
    "file_path = 'data/dump/' + dataset_path + '/label_decoder.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    label_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3e716",
   "metadata": {},
   "source": [
    "<h4> Import the CNNBiLSTM base-node outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c53ba",
   "metadata": {},
   "source": [
    "first we disregard the u' and directly train the h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396cc340",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_CNNBiLSTM_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_DGCN_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv2_edgeAttr_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_RGAT_train.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_EGAT_train.pkl\",\n",
    "]\n",
    "\n",
    "test_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_CNNBiLSTM_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_DGCN_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv2_edgeAttr_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_RGAT_test.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_EGAT_test.pkl\",\n",
    "]\n",
    "\n",
    "val_file_paths = [\n",
    "    \"embed/\" + dataset_path + \"/u_prime_CNNBiLSTM_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_DGCN_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv1_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_GATv2_edgeAttr_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_RGAT_dev.pkl\",\n",
    "    \"embed/\" + dataset_path + \"/h_prime_CNNBiLSTM_EGAT_dev.pkl\",\n",
    "]\n",
    "\n",
    "dictKey = {\n",
    "    0 : 'cnn',\n",
    "    1 : 'cnn-select-few',\n",
    "    2 : 'cnn-select-mod',\n",
    "    3 : 'cnn-select-more',\n",
    "    4 : 'dgcn',\n",
    "    5 : 'dgcn-select',\n",
    "    6 : 'gatv1',\n",
    "    7 : 'gatv1-select',\n",
    "    8 : 'gatv1-edge',\n",
    "    9 : 'gatv1-edge-select',\n",
    "    10 : 'gatv2-edge',\n",
    "    11 : 'gatv2-edge-select',\n",
    "    12 : 'rgat',\n",
    "    13 : 'rgat-select',\n",
    "    14 : 'egat',\n",
    "    15 : 'egat-select',\n",
    "    16 : 'cnn-select-mod-dgcn',\n",
    "    17 : 'cnn-select-mod-gatv1',\n",
    "    18 : 'cnn-select-mod-gatv1-edge',\n",
    "    19 : 'cnn-select-mod-gatv2-edge',\n",
    "    20 : 'cnn-select-mod-rgat',\n",
    "    21 : 'cnn-select-mod-egat',\n",
    "}\n",
    "selected_combination = [0,2,3,4,10,12,14]\n",
    "# selected_combination = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff1721",
   "metadata": {},
   "source": [
    "<h4> Getting CNNlstm and GAT outputs for all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c366b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFeaturesList[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3426c4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embed/dataset_drop_noise/u_prime_CNNBiLSTM_train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                 featuresList\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m featuresList\n\u001b[1;32m---> 20\u001b[0m trainFeaturesList \u001b[38;5;241m=\u001b[39m \u001b[43mimport_h_prime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m testFeaturesList \u001b[38;5;241m=\u001b[39m import_h_prime(test_file_paths)\n\u001b[0;32m     24\u001b[0m valFeaturesList \u001b[38;5;241m=\u001b[39m import_h_prime(val_file_paths)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mimport_h_prime\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m      6\u001b[0m     attList \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[1;32m----> 8\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m             data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#             print(\"Check... \", len(data))\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'embed/dataset_drop_noise/u_prime_CNNBiLSTM_train.pkl'"
     ]
    }
   ],
   "source": [
    "def concatenate_tensors(tensor_list):\n",
    "    return torch.cat(tensor_list, dim=0)\n",
    "\n",
    "def import_h_prime(file_paths):\n",
    "    featuresList = []\n",
    "    attList = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "#             print(\"Check... \", len(data))\n",
    "            if isinstance(data, list):\n",
    "                print(type(data), \" instance of list: \", len(data))\n",
    "                featuresList.append(concatenate_tensors(data))\n",
    "            else:\n",
    "                print(type(data), \" instance of tensor, \", data.shape)\n",
    "                featuresList.append(data)\n",
    "                \n",
    "    return featuresList\n",
    "    \n",
    "trainFeaturesList = import_h_prime(train_file_paths)\n",
    "\n",
    "testFeaturesList = import_h_prime(test_file_paths)\n",
    "\n",
    "valFeaturesList = import_h_prime(val_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeaturesList[0] = trainFeaturesList[0].squeeze(1)\n",
    "testFeaturesList[0] = testFeaturesList[0].squeeze(1)\n",
    "valFeaturesList[0] = valFeaturesList[0].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeaturesList[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33882e42",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getNodalAttn():\n",
    "    nodalAttList = []\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(False)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    nodalAttList.append(True)\n",
    "    return nodalAttList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodalAttn = getNodalAttn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e7687",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e174164",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Checking the structure of graph\n",
    "# for n in range(10):\n",
    "#     tensor_data_np = tensor_utterances[n].detach().numpy()\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(range(len(tensor_data_np)), tensor_data_np)\n",
    "#     plt.title('Line Graph of Tensor Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a3b1f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (1st GAT)\n",
    "# data = cherry_picked_nodes.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a070ce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the h' (2nd GAT)\n",
    "# data = all_node_feats.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# # Print or analyze the similarity matrix\n",
    "# # print(similarities)\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fa315",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Normalize the u' or updated_representations\n",
    "# data = tensor_utterances.detach().numpy()\n",
    "# data_normalized = data / np.linalg.norm(data, axis=1, keepdims=True)\n",
    "\n",
    "# # Compute pairwise cosine similarities\n",
    "# similarities = cosine_similarity(data_normalized)\n",
    "\n",
    "# plt.hist(similarities.flatten(), bins=50, density=True)\n",
    "# plt.title('Distribution of Cosine Similarities')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40320052",
   "metadata": {},
   "source": [
    "<h3> Feature Selection and creating data combination for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f143f",
   "metadata": {},
   "source": [
    "Define select feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501b577",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_norm_features(encoded_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(torch.tensor(encoded_features).clone().detach().requires_grad_(True))\n",
    "    return torch.tensor(features_scaled)\n",
    "\n",
    "def get_selected_features(encoded_features, labels, top_n):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    print(encoded_features.shape)\n",
    "    features_scaled = scaler.fit_transform(encoded_features.detach().numpy())\n",
    "\n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        # Create a binary mask indicating instances belonging to the current class\n",
    "        mask = (labels == label)\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=top_n) \n",
    "        selector.fit(features_scaled, mask)  \n",
    "\n",
    "        top_features_indices = np.argsort(selector.scores_)[-top_n:]\n",
    "        scores = selector.scores_[top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "\n",
    "    selected_features = encoded_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ac5dd",
   "metadata": {
    "code_folding": [
     0,
     18,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Function to train the autoencoder\n",
    "def train_autoencoder(encoded_features, hidden_dim=100, num_epochs=20, lr=0.001):\n",
    "    input_dim = encoded_features.shape[1]\n",
    "    autoencoder = Autoencoder(input_dim, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    X_tensor = torch.tensor(encoded_features, dtype=torch.float32)\n",
    "    train_loader = torch.utils.data.DataLoader(X_tensor, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Autoencoder\", unit=\"epoch\"):\n",
    "        total_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs = data\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = autoencoder(inputs)\n",
    "            loss = criterion(decoded, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_features, _ = autoencoder(torch.tensor(encoded_features, dtype=torch.float32))\n",
    "    \n",
    "    return encoded_features, autoencoder\n",
    "\n",
    "def get_selected_features_autoencoder(autoencoder, encoded_features, labels, top_n=100):\n",
    "    if torch.is_tensor(encoded_features):\n",
    "        encoded_features = encoded_features.detach().cpu().numpy()\n",
    "\n",
    "    reduced_features = autoencoder.encoder(torch.tensor(encoded_features, dtype=torch.float32)).detach().numpy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = scaler.fit_transform(reduced_features)\n",
    "    \n",
    "    top_features_by_class = {}\n",
    "    top_scores = {}\n",
    "\n",
    "    for label in range(7):\n",
    "        mask = (labels == label)\n",
    "        selector = chi2(features_scaled, mask)\n",
    "        top_features_indices = np.argsort(selector[0])[-top_n:]\n",
    "        scores = selector[0][top_features_indices]\n",
    "\n",
    "        top_features_by_class[label] = top_features_indices\n",
    "        top_scores[label] = scores\n",
    "\n",
    "    concatenated_features_set = set()\n",
    "    for label, indices in top_features_by_class.items():\n",
    "        concatenated_features_set.update(indices)\n",
    "\n",
    "    concatenated_features_indices = list(concatenated_features_set)\n",
    "    concatenated_features_indices = np.array(concatenated_features_indices)\n",
    "\n",
    "    selected_features = reduced_features[:, concatenated_features_indices]\n",
    "\n",
    "    return selected_features, concatenated_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d79a94",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(selected_features.detach().numpy())\n",
    "\n",
    "# # Plot the PCA result with color-coded labels\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for label in np.unique(Y_train):\n",
    "#     indices = Y_train == label\n",
    "#     plt.scatter(pca_result[indices, 0], pca_result[indices, 1], label=f'{label_decoder[label]}', alpha=0.5)\n",
    "#     plt.title('PCA Visualization of Selected Utterance Embeddings (Train) with Color-Coded Labels')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e235",
   "metadata": {},
   "source": [
    "3d plottly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccc17d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# X_train = selected_features\n",
    "# X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# # Perform T-SNE dimensionality reduction\n",
    "# tsne = TSNE(n_components=3, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "# # Create a Plotly scatter plot\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=X_tsne[:, 0],\n",
    "#     y=X_tsne[:, 1],\n",
    "#     z=X_tsne[:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=Y_train,  # Assuming Y_train contains labels for coloring\n",
    "#         colorscale='Viridis',  # You can choose a different colorscale\n",
    "#         opacity=0.8\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(title='3D T-SNE Plot', autosize=False,\n",
    "#                   width=800, height=800)\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73f7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the plot as an HTML file\n",
    "# pio.write_html(fig, '3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaf772",
   "metadata": {},
   "source": [
    "Now prepare the data that will be ued to train the classifier, there are 20 combinations. And pick top 7 combinations yielding top F1 weighted-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2057f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61905fb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "trainList = []\n",
    "testList = []\n",
    "valList = []\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/trainList.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/testList.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/valList.pkl\"\n",
    "\n",
    "\n",
    "checkFile1 = os.path.isfile(file_path1)\n",
    "checkFile2 = os.path.isfile(file_path2)\n",
    "checkFile3 = os.path.isfile(file_path3)\n",
    "\n",
    "if checkFile1 and checkFile2 and checkFile3: \n",
    "    with open(file_path1, \"rb\") as file:\n",
    "        trainList = pickle.load(file)\n",
    "    with open(file_path2, \"rb\") as file:\n",
    "        testList = pickle.load(file)\n",
    "    with open(file_path3, \"rb\") as file:\n",
    "        valList = pickle.load(file)\n",
    "else:\n",
    "#     trainFeaturesList.append(data)\n",
    "    #1\n",
    "    trainList.append(trainFeaturesList[0])\n",
    "    testList.append(testFeaturesList[0])\n",
    "    valList.append(valFeaturesList[0])\n",
    "    #2\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 16)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #3\n",
    "    selectedTrainFeatures1b, indicesFeatures1b = get_selected_features(trainFeaturesList[0], y_train, 32)\n",
    "    selectedTestFeatures1b = testFeaturesList[0][:, indicesFeatures1b]\n",
    "    selectedValFeatures1b = valFeaturesList[0][:, indicesFeatures1b]\n",
    "    trainList.append(selectedTrainFeatures1b)\n",
    "    testList.append(selectedTestFeatures1b)\n",
    "    valList.append(selectedValFeatures1b)\n",
    "    #4\n",
    "    selectedTrainFeatures1a, indicesFeatures1a = get_selected_features(trainFeaturesList[0], y_train, 64)\n",
    "    selectedTestFeatures1a = testFeaturesList[0][:, indicesFeatures1a]\n",
    "    selectedValFeatures1a = valFeaturesList[0][:, indicesFeatures1a]\n",
    "    trainList.append(selectedTrainFeatures1a)\n",
    "    testList.append(selectedTestFeatures1a)\n",
    "    valList.append(selectedValFeatures1a)\n",
    "    #5\n",
    "    trainList.append(trainFeaturesList[1])\n",
    "    testList.append(testFeaturesList[1])\n",
    "    valList.append(valFeaturesList[1])\n",
    "    #6\n",
    "    selectedTrainFeatures2, indicesFeatures2 = get_selected_features(trainFeaturesList[1], y_train, 12)\n",
    "    selectedTestFeatures2 = testFeaturesList[1][:, indicesFeatures2]\n",
    "    selectedValFeatures2 = valFeaturesList[1][:, indicesFeatures2]\n",
    "    trainList.append(selectedTrainFeatures2)\n",
    "    testList.append(selectedTestFeatures2)\n",
    "    valList.append(selectedValFeatures2)\n",
    "    #7\n",
    "    trainList.append(trainFeaturesList[2])\n",
    "    testList.append(testFeaturesList[2])\n",
    "    valList.append(valFeaturesList[2])\n",
    "    #8\n",
    "    selectedTrainFeatures3, indicesFeatures3 = get_selected_features(trainFeaturesList[2], y_train, 12)\n",
    "    selectedTestFeatures3 = testFeaturesList[2][:, indicesFeatures3]\n",
    "    selectedValFeatures3 = valFeaturesList[2][:, indicesFeatures3]\n",
    "    trainList.append(selectedTrainFeatures3)\n",
    "    testList.append(selectedTestFeatures3)\n",
    "    valList.append(selectedValFeatures3)\n",
    "    #9\n",
    "    trainList.append(trainFeaturesList[3])\n",
    "    testList.append(testFeaturesList[3])\n",
    "    valList.append(valFeaturesList[3])\n",
    "    #10\n",
    "    selectedTrainFeatures4, indicesFeatures4 = get_selected_features(trainFeaturesList[3], y_train, 12)\n",
    "    selectedTestFeatures4 = testFeaturesList[3][:, indicesFeatures4]\n",
    "    selectedValFeatures4 = valFeaturesList[3][:, indicesFeatures4]\n",
    "    trainList.append(selectedTrainFeatures4)\n",
    "    testList.append(selectedTestFeatures4)\n",
    "    valList.append(selectedValFeatures4)\n",
    "    #11\n",
    "    trainList.append(trainFeaturesList[4])\n",
    "    testList.append(testFeaturesList[4])\n",
    "    valList.append(valFeaturesList[4])\n",
    "    #12\n",
    "    selectedTrainFeatures5, indicesFeatures5 = get_selected_features(trainFeaturesList[4], y_train, 12)\n",
    "    selectedTestFeatures5 = testFeaturesList[4][:, indicesFeatures5]\n",
    "    selectedValFeatures5 = valFeaturesList[4][:, indicesFeatures5]\n",
    "    trainList.append(selectedTrainFeatures5)\n",
    "    testList.append(selectedTestFeatures5)\n",
    "    valList.append(selectedValFeatures5)\n",
    "    #13\n",
    "    trainList.append(trainFeaturesList[5])\n",
    "    testList.append(testFeaturesList[5])\n",
    "    valList.append(valFeaturesList[5])\n",
    "    #14\n",
    "    selectedTrainFeatures6, indicesFeatures6 = get_selected_features(trainFeaturesList[5], y_train, 12)\n",
    "    selectedTestFeatures6 = testFeaturesList[5][:, indicesFeatures6]\n",
    "    selectedValFeatures6 = valFeaturesList[5][:, indicesFeatures6]\n",
    "    trainList.append(selectedTrainFeatures6)\n",
    "    testList.append(selectedTestFeatures6)\n",
    "    valList.append(selectedValFeatures6)\n",
    "    #15\n",
    "    trainList.append(trainFeaturesList[6])\n",
    "    testList.append(testFeaturesList[6])\n",
    "    valList.append(valFeaturesList[6])\n",
    "    #16\n",
    "    selectedTrainFeatures7, indicesFeatures7 = get_selected_features(trainFeaturesList[6], y_train, 12)\n",
    "    selectedTestFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    selectedValFeatures7 = testFeaturesList[6][:, indicesFeatures7]\n",
    "    trainList.append(selectedTrainFeatures7)\n",
    "    testList.append(selectedTestFeatures7)\n",
    "    valList.append(selectedValFeatures7)\n",
    "    selectedNormTrainFeatures1 = get_norm_features(selectedTrainFeatures1b)\n",
    "    selectedNormTestFeatures1 = get_norm_features(selectedTestFeatures1b)\n",
    "    selectedNormValFeatures1 = get_norm_features(selectedValFeatures1b)\n",
    "\n",
    "    #17\n",
    "    trainNormFeatures2 = get_norm_features(trainFeaturesList[1].detach().numpy())\n",
    "    testNormFeatures2 = get_norm_features(testFeaturesList[1].detach().numpy())\n",
    "    valNormFeatures2 = get_norm_features(valFeaturesList[1].detach().numpy())\n",
    "    concatenatedTrainFeatures2 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures2), dim=1)\n",
    "    concatenatedTestFeatures2 = torch.cat((selectedNormTestFeatures1, testNormFeatures2), dim=1)\n",
    "    concatenatedValFeatures2 = torch.cat((selectedNormValFeatures1, valNormFeatures2), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures2)\n",
    "    testList.append(concatenatedTestFeatures2)\n",
    "    valList.append(concatenatedValFeatures2)\n",
    "    #18\n",
    "    trainNormFeatures3 = get_norm_features(trainFeaturesList[2].detach().numpy())\n",
    "    testNormFeatures3 = get_norm_features(testFeaturesList[2].detach().numpy())\n",
    "    valNormFeatures3 = get_norm_features(valFeaturesList[2].detach().numpy())\n",
    "    concatenatedTrainFeatures3 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures3), dim=1)\n",
    "    concatenatedTestFeatures3 = torch.cat((selectedNormTestFeatures1, testNormFeatures3), dim=1)\n",
    "    concatenatedValFeatures3 = torch.cat((selectedNormValFeatures1, valNormFeatures3), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures3)\n",
    "    testList.append(concatenatedTestFeatures3)\n",
    "    valList.append(concatenatedValFeatures3)\n",
    "    #19\n",
    "    trainNormFeatures4 = get_norm_features(trainFeaturesList[3].detach().numpy())\n",
    "    testNormFeatures4 = get_norm_features(testFeaturesList[3].detach().numpy())\n",
    "    valNormFeatures4 = get_norm_features(valFeaturesList[3].detach().numpy())\n",
    "    concatenatedTrainFeatures4 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures4), dim=1)\n",
    "    concatenatedTestFeatures4 = torch.cat((selectedNormTestFeatures1, testNormFeatures4), dim=1)\n",
    "    concatenatedValFeatures4 = torch.cat((selectedNormValFeatures1, valNormFeatures4), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures4)\n",
    "    testList.append(concatenatedTestFeatures4)\n",
    "    valList.append(concatenatedValFeatures4)\n",
    "    #20\n",
    "    trainNormFeatures5 = get_norm_features(trainFeaturesList[4].detach().numpy())\n",
    "    testNormFeatures5 = get_norm_features(testFeaturesList[4].detach().numpy())\n",
    "    valNormFeatures5 = get_norm_features(valFeaturesList[4].detach().numpy())\n",
    "    concatenatedTrainFeatures5 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures5), dim=1)\n",
    "    concatenatedTestFeatures5 = torch.cat((selectedNormTestFeatures1, testNormFeatures5), dim=1)\n",
    "    concatenatedValFeatures5 = torch.cat((selectedNormValFeatures1, valNormFeatures5), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures5)\n",
    "    testList.append(concatenatedTestFeatures5)\n",
    "    valList.append(concatenatedValFeatures5)\n",
    "\n",
    "    #21\n",
    "    trainNormFeatures6 = get_norm_features(trainFeaturesList[5].detach().numpy())\n",
    "    testNormFeatures6 = get_norm_features(testFeaturesList[5].detach().numpy())\n",
    "    valNormFeatures6 = get_norm_features(valFeaturesList[5].detach().numpy())\n",
    "    concatenatedTrainFeatures6 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures6), dim=1)\n",
    "    concatenatedTestFeatures6 = torch.cat((selectedNormTestFeatures1, testNormFeatures6), dim=1)\n",
    "    concatenatedValFeatures6 = torch.cat((selectedNormValFeatures1, valNormFeatures6), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures6)\n",
    "    testList.append(concatenatedTestFeatures6)\n",
    "    valList.append(concatenatedValFeatures6)\n",
    "\n",
    "    #22\n",
    "    trainNormFeatures7 = get_norm_features(trainFeaturesList[6].detach().numpy())\n",
    "    testNormFeatures7 = get_norm_features(testFeaturesList[6].detach().numpy())\n",
    "    valNormFeatures7 = get_norm_features(valFeaturesList[6].detach().numpy())\n",
    "    concatenatedTrainFeatures7 = torch.cat((selectedNormTrainFeatures1, trainNormFeatures7), dim=1)\n",
    "    concatenatedTestFeatures7 = torch.cat((selectedNormTestFeatures1, testNormFeatures7), dim=1)\n",
    "    concatenatedValFeatures7 = torch.cat((selectedNormValFeatures1, valNormFeatures7), dim=1)\n",
    "    trainList.append(concatenatedTrainFeatures7)\n",
    "    testList.append(concatenatedTestFeatures7)\n",
    "    valList.append(concatenatedValFeatures7)\n",
    "\n",
    "    trainList_tensors = [torch.tensor(item) for item in trainList]\n",
    "    testList_tensors = [torch.tensor(item) for item in testList]\n",
    "    valList_tensors = [torch.tensor(item) for item in valList]\n",
    "\n",
    "    with open(file_path1, 'wb') as file:\n",
    "        pickle.dump(trainList_tensors, file)\n",
    "    with open(file_path2, 'wb') as file:\n",
    "        pickle.dump(testList_tensors, file)\n",
    "    with open(file_path3, 'wb') as file:\n",
    "        pickle.dump(valList_tensors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59343465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265dc4c4",
   "metadata": {},
   "source": [
    "1. Prep data - normalize and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8647e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# for item in trainFeaturesList:\n",
    "#     print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb893e7",
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "def to_tensor(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return torch.tensor(data)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")\n",
    "        \n",
    "def prep_data(features, labels, isOversample):\n",
    "    num_classes = 7\n",
    "\n",
    "    if isOversample:\n",
    "        X_set, Y_set = oversample_data(features, labels, num_classes)\n",
    "    else:\n",
    "        X_set, Y_set = features, labels\n",
    "\n",
    "    if isinstance(X_set, torch.Tensor):\n",
    "        X_tensor = X_set.float()\n",
    "    else:\n",
    "        X_tensor = torch.tensor(X_set, dtype=torch.float32)\n",
    "    \n",
    "    if isinstance(Y_set, torch.Tensor):\n",
    "        Y_tensor = Y_set.long()\n",
    "    else:\n",
    "        Y_tensor = torch.tensor(Y_set, dtype=torch.long)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(Y_set, return_counts=True)\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ebf30",
   "metadata": {},
   "source": [
    "2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4bff2",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71c886",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def model_train1(X_set, Y_set, num_epochs=20, batch_size=32, loss_difference_threshold=0.01, \n",
    "#                  hidden_dims=[256, 128], dropout_rate=0.5, lr=0.0001, \n",
    "#                  optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, matchAtt\n",
    "#                  ):\n",
    "#     output_dim = 7  # Number of classes\n",
    "#     model = MyNetwork(len(X_set[0]), hidden_dims, output_dim, dropout_rate)\n",
    "#     criterion = criterion_class()\n",
    "#     optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "#     loss_history = []\n",
    "#     accuracy_history = []\n",
    "# #     print_interval = 1  # Print tqdm every epoch\n",
    "#     previous_loss = float('inf')\n",
    "\n",
    "#     # Create dataset and dataloader\n",
    "#     dataset = TensorDataset(X_set, Y_set)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     epoch_num = num_epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         correct_predictions = 0\n",
    "#         total_instances = 0\n",
    "#         with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "#             for inputs, labels in dataloader:\n",
    "#                 inputs = inputs.float()  # Ensure inputs are float32\n",
    "#                 labels = labels.long()   # Ensure labels are long\n",
    "#                 optimizer.zero_grad()\n",
    "# #                 TODO\n",
    "#                 outputs = model(inputs)\n",
    "#                 outputs = outputs.squeeze()\n",
    "#                 labels = labels.squeeze()\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # Check for NaN loss values\n",
    "#                 if torch.isnan(loss):\n",
    "#                     print(\"NaN loss encountered. Skipping this batch.\")\n",
    "#                     break\n",
    "                \n",
    "#                 # Apply gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 total_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, dim=1)\n",
    "#                 correct_predictions += (predicted == labels).sum().item()\n",
    "#                 total_instances += labels.size(0)\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#         epoch_loss = total_loss / total_instances\n",
    "#         epoch_accuracy = correct_predictions / total_instances\n",
    "#         loss_history.append(epoch_loss)\n",
    "#         accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "#         if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "#             epoch_num = epoch\n",
    "#             break\n",
    "\n",
    "#         previous_loss = epoch_loss\n",
    "\n",
    "#     return model, epoch_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0827f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X_set, Y_set):\n",
    "    indices = np.arange(len(X_set))\n",
    "    np.random.shuffle(indices)\n",
    "    return X_set[indices], Y_set[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237e4fd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train1(X_set=None, Y_set=None, num_epochs=50, loss_difference_threshold=0.0001, \n",
    "                 hidden_dims=[64, 32], dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = MyNetwork(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=7, dropout_rate=0.5)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "                \n",
    "                if nodalAtt:\n",
    "                    loss.backward(retain_graph=True)\n",
    "                else:\n",
    "                    loss.backward(retain_graph=False)\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "        if epoch > 0 and abs(epoch_loss - previous_loss) < loss_difference_threshold:\n",
    "            epoch_num = epoch\n",
    "            break\n",
    "\n",
    "        previous_loss = epoch_loss\n",
    "\n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae7cd0",
   "metadata": {},
   "source": [
    "Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03e2cd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_train2(X_set=None, Y_set=None, num_epochs=20, loss_difference_threshold=0.001, \n",
    "                 hidden_dims=128, dropout_rate=0.5, lr=0.0001, \n",
    "                 optimizer_class=optim.Adam, criterion_class=nn.CrossEntropyLoss, \n",
    "                 nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    \n",
    "    output_dim = 7  # Number of classes\n",
    "    input_dim = X_set.shape[1] if len(X_set) > 0 else 0\n",
    "    model = FCClassifier(input_dim, hidden_dims, output_dim, dropout_rate)\n",
    "    criterion = criterion_class()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    previous_loss = float('inf')\n",
    "\n",
    "    epoch_num = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_instances = 0\n",
    "        \n",
    "        # Shuffle the dataset at the beginning of each epoch\n",
    "        X_set, Y_set = shuffle_data(X_set, Y_set)\n",
    "        \n",
    "        with tqdm(total=len(ranges), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) as pbar:\n",
    "            for i, (start, end) in enumerate(ranges):\n",
    "                inputs = X_set[start:end+1].float()\n",
    "                labels = Y_set[start:end+1].long()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                last_idx = seq_len[i][0]\n",
    "                \n",
    "                umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "#                 print(\"inputs: \", inputs)\n",
    "#                 print(\"seq_len: \", seq_len[i])\n",
    "#                 print(\"umask_slice: \", umask_slice)\n",
    "                outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "                \n",
    "                log_prob = F.log_softmax(outputs, 1)\n",
    "                \n",
    "                loss = criterion(log_prob, labels)\n",
    "                \n",
    "                # Check for NaN loss values\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN loss encountered. Skipping this batch.\")\n",
    "                    break\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "#                 if nodalAtt:\n",
    "                loss.backward(retain_graph=True)\n",
    "#                 else:\n",
    "#                     loss.backward(retain_graph=False)\n",
    "                    \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(log_prob, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_instances += labels.size(0)\n",
    "                pbar.update(1)\n",
    "                \n",
    "        epoch_loss = total_loss / total_instances\n",
    "        epoch_accuracy = correct_predictions / total_instances\n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(epoch_accuracy)\n",
    "        \n",
    "        if epoch > 0:\n",
    "            loss_diff = abs(epoch_loss - previous_loss)\n",
    "            if loss_diff < loss_difference_threshold:\n",
    "                print(f\"Training stopped early at epoch {epoch+1}.\")\n",
    "                print(f\"Loss difference ({loss_diff}) is below the threshold ({loss_difference_threshold}).\")\n",
    "                epoch_num = epoch + 1\n",
    "                break\n",
    "        \n",
    "        previous_loss = epoch_loss\n",
    "    \n",
    "    return model, epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435c98b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_emotions(model=None, X_set=None, Y_set=None, typeSet=None, \n",
    "                      isSimpleFC=False, i_dict=None,\n",
    "                      nodalAtt=None, umask=None, seq_len=None, no_cuda=True, ranges=None):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize empty lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    i = 0\n",
    "    # Iterate over the given ranges\n",
    "    for start, end in ranges:\n",
    "        X_batch = X_set[start:end+1].float()\n",
    "        Y_batch = Y_set[start:end+1].long()\n",
    "\n",
    "#         if X_batch.dtype != torch.float32:\n",
    "#             X_batch = X_batch.float()\n",
    "\n",
    "        # Use no_grad to save memory and computations\n",
    "        with torch.no_grad():\n",
    "            inputs = X_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            labels = Y_batch.to('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "            last_idx = seq_len[i][0]\n",
    "            \n",
    "            umask_slice = umask[i][0][:last_idx].unsqueeze(0)\n",
    "\n",
    "            outputs = model(inputs, nodalAtt, seq_len[i], umask_slice)\n",
    "\n",
    "            outputs = outputs.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "            log_prob = F.log_softmax(outputs, 1)\n",
    "\n",
    "            _, predicted = torch.max(log_prob, 1)\n",
    "\n",
    "            # Append the predictions and labels to the lists\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            i = i+1\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_decoder.values(), output_dict=True, zero_division=0)\n",
    "\n",
    "    # Calculate the required metrics\n",
    "    accuracy = report['accuracy']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    weighted_f1 = report['weighted avg']['f1-score']\n",
    "    f1_micro = report.get('micro avg', {}).get('f1-score', accuracy)\n",
    "    f1_macro = report.get('macro avg', {}).get('f1-score', 0.0) \n",
    "\n",
    "    if typeSet == \"validation\":\n",
    "        print(\"Classified: \", dictKey[i_dict])\n",
    "    \n",
    "    return dictKey[i_dict], typeSet, isSimpleFC, accuracy, recall, weighted_f1, f1_micro, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35df7e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for trainSet, testSet, valSet, _, _, _ in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "#     print(i, type(trainSet))\n",
    "#     if isinstance(trainSet, list):\n",
    "#         print(type(trainSet[0]))\n",
    "#         sample = trainSet[0]\n",
    "#         print(sample.shape)\n",
    "#     else:\n",
    "#         print(trainSet.squeeze(0).shape)\n",
    "#     i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9231db5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getGraphComponents(file_path):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    if checkFile:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths = pickle.load(file)  \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return umask, \\\n",
    "            seq_lengths,\\\n",
    "            features, \\\n",
    "            edge_index, \\\n",
    "            edge_norm, \\\n",
    "            edge_type, \\\n",
    "            edge_index_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83d29f",
   "metadata": {
    "code_folding": [
     4,
     7,
     10
    ]
   },
   "outputs": [],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl'\n",
    "\n",
    "train_umask, train_seq_lengths, train_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path1)\n",
    "\n",
    "test_umask, test_seq_lengths, test_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path2)\n",
    "\n",
    "val_umask, val_seq_lengths, val_features, \\\n",
    "    _, _, _, _ = getGraphComponents(file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1984b",
   "metadata": {},
   "source": [
    "UpdateJune 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741daeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_umask), len(train_seq_lengths))\n",
    "print(train_seq_lengths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d2e11",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getSpeakersAndRanges(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        encodedSpeakers, ranges = pickle.load(file)\n",
    "    file.close()\n",
    "    return encodedSpeakers, ranges\n",
    "\n",
    "file_path1 = \"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\"\n",
    "\n",
    "encodedSpeakersTrain, rangesTrain = getSpeakersAndRanges(file_path1)\n",
    "encodedSpeakersTest, rangesTest = getSpeakersAndRanges(file_path2)\n",
    "encodedSpeakersDev, rangesDev = getSpeakersAndRanges(file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60304bfd",
   "metadata": {},
   "source": [
    "Sample run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f1571",
   "metadata": {},
   "source": [
    "Verifying the attention in batch before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9385da0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class MatchingAttention(nn.Module):\n",
    "#     def __init__(self, mem_dim, cand_dim, alpha_dim, att_type='general2'):\n",
    "#         super(MatchingAttention, self).__init__()\n",
    "#         self.mem_dim = mem_dim\n",
    "#         self.cand_dim = cand_dim\n",
    "#         self.alpha_dim = alpha_dim\n",
    "#         self.att_type = att_type\n",
    "\n",
    "#         if self.att_type == 'general2':\n",
    "#             self.transform = nn.Linear(self.mem_dim, self.cand_dim * self.alpha_dim)\n",
    "\n",
    "#     def forward(self, M, x, mask):\n",
    "#         M_ = M.permute(1, 2, 0)  # (batch, mem_dim, seq_len)\n",
    "#         x_ = self.transform(x).unsqueeze(1)  # (batch, 1, cand_dim * alpha_dim)\n",
    "#         mask_ = mask.unsqueeze(2).repeat(1, 1, self.mem_dim).transpose(1, 2)  # (batch, mem_dim, seq_len)\n",
    "        \n",
    "#         M_ = M_ * mask_\n",
    "#         alpha = torch.bmm(x_, M_)  # (batch, 1, seq_len)\n",
    "        \n",
    "#         alpha = F.softmax(alpha, dim=-1)  # Apply softmax to get attention weights\n",
    "#         attended = torch.bmm(alpha, M.permute(1, 0, 2))  # (batch, 1, mem_dim)\n",
    "        \n",
    "#         return attended.squeeze(1), alpha\n",
    "    \n",
    "# def attentive_node_features(emotions, seq_lengths, umask, matchatt_layer):\n",
    "#     max_len = max(seq_lengths)\n",
    "#     batch_size = len(seq_lengths)\n",
    "#     mem_dim = emotions.size(1)\n",
    "\n",
    "#     padded_emotions = []\n",
    "#     for i in range(batch_size):\n",
    "#         length = seq_lengths[i]\n",
    "#         # Assuming emotions is already a 2D tensor of shape (seq_len, mem_dim)\n",
    "#         padded_emotion = F.pad(emotions[:length], (0, 0, 0, max_len - length), \"constant\", 0)\n",
    "#         padded_emotions.append(padded_emotion)\n",
    "\n",
    "#     emotions_padded = torch.stack(padded_emotions, dim=1)  # (max_len, batch_size, mem_dim)\n",
    "    \n",
    "#     att_emotions = []\n",
    "#     alpha_list = []\n",
    "#     for t in range(max_len):\n",
    "#         att_em, alpha = matchatt_layer(emotions_padded, emotions_padded[t], umask)\n",
    "#         att_emotions.append(att_em)\n",
    "#         alpha_list.append(alpha)\n",
    "\n",
    "#     att_emotions = torch.stack(att_emotions, dim=0)  # (max_len, batch_size, mem_dim)\n",
    "\n",
    "#     # Remove the singleton dimension for batch size 1\n",
    "#     att_emotions = att_emotions.squeeze(1)  # (seq_len, mem_dim)\n",
    "\n",
    "#     return att_emotions, alpha_list\n",
    "    \n",
    "# class MyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "#         super(MyNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dims[0])  # Adjust input_dim to match flattened shape\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=64, cand_dim=64, alpha_dim=1, att_type='general2')\n",
    "\n",
    "#     def forward(self, x, nodalAtt, seq_lengths, umask):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "        \n",
    "#         x = self.fc1(att_emotions)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# class FCClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "#         super(FCClassifier, self).__init__()\n",
    "#         self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.matchatt = MatchingAttention(mem_dim=input_dim, cand_dim=input_dim, alpha_dim=1, att_type='general2')\n",
    "#     def forward(self, x=None, nodalAtt=None, seq_lengths=None, umask=None, no_cuda=True):\n",
    "#         if nodalAtt:\n",
    "#             att_emotions, _ = attentive_node_features(x, seq_lengths, umask, self.matchatt)\n",
    "\n",
    "#             # Reshape att_emotions to have a 2D shape (batch_size, input_dim)\n",
    "#             att_emotions = att_emotions.view(att_emotions.size(0), -1)\n",
    "            \n",
    "#         x = F.relu(self.linear1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.linear2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef46abb",
   "metadata": {},
   "source": [
    "part a test case (uncomment the top part to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dc9f6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# seq_lengths = [14]  # Sequence length matches the input tensor\n",
    "# umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "# inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "# # Initialize MatchingAttention\n",
    "# matchatt_layer = MatchingAttention(mem_dim=inputs.shape[1], cand_dim=inputs.shape[1], alpha_dim=1, att_type='general2')\n",
    "\n",
    "# # Call attentive_node_features\n",
    "# att_emotions, alpha_list = attentive_node_features(inputs, seq_lengths, umask, matchatt_layer)\n",
    "\n",
    "# # Print shapes for verification\n",
    "# print(\"att_emotions shape:\", att_emotions.shape)  # Should be (seq_len, mem_dim)\n",
    "# print(\"alpha_list shape:\", len(alpha_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e91b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [14]\n",
    "umask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
    "inputs = torch.randn(14, 64)  # Example inputs, shape [seq_len, input_dim]\n",
    "\n",
    "model = MyNetwork(input_dim=inputs.shape[1], hidden_dims=[128, 30], output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  #.Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea58147",
   "metadata": {},
   "source": [
    "part b test case (uncomment the part and the function to see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCClassifier(input_dim=inputs.shape[1], hidden_dim=64, output_dim=7, dropout_rate=0.5)\n",
    "outputs = model(inputs, True, seq_lengths, umask)\n",
    "print(outputs.shape)  # Expected torch.Size([14, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e24dbc",
   "metadata": {},
   "source": [
    "end of sample experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b7940",
   "metadata": {},
   "source": [
    "Actual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db90480",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a93e4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "file_path = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        df_results_sorted = pickle.load(file)\n",
    "else:\n",
    "    results = []\n",
    "    num_epochs = 30\n",
    "    i = 0\n",
    "    for trainSet, testSet, valSet in dataLoader:\n",
    "        if isinstance(trainSet, list):\n",
    "            trainSet = trainSet[0].squeeze(0)\n",
    "            testSet = testSet[0].squeeze(0)\n",
    "            valSet = valSet[0].squeeze(0)\n",
    "        else:\n",
    "            trainSet = trainSet.squeeze(0)\n",
    "            testSet = testSet.squeeze(0)\n",
    "            valSet = valSet.squeeze(0)\n",
    "\n",
    "        X_tensor = to_tensor(trainSet)\n",
    "        Y_tensor = to_tensor(y_train)\n",
    "\n",
    "        print(\"Curerntly at\", dictKey[i])\n",
    "#         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#                                    isSimpleFC=False, i_dict=i, \n",
    "#                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#                                    ranges=rangesTrain )\n",
    "#         results.append(result1)\n",
    "        if i in selected_combination:\n",
    "            model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "                                     umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "                                   seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "            X_tensor = to_tensor(testSet)\n",
    "            Y_tensor = to_tensor(y_test)\n",
    "            result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "                                       isSimpleFC=True, i_dict=i, \n",
    "                                       nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "                                       ranges=rangesTest)\n",
    "    #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "    #                                    isSimpleFC=True, i_dict=i, \n",
    "    #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "    #                                    ranges=rangesTrain)\n",
    "            results.append(result2)\n",
    "        else:\n",
    "            print(\"This is skipped\")\n",
    "        i = i+1\n",
    "\n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "    df_results = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4ed51",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e36b6",
   "metadata": {},
   "source": [
    "<h4> Select top 10 unique data combinations then tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36973bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 9\n",
    "counter = 0\n",
    "combination1 = []\n",
    "combination2 = []\n",
    "seen_combinations = set()\n",
    "\n",
    "for idx, row in df_results_sorted.iterrows():\n",
    "    if counter >= max_iterations:\n",
    "        break\n",
    "\n",
    "    if row['data_combination'] in seen_combinations:\n",
    "        continue\n",
    "\n",
    "    if row['isSimpleFC']:\n",
    "        combination2.append(row['data_combination'])\n",
    "    else:\n",
    "        combination1.append(row['data_combination'])\n",
    "\n",
    "    seen_combinations.add(row['data_combination'])\n",
    "    counter += 1\n",
    "\n",
    "# Display the results\n",
    "print(\"Combination 1 (isSimpleFC=False):\", combination1)\n",
    "print(\"Combination 2 (isSimpleFC=True):\", combination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54513411",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1 = [key for key, value in dictKey.items() if value in combination1]\n",
    "indices2 = [key for key, value in dictKey.items() if value in combination2]\n",
    "\n",
    "print(\"Indices for isSimpleFC=False:\", indices1)\n",
    "print(\"Indices for isSimpleFC=True:\", indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedTrainDeepList = [trainList[i] for i in indices1]\n",
    "selectedTestDeepList = [testList[i] for i in indices1]\n",
    "selectedValDeepList = [valList[i] for i in indices1]\n",
    "\n",
    "len(selectedTrainDeepList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccae3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainSet in selectedTrainList:\n",
    "#     print(type(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedTrainList = [trainList[i] for i in indices2]\n",
    "selectedTestList = [testList[i] for i in indices2]\n",
    "selectedValList = [valList[i] for i in indices2]\n",
    "\n",
    "len(selectedTrainList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c266050",
   "metadata": {},
   "source": [
    "<h4> Tuning using random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should call both model_train1 and 2\n",
    "\n",
    "def objective_func(X_train, X_test, X_val, y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,\n",
    "                  train_umask, test_umask, val_umask,\n",
    "                  train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                  rangesTrain, rangesTest, rangesDev, nodalAtt):\n",
    "    results = []\n",
    "    hyperparams_string = (\n",
    "        f'num_epochs={hyperparams[\"num_epochs\"]} '\n",
    "        f'loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]} '\n",
    "        f'hidden_dims={hyperparams[\"hidden_dims\"]} '\n",
    "        f'dropout_rate={hyperparams[\"dropout_rate\"]} '\n",
    "        f'learning_rate={hyperparams[\"learning_rate\"]} '\n",
    "        f'optimizers={hyperparams[\"optimizers\"]} '\n",
    "        f'criteria={hyperparams[\"criteria\"]}'\n",
    "    )    \n",
    "    print(hyperparams_string)\n",
    "            \n",
    "    X_train_tensor = to_tensor(X_train)\n",
    "    y_train_tensor = to_tensor(y_train).long()\n",
    "    X_val_tensor = to_tensor(X_val)\n",
    "    y_val_tensor = to_tensor(y_val).long()\n",
    "    X_test_tensor = to_tensor(X_test)\n",
    "    y_test_tensor = to_tensor(y_test).long()\n",
    "# train\n",
    "    start_time = time.time()\n",
    "    if isSimpleFC:\n",
    "        model, num_epoch = model_train2(X_set=X_train_tensor, Y_set=y_train_tensor, \n",
    "                            num_epochs=hyperparams[\"num_epochs\"], loss_difference_threshold=hyperparams[\"loss_difference_threshold\"], \n",
    "                            hidden_dims=hyperparams[\"hidden_dims\"], dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "                            lr=hyperparams[\"learning_rate\"], optimizer_class=hyperparams[\"optimizers\"], \n",
    "                            criterion_class=hyperparams[\"criteria\"],\n",
    "                            umask=train_umask, seq_len=train_seq_lengths, ranges=rangesTrain, nodalAtt=nodalAtt)        \n",
    "#     else:\n",
    "#         model, num_epoch = model_train1(X_train_tensor, y_train_tensor, hyperparams[\"num_epochs\"],\n",
    "#                             hyperparams[\"batch_size\"], hyperparams[\"loss_difference_threshold\"], \n",
    "#                             hyperparams[\"hidden_dims\"], hyperparams[\"dropout_rate\"],\n",
    "#                             hyperparams[\"learning_rate\"], hyperparams[\"optimizers\"], hyperparams[\"criteria\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "# val\n",
    "    result = classify_emotions(model=model, X_set=X_val_tensor, Y_set=y_val_tensor, typeSet='validation', \n",
    "                               isSimpleFC=isSimpleFC, i_dict=i_dict,\n",
    "                               nodalAtt=nodalAtt, umask=val_umask, seq_len=val_seq_lengths, ranges=rangesDev)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    result = list(result)\n",
    "    hyperparams_string = f'num_epochs={hyperparams[\"num_epochs\"]}-loss_difference_threshold={hyperparams[\"loss_difference_threshold\"]}-hidden_dims={hyperparams[\"hidden_dims\"]}-dropout_rate={hyperparams[\"dropout_rate\"]}-learning_rate={hyperparams[\"learning_rate\"]}-optimizers={hyperparams[\"optimizers\"]}-criteria={hyperparams[\"criteria\"]}'\n",
    "    result.append(elapsed_time)\n",
    "    result.append(hyperparams_string)\n",
    "    result.append(num_epoch)\n",
    "    results.append(result)\n",
    "    \n",
    "# test\n",
    "#     result = classify_emotions(model, X_test_tensor, y_test_tensor, \\\n",
    "#                                'test', isSimpleFC, i_dict)\n",
    "    \n",
    "#     result = list(result)\n",
    "#     result.append(elapsed_time)\n",
    "#     result.append(hyperparams_string)\n",
    "#     result.append(num_epoch)\n",
    "#     results.append(result)\n",
    "    \n",
    "    columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch']\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    df_results_sorted = df.sort_values(by='data_combination', ascending=False)\n",
    "    \n",
    "    return df_results_sorted\n",
    "\n",
    "\n",
    "# def objective_func(X_train, X_test, X_val, \n",
    "#                y_train, y_test, y_val, hyperparams, i_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X_train=None, X_test=None, X_val=None, \n",
    "                  y_train=None, y_test=None, y_val=None, \n",
    "                  param_grid=None, isSimpleFC=True, i_dict=None,\n",
    "                  train_umask=None, test_umask=None, val_umask=None,\n",
    "                  train_seq_lengths=None, test_seq_lengths=None, val_seq_lengths=None,\n",
    "                  rangesTrain=None, rangesTest=None, rangesDev=None, nodalAtt=False\n",
    "                 ):\n",
    "    sub_total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    MAX_EVALS = 5\n",
    "    for i in range(MAX_EVALS):\n",
    "        hyperparams = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "\n",
    "        try:\n",
    "            new_results = objective_func(X_train, X_test, X_val,  y_train, y_test, y_val, hyperparams, i_dict, isSimpleFC,  \n",
    "                                        train_umask, test_umask, val_umask,\n",
    "                                        train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                        rangesTrain, rangesTest, rangesDev, nodalAtt)\n",
    "            sub_total_results = pd.concat([sub_total_results, new_results], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparams {hyperparams}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    return sub_total_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295744e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [[256, 128], [128, 64], [64, 32]],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}\n",
    "param_grid2 = {\n",
    "    'num_epochs': [20, 30, 40],\n",
    "    'loss_difference_threshold': [0.001, 0.0001],\n",
    "    'hidden_dims': [128, 256, 512],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "    'optimizers': [optim.Adam, optim.SGD],\n",
    "    'criteria': [nn.CrossEntropyLoss, nn.NLLLoss]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcf2b3",
   "metadata": {},
   "source": [
    "<h5> First find the best hyperparameter combination for the DeepClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamTuning(X_trainSet, X_testSet, X_valSet, y_train, y_test, y_val, isSimpleFC, param_grid, indices,\n",
    "                    train_umask, test_umask, val_umask,\n",
    "                    train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                    rangesTrain, rangesTest, rangesDev, nodalAttn):\n",
    "    \n",
    "    total_results = pd.DataFrame(columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', \\\n",
    "               'Weighted-F1', 'F1-micro', 'F1-macro', 'train_time', 'hyperparams', 'num_epoch'],)\n",
    "    \n",
    "    for i in indices:\n",
    "        print(\"============\", dictKey[i], \"============\")\n",
    "        X_train = X_trainSet[i]\n",
    "        X_test = X_testSet[i]\n",
    "        X_val = X_valSet[i]\n",
    "\n",
    "        sub_total_results = random_search(X_train, X_test, X_val, y_train, y_test, y_val,\n",
    "                     param_grid, isSimpleFC, i,\n",
    "                     train_umask, test_umask, val_umask,\n",
    "                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                     rangesTrain, rangesTest, rangesDev, nodalAttn[i])\n",
    "\n",
    "        total_results = pd.concat([sub_total_results, total_results], ignore_index=True)\n",
    "\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be82a8",
   "metadata": {},
   "source": [
    "Uncomment below next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fd4d3",
   "metadata": {
    "code_folding": [
     3,
     6
    ]
   },
   "outputs": [],
   "source": [
    "# file_path = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/results/deep_classifier_tuned_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         total_results1_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     total_results1 = hyperparamTuning(selectedTrainDeepList, selectedTestDeepList, selectedValDeepList, \\\n",
    "#                                  y_train, y_test, y_val, False, param_grid1, indices1)\n",
    "    \n",
    "#     total_results1_sorted = total_results1.sort_values(by='Weighted-F1', ascending=False)\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(total_results1_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "# pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "# pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "\n",
    "# total_results1_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea75ad",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dataset = FeatureEngineeredDataset(trainList, testList, valList)\n",
    "# dataLoader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# file_path = \"data/dump/\" + dataset_path + \"/CNNBiLSTM_data_for_classifier/results/classifier_test_no_tuning_Df.pkl\"\n",
    "# checkFile = os.path.isfile(file_path)\n",
    "\n",
    "# if checkFile: \n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         df_results_sorted = pickle.load(file)\n",
    "# else:\n",
    "#     results = []\n",
    "#     num_epochs = 30\n",
    "#     i = 0\n",
    "#     for trainSet, testSet, valSet in dataLoader:\n",
    "#         if isinstance(trainSet, list):\n",
    "#             trainSet = trainSet[0].squeeze(0)\n",
    "#             testSet = testSet[0].squeeze(0)\n",
    "#             valSet = valSet[0].squeeze(0)\n",
    "#         else:\n",
    "#             trainSet = trainSet.squeeze(0)\n",
    "#             testSet = testSet.squeeze(0)\n",
    "#             valSet = valSet.squeeze(0)\n",
    "\n",
    "#         X_tensor = to_tensor(trainSet)\n",
    "#         Y_tensor = to_tensor(y_train)\n",
    "\n",
    "#         print(\"Curerntly at\", dictKey[i])\n",
    "# #         model1, _,  = model_train1(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "# #                                  umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "# #                                seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "# #         result1 = classify_emotions(model=model1, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "# #                                    isSimpleFC=False, i_dict=i, \n",
    "# #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "# #                                    ranges=rangesTrain )\n",
    "# #         results.append(result1)\n",
    "#         if i in selected_combination:\n",
    "#             model2, _,  = model_train2(X_set=X_tensor, Y_set=Y_tensor, num_epochs=num_epochs, \n",
    "#                                      umask=train_umask, nodalAtt=nodalAttn[i],\n",
    "#                                    seq_len=train_seq_lengths, ranges=rangesTrain)\n",
    "\n",
    "#             X_tensor = to_tensor(testSet)\n",
    "#             Y_tensor = to_tensor(y_test)\n",
    "#             result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='test', \n",
    "#                                        isSimpleFC=True, i_dict=i, \n",
    "#                                        nodalAtt=nodalAttn[i],  umask=test_umask, seq_len=test_seq_lengths,\n",
    "#                                        ranges=rangesTest)\n",
    "#     #         result2 = classify_emotions(model=model2, X_set=X_tensor, Y_set=Y_tensor,  typeSet='train', \n",
    "#     #                                    isSimpleFC=True, i_dict=i, \n",
    "#     #                                    nodalAtt=nodalAttn[i],  umask=train_umask, seq_len=train_seq_lengths,\n",
    "#     #                                    ranges=rangesTrain)\n",
    "#             results.append(result2)\n",
    "#         else:\n",
    "#             print(\"This is skipped\")\n",
    "#         i = i+1\n",
    "\n",
    "#     columns = ['data_combination', 'typeSet', 'isSimpleFC', 'Accuracy', 'Recall', 'Weighted-F1', 'F1-micro', 'F1-macro']\n",
    "#     df_results = pd.DataFrame(results, columns=columns)\n",
    "#     df_results_sorted = df_results.sort_values(by='Weighted-F1', ascending=False)\n",
    "\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(df_results_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d089c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/dump/\" + dataset_path + \"/CNNlstm_data_for_classifier/results/simple_classifier_tuned_Df.pkl\"\n",
    "checkFile = os.path.isfile(file_path)\n",
    "\n",
    "if checkFile: \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        total_results2_sorted = pickle.load(file)\n",
    "else: \n",
    "\n",
    "    total_results2 = hyperparamTuning(trainList, testList, valList, \n",
    "                                     y_train, y_test, y_val, True, param_grid2, indices2,\n",
    "                                     train_umask, test_umask, val_umask,\n",
    "                                     train_seq_lengths, test_seq_lengths, val_seq_lengths,\n",
    "                                     rangesTrain, rangesTest, rangesDev, nodalAttn)\n",
    "    \n",
    "    total_results2_sorted = total_results2.sort_values(by='Weighted-F1', ascending=False)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(total_results2_sorted, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Don't limit the width of the display\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "    \n",
    "total_results2_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
