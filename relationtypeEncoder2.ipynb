{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c64aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import torch.nn.init as init\n",
    "import dgl\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f920f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aa9fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        torch.manual_seed(42)\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "        \n",
    "        # Edge type embeddings\n",
    "        torch.manual_seed(42)\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "        \n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of neighbor node representations\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).mean(dim=2)\n",
    "\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores\n",
    "\n",
    "class EGATConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_node_feats,\n",
    "                 in_edge_feats,\n",
    "                 out_node_feats,\n",
    "                 out_edge_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 **kw_args):\n",
    "\n",
    "        super().__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._out_node_feats = out_node_feats\n",
    "        self._out_edge_feats = out_edge_feats\n",
    "        \n",
    "        self.fc_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=bias)\n",
    "        self.fc_ni = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_nj = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        \n",
    "        # Attention parameter\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.manual_seed(42)\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.xavier_normal_(self.fc_node.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "        init.xavier_normal_(self.attn, gain=gain)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "        with graph.local_scope():\n",
    "            graph.edata['f'] = efeats\n",
    "            graph.ndata['h'] = nfeats\n",
    "            \n",
    "            f_ni = self.fc_ni(nfeats)\n",
    "            f_nj = self.fc_nj(nfeats)\n",
    "            f_fij = self.fc_fij(efeats)\n",
    "            graph.srcdata.update({'f_ni' : f_ni})\n",
    "            graph.dstdata.update({'f_nj' : f_nj})\n",
    "            \n",
    "            graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "            f_out = graph.edata.pop('f_tmp') + f_fij\n",
    "            \n",
    "            if self.bias is not None:\n",
    "                f_out += self.bias\n",
    "            f_out = nn.functional.leaky_relu(f_out)\n",
    "            f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "            \n",
    "            e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.edata['a'] = edge_softmax(graph, e)\n",
    "            graph.ndata['h_out'] = self.fc_node(nfeats).view(-1, self._num_heads, self._out_node_feats)\n",
    "            \n",
    "            graph.update_all(fn.u_mul_e('h_out', 'a', 'm'), fn.sum('m', 'h_out'))\n",
    "\n",
    "            h_out = graph.ndata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "            if get_attention:\n",
    "                return h_out, f_out, graph.edata.pop('a')\n",
    "            else:\n",
    "                return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7adf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ohe(edge_types):\n",
    "    one_hot_encoding = []\n",
    "    for edge_type in edge_types:\n",
    "        if edge_type == 0:\n",
    "            one_hot_encoding.append([1., 0., 0.])\n",
    "        elif edge_type == 1:\n",
    "            one_hot_encoding.append([0., 1., 0.])\n",
    "        elif edge_type == 2:\n",
    "            one_hot_encoding.append([0., 0., 1.])\n",
    "    return torch.tensor(one_hot_encoding)\n",
    "\n",
    "def get_inferred_edgetypes_GAT(dialog, edge_types):\n",
    "    inferred_edge_types = []\n",
    "    inferred_edge_indices = []\n",
    "    for target_node in dialog.values():\n",
    "        if len(target_node) == 1:\n",
    "            inferred_edge_types.append(0)\n",
    "            inferred_edge_indices.append(0)\n",
    "        else:\n",
    "            edge_index = target_node[0][0]\n",
    "            highest_attention = target_node[0][1]\n",
    "            for src_node in target_node[1:]:\n",
    "                if highest_attention < src_node[1]:\n",
    "                    highest_attention = src_node[1]\n",
    "                    edge_index = src_node[0]\n",
    "            inferred_edge_indices.append(edge_index)\n",
    "            inferred_edge_types.append(edge_types[edge_index].tolist())\n",
    "    return inferred_edge_indices, inferred_edge_types\n",
    "\n",
    "def get_inferred_edgetypes_EGAT(edges_target_nodes, sample_edge_types, size_dialog, dialog_id):\n",
    "    inferred_edge_types = []\n",
    "    for target_idx in range(size_dialog):\n",
    "        num_edges = len(edges_target_nodes[target_idx])\n",
    "        if num_edges == 1:\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "        else:\n",
    "            highest_attn_score = max(edges_target_nodes[target_idx][0][1])\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            for sample_edge in range(1, num_edges):\n",
    "                cur_highest_attn_score = max(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                if cur_highest_attn_score > highest_attn_score:\n",
    "                    highest_attn_score = cur_highest_attn_score\n",
    "                    edgetype_idx = np.argmax(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                    edge_idx = edges_target_nodes[target_idx][sample_edge][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "    return inferred_edge_types\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7639e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 0\n",
    "        while (val <= 3) and (i+val <= end_idx):\n",
    "            target_idx = i+val\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "            val = val+1\n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "    return adjacency_list_dict\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    for i in range(dialogs_len):\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "        for j in range(dialog_len):\n",
    "            src_node = dialog_nodes_pairs[j]\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k]\n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))\n",
    "    return whole_edge_type_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91faed3",
   "metadata": {},
   "source": [
    "<h3> Data Preparetion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006630b",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8a8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder_train.pkl\")\n",
    "encodedSpeakersTrain = []\n",
    "rangesTrain = []\n",
    "\n",
    "if not checkFile:\n",
    "    print(\"Run first the contextEncoder2 to generate this file\")\n",
    "else:\n",
    "    with open('data/dump/speaker_encoder_train.pkl', \"rb\") as file:\n",
    "        encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/adjListTrain.pkl\")\n",
    "adjacencyListTrain = []\n",
    "\n",
    "if key:\n",
    "    adjacencyListTrain = get_all_adjacency_list(rangesTrain)\n",
    "else:\n",
    "    with open('data/dump/adjListTrain', \"rb\") as file:\n",
    "        adjacencyListTrain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "771b8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_BERT_train.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    contextualEmbeddingsTrain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6055971",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain)\n",
    "edgeTypesTrain = get_all_edge_type_list(edgeIndicesTrain, encodedSpeakersTrain)\n",
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c67ff",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df15e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/speaker_encoder_test.pkl\")\n",
    "encodedSpeakersTest = []\n",
    "rangesTest = []\n",
    "\n",
    "if not checkFile:\n",
    "    print(\"Run first the contextEncoder2 to generate this file\")\n",
    "else:\n",
    "    with open('data/dump/speaker_encoder_test.pkl', \"rb\") as file:\n",
    "        encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "\n",
    "checkFile = os.path.isfile(\"data/dump/adjListTest.pkl\")\n",
    "adjacencyListTest = []\n",
    "\n",
    "if key:\n",
    "    adjacencyListTest = get_all_adjacency_list(rangesTest)\n",
    "else:\n",
    "    with open('data/dump/adjListTest', \"rb\") as file:\n",
    "        adjacencyListTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ef966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'embed/u_prime_BERT_test.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    contextualEmbeddingsTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17973a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTest = get_all_adjacency_list(rangesTest)\n",
    "edgeTypesTest = get_all_edge_type_list(edgeIndicesTest, encodedSpeakersTest)\n",
    "edgeIndicesTest = get_all_adjacency_list(rangesTest, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14cedb",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2ffa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO repeat the one above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a44f59",
   "metadata": {},
   "source": [
    "<h3> Get GAT output from each set of data (train, test, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36c94f",
   "metadata": {},
   "source": [
    "<h4> Instantiating the GAT (1st implementation) for 1 sample train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cacafa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in_features = len(contextualEmbeddingsTrain[0][0])\n",
    "num_out_features = len(contextualEmbeddingsTrain[0][0])\n",
    "num_heads = 4\n",
    "num_edge_types = 3\n",
    "gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a8775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_prime shape:  torch.Size([14, 50]) attention_coef shape:  torch.Size([14, 768, 50])\n"
     ]
    }
   ],
   "source": [
    "i = 0  # dialogue id\n",
    "relationalEmbedding, attentionCoef = gat_layer((contextualEmbeddingsTrain[i], edgeIndicesTrain[i]), edgeTypesTrain[i])\n",
    "print(\"h_prime shape: \", relationalEmbedding.shape, \"attention_coef shape: \", attentionCoef.shape)\n",
    "\n",
    "targetNodes = edgeIndicesTrain[i][1].tolist()\n",
    "\n",
    "sample = {}\n",
    "sampleEdgetypes = []\n",
    "\n",
    "for target_i in sorted(set(targetNodes)):\n",
    "    sample[target_i] = []\n",
    "\n",
    "for targetNode, idx in zip(targetNodes, range(len(targetNodes))):\n",
    "    sample[targetNode].append([idx, relationalEmbedding[targetNode][idx].tolist()])\n",
    "\n",
    "listEdgeIdxTrain, inferredEdgeTypes = get_inferred_edgetypes_GAT(sample, edgeTypesTrain[i])\n",
    "sampleEdgetypes.append(inferredEdgeTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "258b7da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/dump/label_decoder.pkl', 'rb')\n",
    "label_decoder = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "label_decoder = list(label_decoder.values())\n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4ed00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/labels_train.pkl\")\n",
    "\n",
    "if checkFile is False:\n",
    "    print(\"Please run the contextEncoder2 notebook to save the label file\")\n",
    "else:\n",
    "    file = open('data/dump/labels_train.pkl', 'rb')\n",
    "    y_train = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5394940",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFile = os.path.isfile(\"data/dump/labels_test.pkl\")\n",
    "\n",
    "if checkFile is False:\n",
    "    print(\"Please run the contextEncoder2 notebook to save the label file\")\n",
    "else:\n",
    "    file = open('data/dump/labels_test.pkl', 'rb')\n",
    "    y_test = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223125",
   "metadata": {},
   "source": [
    "<h4> Visualize 1 instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1a41a39",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming h_prime contains the node embeddings\n",
    "# utt_size = 13\n",
    "# labels = torch.tensor(y_train[:utt_size + 1])\n",
    "\n",
    "# cherrypicked_nodes = []\n",
    "# for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#     cherrypicked_nodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "# cherrypicked_nodes = torch.tensor(cherrypicked_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47a97231",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# h_prime_np = cherrypicked_nodes.detach().numpy()\n",
    "\n",
    "# # Perform dimensionality reduction using t-SNE\n",
    "# tsne = TSNE(n_components=3, perplexity=5, random_state=42)\n",
    "# h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# # Plot the node embeddings with different colors for each label\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#     indices = (labels == label).nonzero().squeeze()\n",
    "#     plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "# plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "# plt.xlabel('Dimension 1', color=\"white\")\n",
    "# plt.ylabel('Dimension 2', color=\"white\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8043a6a",
   "metadata": {},
   "source": [
    "<h4>Now get new representations of all train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "588776d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "\n",
    "def get_GAT_representation(filePath, contextualEmbeddings, edgeIndices, edgeTypes):\n",
    "#     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-GAT_train.pkl\") #replace it with key when deployed\n",
    "    if key:\n",
    "        print(\"Start of getting output of 1st GAT\")\n",
    "        allInferredEdgetypes = []\n",
    "        listAllEdgeIdx = []\n",
    "        cherrypickedNodes = []\n",
    "        for dialog, dialog_id in zip(contextualEmbeddings, range(len(contextualEmbeddings))):\n",
    "            h_prime, attention_coef = gat_layer((dialog, edgeIndices[dialog_id]), edgeTypes[dialog_id])\n",
    "            target_nodes = edgeIndices[dialog_id][1].tolist() # first idx represents dialogue id\n",
    "\n",
    "            sample_edgetypes = {}\n",
    "            for i in set(target_nodes):\n",
    "                sample_edgetypes[i] = []\n",
    "\n",
    "            for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "                sample_edgetypes[target_node].append([edge_idx, h_prime[target_node][edge_idx].tolist()])\n",
    "\n",
    "            list_edge_idx, inferred_edgetypes = get_inferred_edgetypes_GAT(sample_edgetypes,  edgeTypes[dialog_id])\n",
    "            listAllEdgeIdx.append(list_edge_idx)\n",
    "            allInferredEdgetypes.append(inferred_edgetypes)\n",
    "\n",
    "            for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "                cherrypickedNodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "\n",
    "        cherrypickedNodes = torch.tensor(cherrypickedNodes)\n",
    "        cherrypickedNodes.shape\n",
    "        print(\"End of getting output of 1st GAT\")\n",
    "\n",
    "        pickle.dump([cherrypickedNodes, allInferredEdgetypes],\n",
    "                    open(filePath, 'wb'))\n",
    "\n",
    "    else:\n",
    "        file = open(filePath, 'rb')\n",
    "        cherrypickedNodes, allInferredEdgetypes = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return cherrypickedNodes, allInferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65d7c89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of getting output of 1st GAT\n",
      "End of getting output of 1st GAT\n",
      "Start of getting output of 1st GAT\n",
      "End of getting output of 1st GAT\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "cherrypickedNodesTrain, allInferredEdgetypesTrain = get_GAT_representation(\n",
    "                                                    \"embed/h_prime_BERT-GAT_train.pkl\",\n",
    "                                                    contextualEmbeddingsTrain,\n",
    "                                                    edgeIndicesTrain,\n",
    "                                                    edgeTypesTrain)\n",
    "# only save the pickle data for test and validation\n",
    "_, _ = get_GAT_representation(\"embed/h_prime_BERT-GAT_test.pkl\",\n",
    "                        contextualEmbeddingsTest,\n",
    "                        edgeIndicesTest,\n",
    "                        edgeTypesTest)\n",
    "# TODO add valid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5fbe0a",
   "metadata": {},
   "source": [
    "<h5> Visualize Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "548c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(y_train)\n",
    "h_prime_np = cherrypickedNodesTrain.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74050dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d86332b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caea68",
   "metadata": {},
   "source": [
    "<h5> Analyze the edgetypes of all train nodes in the context of a dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c236b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `all_inferred_edgetypes` and `y_train` are defined\n",
    "df_eda = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(allInferredEdgetypesTrain),\n",
    "     'label': y_train,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46b7f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab Result:\n",
      "label        0    1    2     3     4    5    6\n",
      "edgetype                                      \n",
      "0          407  117   97   941  2218  277  591\n",
      "2         1093  247  241  1371  3742  599  899\n",
      "\n",
      "The P-Value of the Chi-Squared Test is: 6.372109133776597e-20\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "# Assuming `df_eda` and `CrosstabResult` are defined\n",
    "CrosstabResult = pd.crosstab(index=df_eda['edgetype'], columns=df_eda['label'])\n",
    "\n",
    "print(\"Crosstab Result:\")\n",
    "print(CrosstabResult)\n",
    "print()\n",
    "\n",
    "# Performing Chi-squared test\n",
    "ChiSqResult = chi2_contingency(CrosstabResult)\n",
    "\n",
    "# P-Value is the Probability of H0 being True\n",
    "# If P-Value > 0.05 then only we Accept the assumption(H0)\n",
    "# H0: The variables are not correlated with each other.\n",
    "\n",
    "print('The P-Value of the Chi-Squared Test is:', ChiSqResult[1])\n",
    "\n",
    "if ChiSqResult[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb0a7",
   "metadata": {},
   "source": [
    "<h3> Get EGAT output from each set of data (train, test, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60061c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "egat = EGATConv(in_node_feats=len(contextualEmbeddingsTrain[0][0]),\n",
    "                    in_edge_feats=3,\n",
    "                    out_node_feats=len(contextualEmbeddingsTrain[0][0]),\n",
    "                    out_edge_feats=3,\n",
    "                    num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc9167ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "#            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "def get_EGAT_representations(filePath, contextualEmbeddings, edgeIndices, edgeTypes):\n",
    "#     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-EGAT_train.pkl\")\n",
    "    if key:\n",
    "        print(\"Start of getting output of 2nd GAT\")\n",
    "        inferredEdgetypes = []\n",
    "        allNodeFeats = []\n",
    "\n",
    "        # Iterate over each dialogue\n",
    "        for dialog_id in range(len(contextualEmbeddings)):\n",
    "            # Create a DGL graph\n",
    "            graph = dgl.graph((edgeIndices[dialog_id][0], edgeIndices[dialog_id][1]))\n",
    "\n",
    "            # Get one-hot encoded edge features\n",
    "            edge_feats = get_ohe(edgeTypes[dialog_id])\n",
    "\n",
    "            # Get outputs from the second GAT layer\n",
    "            egat_output = egat(graph, contextualEmbeddings[dialog_id], edge_feats)\n",
    "            new_node_feats, new_edge_feats = egat_output\n",
    "\n",
    "            # Compute mean edge features\n",
    "            mean_edge_feats = new_edge_feats.mean(dim=1)\n",
    "            allNodeFeats.append(new_node_feats.mean(dim=1).tolist())\n",
    "\n",
    "            # Prepare edge features for inference\n",
    "            target_nodes = edgeIndices[dialog_id][1].tolist()\n",
    "            sample_edgetypes = {}\n",
    "            for i in set(target_nodes):\n",
    "                sample_edgetypes[i] = []\n",
    "            for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "                sample_edgetypes[target_node].append([edge_idx, \n",
    "                                                      mean_edge_feats[edge_idx].tolist()])\n",
    "\n",
    "            # Infer edge types\n",
    "            sample_edgetypes = get_inferred_edgetypes_EGAT(sample_edgetypes, edgeTypes[dialog_id], \n",
    "                                                           len(contextualEmbeddings[dialog_id]), \n",
    "                                                           dialog_id)\n",
    "            inferredEdgetypes.append(sample_edgetypes)\n",
    "\n",
    "        # Flatten and convert node features to tensor\n",
    "        allNodeFeats = torch.tensor(flatten_extend(allNodeFeats))\n",
    "\n",
    "        print(\"End of getting output of 2nd GAT\")\n",
    "\n",
    "        # Save the data to a pickle file\n",
    "        pickle.dump([allNodeFeats, inferredEdgetypes], open(filePath, 'wb'))\n",
    "    else:\n",
    "        # Load data from the existing pickle file\n",
    "        file = open(filePath, 'rb')\n",
    "        all_node_feats, inferredEdgetypes = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return allNodeFeats, inferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a685431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of getting output of 2nd GAT\n",
      "End of getting output of 2nd GAT\n",
      "Start of getting output of 2nd GAT\n",
      "End of getting output of 2nd GAT\n"
     ]
    }
   ],
   "source": [
    "allNodeFeatsTrain, inferredEdgetypesTrain = get_EGAT_representations(\n",
    "                                        \"embed/h_prime_BERT-EGAT_train.pkl\",\n",
    "                                        contextualEmbeddingsTrain,\n",
    "                                        edgeIndicesTrain,\n",
    "                                        edgeTypesTrain\n",
    "                                )\n",
    "_, _ = get_EGAT_representations(\n",
    "        \"embed/h_prime_BERT-EGAT_test.pkl\",\n",
    "        contextualEmbeddingsTest,\n",
    "        edgeIndicesTest,\n",
    "        edgeTypesTest\n",
    ")\n",
    "#TODO do for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "938645cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda2 = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(inferredEdgetypesTrain),\n",
    "     'label': y_train,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d83d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab Result:\n",
      " label        0    1    2     3     4    5     6\n",
      "edgetype                                       \n",
      "0            1    0    0     2     5    2     2\n",
      "1         1499  364  338  2310  5955  874  1488\n",
      "The P-Value of the ChiSq Test is: 0.8318329952175869\n",
      "Variables are not correlated with each other\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from your data (df_eda2)\n",
    "# Assuming df_eda2 is already defined\n",
    "\n",
    "# Crosstabulation\n",
    "CrosstabResult2 = pd.crosstab(index=df_eda2['edgetype'], columns=df_eda2['label'])\n",
    "print(\"Crosstab Result:\\n\", CrosstabResult2)\n",
    "\n",
    "# Performing Chi-squared test\n",
    "ChiSqResult2 = chi2_contingency(CrosstabResult2)\n",
    "\n",
    "# Print the p-value of the Chi-squared test\n",
    "print('The P-Value of the ChiSq Test is:', ChiSqResult2[1])\n",
    "\n",
    "# Interpret the p-value\n",
    "if ChiSqResult2[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213c68a",
   "metadata": {},
   "source": [
    "Testing on 1 dialog data before scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84641d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Node Features Shape: torch.Size([14, 4, 768])\n",
      "New Edge Features Shape: torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "dialog_id = 0\n",
    "\n",
    "# Create a DGL graph\n",
    "graph = dgl.graph((edgeIndicesTrain[dialog_id][0], edgeIndicesTrain[dialog_id][1]))\n",
    "\n",
    "# Obtain one-hot encoded edge features\n",
    "edge_feats = get_ohe(edgeTypesTrain[dialog_id])\n",
    "\n",
    "# Pass the graph, node representations, and edge features through the EGAT model\n",
    "newNodeFeats, newEdgeFeats = egat(graph, contextualEmbeddingsTrain[dialog_id], edge_feats)\n",
    "\n",
    "# Print the shapes of the new node and edge features\n",
    "print(\"New Node Features Shape:\", newNodeFeats.shape)\n",
    "print(\"New Edge Features Shape:\", newEdgeFeats.shape)\n",
    "\n",
    "# Calculate the mean of node features along the second dimension (number of nodes)\n",
    "h_prime_mean = newNodeFeats.mean(dim=1)\n",
    "\n",
    "# Assuming you want to select only a subset of labels for visualization\n",
    "utt_size = 13\n",
    "labels = torch.tensor(y_train[:utt_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bbcf4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "h_prime_np = h_prime_mean.detach().numpy()\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# Plot the node embeddings with different colors for each label\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "    indices = (labels == label).nonzero().squeeze()\n",
    "    plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "plt.xlabel('Dimension 1', color=\"white\")\n",
    "plt.ylabel('Dimension 2', color=\"white\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e75975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "851a250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # Convert tensor to numpy array\n",
    "    h_prime_np = allNodeFeatsTrain.detach().numpy()\n",
    "    labels = torch.tensor(y_train)\n",
    "    \n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "        print(h_prime_tsne.shape)\n",
    "        \n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
