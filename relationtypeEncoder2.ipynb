{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c64aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, math, pickle, sys, random, time\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "import dgl,numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from collections import Counter\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import RGCNConv, GraphConv\n",
    "from model import DialogueGCN_MELDModel, GraphNetwork_RGCN, GraphNetwork_GAT, \\\n",
    "GraphNetwork_GAT_EdgeFeat, GraphNetwork_GATv2, GraphNetwork_GATv2_EdgeFeat, GraphNetwork_RGAT\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from graph_context_dataset import GraphContextDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f920f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153dd45",
   "metadata": {},
   "source": [
    "<b>Make sure to specify which dataset to use\n",
    "<br>\n",
    " - dataset_original\n",
    "<br>\n",
    " - dataset_drop_noise\n",
    "<br>\n",
    " - dataset_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a74ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_original\"\n",
    "# dataset_path = \"dataset_drop_noise\"\n",
    "# dataset_path = \"dataset_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa9fe91",
   "metadata": {
    "code_folding": [
     0,
     38,
     60
    ]
   },
   "outputs": [],
   "source": [
    "class GATLayerWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_in_features_per_head, num_out_features_per_head, num_heads, num_edge_types):\n",
    "        super(GATLayerWithEdgeType, self).__init__()\n",
    "        self.num_in_features_per_head = num_in_features_per_head\n",
    "        self.num_out_features_per_head = num_out_features_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_edge_types = num_edge_types\n",
    "\n",
    "        # Linear projection for node features\n",
    "        torch.manual_seed(42)\n",
    "        self.linear_proj = nn.Linear(self.num_in_features_per_head, self.num_heads * self.num_out_features_per_head)\n",
    "        \n",
    "        # Edge type embeddings\n",
    "        torch.manual_seed(42)\n",
    "        self.edge_type_embedding = nn.Embedding(self.num_edge_types, self.num_heads)\n",
    "        \n",
    "    def forward(self, input_data, edge_type):\n",
    "        node_features, edge_indices = input_data\n",
    "\n",
    "        # Linear projection for node features\n",
    "        h_linear = self.linear_proj(node_features.view(-1, self.num_in_features_per_head))\n",
    "        h_linear = h_linear.view(-1, self.num_heads, self.num_out_features_per_head)\n",
    "        h_linear = h_linear.permute(0, 2, 1)\n",
    "\n",
    "        # Edge type embedding\n",
    "        edge_type_embedding = self.edge_type_embedding(edge_type).transpose(0, 1)\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        attention_scores = torch.matmul(h_linear, edge_type_embedding).squeeze(-1)\n",
    "\n",
    "        # Softmax to get attention coefficients\n",
    "        attention_coefficients = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of neighbor node representations\n",
    "        updated_representation = torch.matmul(attention_coefficients.transpose(1, 2), h_linear).mean(dim=2)\n",
    "\n",
    "        return updated_representation, attention_coefficients\n",
    "    \n",
    "class GATWithEdgeType(nn.Module):\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, num_edge_types):\n",
    "        super(GATWithEdgeType, self).__init__()\n",
    "\n",
    "        self.gat_net = nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_of_layers):\n",
    "            num_in_features = num_heads_per_layer[layer - 1] * num_features_per_layer[layer - 1] if layer > 0 else num_features_per_layer[0]\n",
    "            num_out_features = num_heads_per_layer[layer] * num_features_per_layer[layer]\n",
    "            self.gat_net.append(GATLayerWithEdgeType(num_in_features, num_out_features, num_heads_per_layer[layer], num_edge_types))\n",
    "\n",
    "    def forward(self, node_features, edge_indices, edge_types):\n",
    "        h = node_features\n",
    "\n",
    "        attention_scores = []\n",
    "\n",
    "        for layer in self.gat_net:\n",
    "            h, attention_coefficients = layer((h, edge_indices), edge_types)\n",
    "            attention_scores.append(attention_coefficients)\n",
    "\n",
    "        return h, attention_scores\n",
    "\n",
    "class EGATConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_node_feats,\n",
    "                 in_edge_feats,\n",
    "                 out_node_feats,\n",
    "                 out_edge_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 **kw_args):\n",
    "\n",
    "        super().__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._out_node_feats = out_node_feats\n",
    "        self._out_edge_feats = out_edge_feats\n",
    "        \n",
    "        self.fc_node = nn.Linear(in_node_feats, out_node_feats * num_heads, bias=bias)\n",
    "        self.fc_ni = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats * num_heads, bias=False)\n",
    "        self.fc_nj = nn.Linear(in_node_feats, out_edge_feats * num_heads, bias=False)\n",
    "        \n",
    "        # Attention parameter\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.manual_seed(42)\n",
    "        gain = init.calculate_gain('relu')\n",
    "        init.xavier_normal_(self.fc_node.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "        init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "        init.xavier_normal_(self.attn, gain=gain)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "        with graph.local_scope():\n",
    "            graph.edata['f'] = efeats\n",
    "            graph.ndata['h'] = nfeats\n",
    "            \n",
    "            f_ni = self.fc_ni(nfeats)\n",
    "            f_nj = self.fc_nj(nfeats)\n",
    "            f_fij = self.fc_fij(efeats)\n",
    "            graph.srcdata.update({'f_ni' : f_ni})\n",
    "            graph.dstdata.update({'f_nj' : f_nj})\n",
    "            \n",
    "            graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "            f_out = graph.edata.pop('f_tmp') + f_fij\n",
    "            \n",
    "            if self.bias is not None:\n",
    "                f_out += self.bias\n",
    "            f_out = nn.functional.leaky_relu(f_out)\n",
    "            f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "            \n",
    "            e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.edata['a'] = edge_softmax(graph, e)\n",
    "            graph.ndata['h_out'] = self.fc_node(nfeats).view(-1, self._num_heads, self._out_node_feats)\n",
    "            \n",
    "            graph.update_all(fn.u_mul_e('h_out', 'a', 'm'), fn.sum('m', 'h_out'))\n",
    "\n",
    "            h_out = graph.ndata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "            if get_attention:\n",
    "                return h_out, f_out, graph.edata.pop('a')\n",
    "            else:\n",
    "                return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7adf37e",
   "metadata": {
    "code_folding": [
     0,
     11,
     29,
     31
    ]
   },
   "outputs": [],
   "source": [
    "def get_ohe(edge_types):\n",
    "    one_hot_encoding = []\n",
    "    for edge_type in edge_types:\n",
    "        if edge_type == 0:\n",
    "            one_hot_encoding.append([1., 0., 0.])\n",
    "        elif edge_type == 1:\n",
    "            one_hot_encoding.append([0., 1., 0.])\n",
    "        elif edge_type == 2:\n",
    "            one_hot_encoding.append([0., 0., 1.])\n",
    "    return torch.tensor(one_hot_encoding)\n",
    "\n",
    "def get_inferred_edgetypes_GAT(dialog, edge_types):\n",
    "    inferred_edge_types = []\n",
    "    inferred_edge_indices = []\n",
    "    for target_node in dialog.values():\n",
    "        if len(target_node) == 1:\n",
    "            inferred_edge_types.append(0)\n",
    "            inferred_edge_indices.append(0)\n",
    "        else:\n",
    "            edge_index = target_node[0][0]\n",
    "            highest_attention = target_node[0][1]\n",
    "            for src_node in target_node[1:]:\n",
    "                if highest_attention < src_node[1]:\n",
    "                    highest_attention = src_node[1]\n",
    "                    edge_index = src_node[0]\n",
    "            inferred_edge_indices.append(edge_index)\n",
    "            inferred_edge_types.append(edge_types[edge_index].tolist())\n",
    "    return inferred_edge_indices, inferred_edge_types\n",
    "\n",
    "def get_inferred_edgetypes_EGAT(edges_target_nodes, sample_edge_types, size_dialog, dialog_id):\n",
    "    inferred_edge_types = []\n",
    "    for target_idx in range(size_dialog):\n",
    "        num_edges = len(edges_target_nodes[target_idx])\n",
    "        if num_edges == 1:\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "        else:\n",
    "            highest_attn_score = max(edges_target_nodes[target_idx][0][1])\n",
    "            edgetype_idx = np.argmax(edges_target_nodes[target_idx][0][1])\n",
    "            edge_idx = edges_target_nodes[target_idx][0][0]\n",
    "            for sample_edge in range(1, num_edges):\n",
    "                cur_highest_attn_score = max(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                if cur_highest_attn_score > highest_attn_score:\n",
    "                    highest_attn_score = cur_highest_attn_score\n",
    "                    edgetype_idx = np.argmax(edges_target_nodes[target_idx][sample_edge][1])\n",
    "                    edge_idx = edges_target_nodes[target_idx][sample_edge][0]\n",
    "            inferred_edge_types.append(edgetype_idx)\n",
    "    return inferred_edge_types\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7639e77",
   "metadata": {
    "code_folding": [
     0,
     15,
     25,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def create_node_pairs_list(start_idx, end_idx):\n",
    "    list_node_i = []\n",
    "    list_node_j = []\n",
    "    end_idx = end_idx - start_idx\n",
    "    start_idx = 0\n",
    "    for i in range(start_idx, end_idx+1):\n",
    "        val = 0\n",
    "        while (val <= 3) and (i+val <= end_idx):\n",
    "            target_idx = i+val\n",
    "            if target_idx >= 0:\n",
    "                list_node_i.append(i)\n",
    "                list_node_j.append(target_idx)\n",
    "            val = val+1\n",
    "    return [list_node_i, list_node_j]\n",
    "\n",
    "def create_adjacency_dict(node_pairs):\n",
    "    adjacency_list_dict = {}\n",
    "    for i in range(0, len(node_pairs[0])):\n",
    "        source_node, target_node = node_pairs[0][i], node_pairs[1][i]\n",
    "        if source_node not in adjacency_list_dict:\n",
    "            adjacency_list_dict[source_node] = [target_node]\n",
    "        else:\n",
    "            adjacency_list_dict[source_node].append(target_node)\n",
    "    return adjacency_list_dict\n",
    "\n",
    "def get_all_adjacency_list(ranges, key=0):\n",
    "    all_adjacency_list = []\n",
    "    for range_pair in ranges:\n",
    "        start_idx, end_idx = range_pair\n",
    "        if key == 0:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = create_adjacency_dict(output)\n",
    "        elif key == 1:\n",
    "            output = create_node_pairs_list(start_idx, end_idx)\n",
    "            output = torch.tensor(output)\n",
    "        else:\n",
    "            print(\"N/A\")\n",
    "        all_adjacency_list.append(output)\n",
    "    return all_adjacency_list\n",
    "\n",
    "def get_all_edge_type_list(edge_indices, encoded_speaker_list):\n",
    "    dialogs_len = len(edge_indices)\n",
    "    whole_edge_type_list = []\n",
    "    for i in range(dialogs_len):\n",
    "        dialog_nodes_pairs = edge_indices[i]\n",
    "        dialog_speakers = list(encoded_speaker_list[i])\n",
    "        dialog_len = len(dialog_nodes_pairs.keys())\n",
    "        edge_type_list = []\n",
    "        for j in range(dialog_len):\n",
    "            src_node = dialog_nodes_pairs[j]\n",
    "            node_i_idx = j\n",
    "            win_len = len(src_node)\n",
    "            for k in range(win_len):\n",
    "                node_j_idx = src_node[k]\n",
    "                if node_i_idx == node_j_idx:\n",
    "                    edge_type_list.append(0)\n",
    "                else:\n",
    "                    if dialog_speakers[node_i_idx] != dialog_speakers[node_j_idx]:\n",
    "                        edge_type_list.append(1)\n",
    "                    else:\n",
    "                        edge_type_list.append(2)\n",
    "        whole_edge_type_list.append(torch.tensor(edge_type_list).to(torch.int64))\n",
    "    return whole_edge_type_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed53f3f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddb394",
   "metadata": {},
   "source": [
    "<h3> Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c424c",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Train, Test and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8a8f87",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\")\n",
    "# encodedSpeakersTrain = []\n",
    "# rangesTrain = []\n",
    "\n",
    "# if not checkFile:\n",
    "#     print(\"Run first the contextEncoder1 or 2 to generate this file\")\n",
    "# else:\n",
    "#     with open('data/dump/' + dataset_path + '/speaker_encoder_train.pkl', \"rb\") as file:\n",
    "#         encodedSpeakersTrain, rangesTrain = pickle.load(file)\n",
    "\n",
    "# checkFile = os.path.isfile(\"data/dump/\" + dataset_path +\"/adjListTrain.pkl\")\n",
    "# adjacencyListTrain = []\n",
    "\n",
    "# if key:\n",
    "#     adjacencyListTrain = get_all_adjacency_list(rangesTrain)\n",
    "# else:\n",
    "#     with open('data/dump/' + dataset_path + '/adjListTrain', \"rb\") as file:\n",
    "#         adjacencyListTrain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5170d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeakersAndRanges(file_path):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    encodedSpeakers = []\n",
    "#     ranges = []\n",
    "    if not checkFile:\n",
    "        print(\"Run first the contextEncoder1.5 to generate this file\")\n",
    "        return None\n",
    "    else:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            encodedSpeakers, ranges = pickle.load(file)\n",
    "        return encodedSpeakers, ranges\n",
    "    \n",
    "def getAdjacencyList(file_path, ranges):\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    adjacencyList = []\n",
    "\n",
    "    if key:\n",
    "        adjacencyList = get_all_adjacency_list(ranges)\n",
    "    else:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            adjacencyList = pickle.load(file)\n",
    "    \n",
    "    return adjacencyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7be6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = \"data/dump/\" + dataset_path + \"/speaker_encoder_train.pkl\"\n",
    "file_path2 = \"data/dump/\" + dataset_path + \"/speaker_encoder_test.pkl\"\n",
    "file_path3 = \"data/dump/\" + dataset_path + \"/speaker_encoder_dev.pkl\"\n",
    "\n",
    "encodedSpeakersTrain, rangesTrain = getSpeakersAndRanges(file_path1)\n",
    "encodedSpeakersTest, rangesTest = getSpeakersAndRanges(file_path2)\n",
    "encodedSpeakersDev, rangesDev = getSpeakersAndRanges(file_path3)\n",
    "\n",
    "file_path1 = 'data/dump/' + dataset_path + '/adjListTrain'\n",
    "file_path2 = 'data/dump/' + dataset_path + '/adjListTest'\n",
    "file_path3 = 'data/dump/' + dataset_path + '/adjListDev'\n",
    "\n",
    "adjacencyListTrain = getAdjacencyList(file_path1, rangesTrain)\n",
    "adjacencyListTest = getAdjacencyList(file_path1, rangesTest)\n",
    "adjacencyListDev = getAdjacencyList(file_path1, rangesDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "771b8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/u_prime_BERT_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/u_prime_BERT_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/u_prime_BERT_dev.pkl'\n",
    "\n",
    "# Load the list from the file using pickle\n",
    "def getFeatures(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        emotions = pickle.load(file)\n",
    "    return emotions\n",
    "\n",
    "contextualEmbeddingsTrain = getFeatures(file_path1)\n",
    "contextualEmbeddingsTest = getFeatures(file_path2)\n",
    "contextualEmbeddingsDev = getFeatures(file_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6698b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(contextualEmbeddingsTrain.shape, contextualEmbeddingsTest.shape, contextualEmbeddingsDev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6055971",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain)\n",
    "edgeTypesTrain = get_all_edge_type_list(edgeIndicesTrain, encodedSpeakersTrain)\n",
    "edgeIndicesTrain = get_all_adjacency_list(rangesTrain, key=1)\n",
    "\n",
    "edgeIndicesTest = get_all_adjacency_list(rangesTest)\n",
    "edgeTypesTest = get_all_edge_type_list(edgeIndicesTest, encodedSpeakersTest)\n",
    "edgeIndicesTest = get_all_adjacency_list(rangesTest, key=1)\n",
    "\n",
    "edgeIndicesDev = get_all_adjacency_list(rangesDev)\n",
    "edgeTypesDev = get_all_edge_type_list(edgeIndicesDev, encodedSpeakersDev)\n",
    "edgeIndicesDev = get_all_adjacency_list(rangesDev, key=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b48a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e18118d",
   "metadata": {},
   "source": [
    "<h4> Creating \"SAMPLE\" graph features based on various graph networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a469bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rangesTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b197fe2",
   "metadata": {},
   "source": [
    "Start of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc943c7d",
   "metadata": {},
   "source": [
    "<h5>DGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9caf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.features = [torch.rand(14, 200)]\n",
    "        self.edge_index = [torch.randint(0, 14, (2, 69))]\n",
    "        self.edge_type = [torch.randint(0, 4, (69,))]\n",
    "        self.edge_index_lengths = [torch.tensor([69])]\n",
    "        self.umask = [torch.randint(0, 2, (1, 14))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.edge_index[idx], self.edge_type[idx], self.edge_index_lengths[idx], self.umask[idx])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = SampleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4874963d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN Representation Shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "# D_m = 200\n",
    "# D_g = 100\n",
    "# D_p = 100\n",
    "# D_e = 100\n",
    "# D_h = 100\n",
    "# D_a = 100\n",
    "# graph_hidden_size = 64\n",
    "# n_speakers = 2\n",
    "# max_seq_len = 110\n",
    "# window_past = 0\n",
    "# window_future = 5\n",
    "# n_classes = 7\n",
    "# dropout_rec = 0.5\n",
    "# dropout = 0.5\n",
    "nodal_attention = True\n",
    "avec = False\n",
    "no_cuda = False\n",
    "\n",
    "features = torch.randn(14, 200)\n",
    "edge_index = [torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0]])]\n",
    "edge_type = [torch.tensor([1, 0, 2, 2, 2, 0, 1, 3, 2, 2, 0, 2, 2, 2])]\n",
    "seq_lengths  = torch.tensor([[14]])\n",
    "umask = torch.ones(1, 1, 14)\n",
    "\n",
    "nodal_attn = False\n",
    "avec = False\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphNetwork_RGCN(num_features=200, num_classes=7, num_relations=4, max_seq_len=14)\n",
    "gcn_representation = model(features, edge_index, edge_type, seq_lengths, umask, nodal_attn, avec)\n",
    "print(\"GCN Representation Shape:\", gcn_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3846a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d48a0",
   "metadata": {},
   "source": [
    "<h5>GAT w/o edge feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e6cdb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 4  # This parameter is not used with GATConv but kept for compatibility\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_type = [torch.randint(0, num_relations, (20,))]  # Example edge types\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "928e2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862804c",
   "metadata": {},
   "source": [
    "<h5>GAT with edge feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "276cee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 4  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))\n",
    "edge_index = [torch.randint(0, 14, (2, 20))]\n",
    "edge_attr = [torch.randn((20, num_relations))]\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1869288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_attr = torch.randint(0, 2, (20, 1)).float()  # Example binary edge features\n",
    "# edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b632",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/o edgetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab557a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n",
      "Output shape with attention: torch.Size([14, 64])\n",
      "Attention weights shape: torch.Size([34, 1])\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2(num_features, num_classes, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# Forward pass with attention weights\n",
    "out, (edge_index, attention_weights) = model(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "print(\"Output shape with attention:\", out.shape)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5781299",
   "metadata": {},
   "source": [
    "<h5>GATv2 with edge type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b5ae9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 4\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_attr = torch.randn((20, num_relations))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_attr)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d91e3",
   "metadata": {},
   "source": [
    "<h5>RGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d536b5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  9,  5,  2,  8,  3, 13,  9,  1,  4, 11, 13,  7,  2, 13,  6, 13,  1,\n",
       "          0,  5],\n",
       "        [11,  4,  9,  1, 12, 11,  9, 13,  6, 11,  9,  3,  1,  0, 11,  5,  3,  2,\n",
       "          7,  5]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e05a3a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([14, 64])\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 3  # Example number of relations\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "edge_dim = 1  # Dimensionality of edge attributes\n",
    "no_cuda = False\n",
    "\n",
    "\n",
    "model = GraphNetwork_RGAT(num_features, num_classes, num_relations, hidden_size, num_heads, dropout, edge_dim, no_cuda)\n",
    "\n",
    "# Dummy inputs for testing\n",
    "x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# Forward pass\n",
    "out = model(x, edge_index, edge_type=edge_type)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94a009",
   "metadata": {},
   "source": [
    "End of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317848a",
   "metadata": {},
   "source": [
    "<h4> Encode speaker to train, test, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7bc233c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class GraphContextDataset(Dataset):\n",
    "#     def __init__(self, rangeSet, labels, features, edge_index, edge_type, \\\n",
    "#                  edge_index_lengths, umask, seq_lengths):\n",
    "#         self.rangeSet = rangeSet\n",
    "#         self.labels = [torch.tensor(label) for label in labels]\n",
    "#         self.features = [torch.tensor(feature) for feature in features]\n",
    "#         self.edge_index = [torch.tensor(edge) for edge in edge_index]\n",
    "#         self.edge_type = [torch.tensor(edge) for edge in edge_type]\n",
    "#         self.edge_index_lengths = [torch.tensor(length) for length in edge_index_lengths]\n",
    "#         self.umask = umask\n",
    "#         self.seq_lengths = seq_lengths\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.rangeSet)  # Use rangeSet for length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         startIdx, endIdx = self.rangeSet[idx]\n",
    "#         return (\n",
    "#             self.labels[startIdx: endIdx+1],\n",
    "#             self.features[idx],\n",
    "#             self.edge_index[idx],\n",
    "#             self.edge_type[idx],\n",
    "#             self.edge_index_lengths[idx],\n",
    "#             self.umask[idx],\n",
    "#             self.seq_lengths[idx]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbbdf11a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# file_path1 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_train.pkl'\n",
    "# file_path2 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_test.pkl'\n",
    "# file_path3 = 'embed/' + dataset_path + '/pre_h_prime_CNNBiLSTM_dev.pkl'\n",
    "\n",
    "# with open(file_path1, 'rb') as file1:\n",
    "#      all_umask, \\\n",
    "#      all_seq_lengths,\\\n",
    "#      all_features, \\\n",
    "#      all_edge_index, \\\n",
    "#      all_edge_norm, \\\n",
    "#      all_edge_type, \\\n",
    "#      all_edge_index_lengths = pickle.load(file1)\n",
    "\n",
    "# file_path2 = 'data/dump/train_labels.pkl'\n",
    "# with open(file_path2, 'rb') as file2:\n",
    "#     trainLabels = pickle.load(file2)\n",
    "\n",
    "# trainDataset = GraphContextDataset(rangesTrain, trainLabels,\n",
    "#                                    all_features, all_edge_index,\n",
    "#                                    all_edge_type,\n",
    "#                                    all_edge_index_lengths,\n",
    "#                                    all_umask, all_seq_lengths)\n",
    "# dataLoader = DataLoader(trainDataset, batch_size=1, shuffle=False, num_workers=4)  # Use num_workers for parallel data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69ad33d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.features = [torch.tensor(feature) for feature in features]\n",
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_index = [torch.tensor(edge) for edge in edge_index]\n",
      "D:\\final_yr\\23-24 t2\\THSST-2\\ug_thesis\\ER_GAT\\graph_context_dataset.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_type = [torch.tensor(edge) for edge in edge_type]\n"
     ]
    }
   ],
   "source": [
    "def getDataLoaderAndLabels(file_path, ranges):\n",
    "    with open(file_path[0], 'rb') as file:\n",
    "         all_umask, \\\n",
    "         all_seq_lengths,\\\n",
    "         all_features, \\\n",
    "         all_edge_index, \\\n",
    "         all_edge_norm, \\\n",
    "         all_edge_type, \\\n",
    "         all_edge_index_lengths = pickle.load(file)\n",
    "\n",
    "    with open(file_path[1], 'rb') as file:\n",
    "        labels = pickle.load(file)\n",
    "\n",
    "    dataset = GraphContextDataset(ranges, labels,\n",
    "                                       all_features, all_edge_index,\n",
    "                                       all_edge_type,\n",
    "                                       all_edge_index_lengths,\n",
    "                                       all_umask, all_seq_lengths)\n",
    "    dataLoader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "    return dataLoader, labels\n",
    "\n",
    "file_path1 = ['embed/' + dataset_path + '/pre_h_prime_BERT_train.pkl', 'data/dump/' + dataset_path + '/labels_train.pkl']\n",
    "file_path2 = ['embed/' + dataset_path + '/pre_h_prime_BERT_test.pkl' , 'data/dump/' + dataset_path + '/labels_test.pkl']\n",
    "file_path3 = ['embed/' + dataset_path + '/pre_h_prime_BERT_dev.pkl', 'data/dump/' + dataset_path + '/labels_dev.pkl']\n",
    "\n",
    "dataLoaderTrain, trainLabels = getDataLoaderAndLabels(file_path1, rangesTrain)\n",
    "dataLoaderTest, testLabels = getDataLoaderAndLabels(file_path2, rangesTest)\n",
    "dataLoaderDev, devLabels = getDataLoaderAndLabels(file_path3, rangesDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76fc8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RelationEncoding(file_path, dataLoader, model, config):\n",
    "    all_h_prime = []\n",
    "    checkFile = os.path.isfile(file_path)\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    j = 1\n",
    "    for _, features_in, edge_index_in, edge_type_in, _, umask, seq_lengths_in in tqdm(dataLoader, desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "        if config == \"dgcn\":\n",
    "            avec, no_cuda, nodal_attn = False, False, False\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            seq_lengths = torch.tensor(seq_lengths_in).view(1, 1)\n",
    "            graph_representation = model(feature, [edge_index], [edge_type], seq_lengths, umask, nodal_attn, avec)\n",
    "        elif config == \"GATv1_noAttn\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            graph_representation = model(feature, [edge_index])\n",
    "            \n",
    "        elif config == \"GATv1\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            num_edge_types = 8\n",
    "            edge_attr = torch.zeros((edge_type.size(0), num_edge_types))\n",
    "            edge_attr.scatter_(1, edge_type.view(-1, 1), 1)\n",
    "            graph_representation = model(feature, [edge_index], [edge_attr])\n",
    "            \n",
    "        elif config == \"GATv2_noAttn\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            graph_representation = model(feature, edge_index)\n",
    "        \n",
    "        elif config == \"GATv2\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            num_edge_types = 8\n",
    "            edge_attr = torch.zeros((edge_type.size(0), num_edge_types))\n",
    "            edge_attr.scatter_(1, edge_type.view(-1, 1), 1)\n",
    "            graph_representation = model(feature, edge_index, edge_attr)\n",
    "        \n",
    "        elif config == \"RGAT\":\n",
    "            feature = features_in.squeeze(0)\n",
    "            edge_index = edge_index_in.squeeze(0)\n",
    "            edge_type = edge_type_in.squeeze(0)\n",
    "            graph_representation = model(feature, edge_index, edge_type)\n",
    "        \n",
    "        all_h_prime.append(graph_representation.cpu())\n",
    "        \n",
    "        i = i + 1\n",
    "        if i % 500 == 0 and config == \"RGAT\":\n",
    "            pt_file_path = file_path + str(j) + \".pkl\"\n",
    "            with open(pt_file_path, 'wb') as file:  # Corrected the file path\n",
    "                pickle.dump(all_h_prime, file)\n",
    "            all_h_prime = []\n",
    "            j = j + 1\n",
    "            \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took\", elapsed_time, \"seconds to encode train\", config)\n",
    "    \n",
    "    if config == \"RGAT\":\n",
    "        pt_file_path = file_path + str(j) + \".pkl\"\n",
    "        with open(pt_file_path, 'wb') as file:\n",
    "            pickle.dump(all_h_prime, file)\n",
    "    else:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(all_h_prime, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3073ff8",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# features = torch.randn(14, 200)\n",
    "# edge_index = [torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "#                             [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0]])]\n",
    "# edge_type = [torch.tensor([1, 0, 2, 2, 2, 0, 1, 3, 2, 2, 0, 2, 2, 2])]\n",
    "# seq_lengths  = torch.tensor([[14]])\n",
    "# umask = torch.ones(1, 1, 14)\n",
    "# print(\"features: \", features.shape)\n",
    "# print(\"edge_index.shape: \", edge_index[0].shape)\n",
    "# print(\"edge_index: \", edge_index)\n",
    "# print(\"edge_type.shape: \", edge_type[0].shape)\n",
    "# print(\"edge_type: \", edge_type)\n",
    "# print(\"seq_lengths.shape: \", seq_lengths.shape)\n",
    "# print(\"seq_lengths: \", seq_lengths)\n",
    "# print(\"umask.shape: \", umask.shape)\n",
    "# print(\"umask: \", umask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b33d64f3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# edge_type_mapping = {}\n",
    "\n",
    "# for j in range(2):\n",
    "#     for k in range(2):\n",
    "#         edge_type_mapping[str(j) + str(k) + '0'] = len(edge_type_mapping)\n",
    "#         edge_type_mapping[str(j) + str(k) + '1'] = len(edge_type_mapping)\n",
    "# edge_type_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648061a8",
   "metadata": {},
   "source": [
    "<h5>DGCN_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75c77723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:22<00:00, 95.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 22.617549180984497 seconds to encode train dgcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:10<00:00, 52.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.936208486557007 seconds to encode train dgcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:08<00:00, 32.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8.370967388153076 seconds to encode train dgcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path1 = 'embed/' + dataset_path + '/h_prime_BERT_DGCN_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_BERT_DGCN_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_BERT_DGCN_dev.pkl'\n",
    "\n",
    "model = GraphNetwork_RGCN(num_features=768, num_classes=7, num_relations=8, max_seq_len=30)\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"dgcn\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"dgcn\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"dgcn\")\n",
    "\n",
    "# gcn_representation = model(features, edge_index, edge_type, seq_lengths, umask, nodal_attn, avec)\n",
    "# print(\"GCN Representation Shape:\", gcn_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46527c",
   "metadata": {},
   "source": [
    "<h5>GAT w/o edge_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0bcdb",
   "metadata": {},
   "source": [
    "reviwing the types and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31e9a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_edge_type[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18aa4149",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_attr = [torch.randn((20, num_relations))]  # Example edge features\n",
    "# print(\"x.shape: \", x.shape)\n",
    "# print(\"edge_index.shape: \", edge_index[0].shape)\n",
    "# print(\"edge_attr.shape: \", edge_attr[0].shape)\n",
    "# print(\"x: \", x)\n",
    "# print(\"edge_index: \", edge_index)\n",
    "# print(\"edge_attr: \", edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "758ba04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:24<00:00, 88.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 24.480356454849243 seconds to encode train GATv1_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:10<00:00, 55.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.379675388336182 seconds to encode train GATv1_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:07<00:00, 34.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 7.862674713134766 seconds to encode train GATv1_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 4  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_BERT_GATv1_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_BERT_GATv1_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_BERT_GATv1_dev.pkl'\n",
    "\n",
    "model = GraphNetwork_GAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv1_noAttn\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv1_noAttn\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv1_noAttn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb39da0",
   "metadata": {},
   "source": [
    "<h5>GAT w/ edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01092d30",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = [torch.randint(0, 14, (2, 20))]  # Example edge index with 20 edges\n",
    "# edge_attr = [torch.randn((20, num_relations))]  # Example edge features\n",
    "# print(edge_index[0].shape)\n",
    "# print(edge_attr[0].shape)\n",
    "# print(edge_index)\n",
    "# print(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "195e2641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:23<00:00, 91.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 23.52733278274536 seconds to encode train GATv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:10<00:00, 54.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.677526235580444 seconds to encode train GATv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:09<00:00, 29.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9.291007995605469 seconds to encode train GATv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 8  # Assuming edge features have 4 dimensions\n",
    "max_seq_len = 14\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GAT_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_edgeAttr_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_edgeAttr_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv1_edgeAttr_dev.pkl'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv1\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv1\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06c4f7",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/o edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff3525a0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "# print(x.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(edge_attr.shape)\n",
    "# print(x)\n",
    "# print(edge_index)\n",
    "# print(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a40b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|███████████████████████████████████████████████████████| 2160/2160 [00:21<00:00, 101.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 21.359537839889526 seconds to encode train GATv2_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:10<00:00, 54.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.568776369094849 seconds to encode train GATv2_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:08<00:00, 33.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8.163758993148804 seconds to encode train GATv2_noAttn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "model = GraphNetwork_GATv2(num_features, num_classes, hidden_size, num_heads, dropout, no_cuda)\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_CNNBiLSTM_GATv2_dev.pkl'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv2_noAttn\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv2_noAttn\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv2_noAttn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb8298",
   "metadata": {},
   "source": [
    "<h5>GATv2 w/ edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea9a6dcd",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_attr = torch.randn((20, num_relations))  # Example edge features\n",
    "# print(x.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "245ee655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [00:25<00:00, 83.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 25.757128953933716 seconds to encode train GATv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:11<00:00, 51.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 11.162024974822998 seconds to encode train GATv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:08<00:00, 32.03batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8.433340311050415 seconds to encode train GATv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 8\n",
    "max_seq_len = 30\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_GATv2_EdgeFeat(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_BERT_GATv2_edgeAttr_train.pkl'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_BERT_GATv2_edgeAttr_test.pkl'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_BERT_GATv2_edgeAttr_dev.pkl'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"GATv2\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"GATv2\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"GATv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609069ea",
   "metadata": {},
   "source": [
    "<h5> RGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db77a2c4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "# # edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "# print(edge_type.shape)\n",
    "# print(edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b92fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|████████████████████████████████████████████████████████| 2160/2160 [02:09<00:00, 16.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 129.02660059928894 seconds to encode train RGAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 577/577 [00:37<00:00, 15.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 37.207568407058716 seconds to encode train RGAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|██████████████████████████████████████████████████████████| 270/270 [00:16<00:00, 16.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 16.019785165786743 seconds to encode train RGAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 768\n",
    "num_classes = 7\n",
    "num_relations = 8\n",
    "hidden_size = 64\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "edge_dim = 1  # Dimensionality of edge attributes\n",
    "no_cuda = False\n",
    "\n",
    "model = GraphNetwork_RGAT(num_features, num_classes, num_relations, hidden_size, num_heads, dropout, edge_dim, no_cuda)\n",
    "file_path1 = 'embed/' + dataset_path + '/h_prime_BERT_RGAT_train'\n",
    "file_path2 = 'embed/' + dataset_path + '/h_prime_BERT_RGAT_test'\n",
    "file_path3 = 'embed/' + dataset_path + '/h_prime_BERT_RGAT_dev'\n",
    "\n",
    "RelationEncoding(file_path1, dataLoaderTrain, model, \"RGAT\")\n",
    "RelationEncoding(file_path2, dataLoaderTest, model, \"RGAT\")\n",
    "RelationEncoding(file_path3, dataLoaderDev, model, \"RGAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36a4c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinePartitionedData(file_paths, output_file_path):\n",
    "    combined_data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    data = pickle.load(file)\n",
    "                    combined_data.extend(data)\n",
    "            except (pickle.UnpicklingError, EOFError) as e:\n",
    "                print(f\"Error loading data from {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred while processing {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {file_path} does not exist and will be skipped.\")\n",
    "\n",
    "    # Save the combined data to a new pickle file\n",
    "    try:\n",
    "        with open(output_file_path, 'wb') as file:\n",
    "            pickle.dump(combined_data, file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving combined data to {output_file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Delete the original files\n",
    "    for file_path in file_paths:\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except OSError as e:\n",
    "                print(f\"Error deleting file {file_path}: {e}\")\n",
    "        \n",
    "file_paths1 = ['embed/' + dataset_path + '/h_prime_BERT_RGAT_train1.pkl', 'embed/' + dataset_path + '/h_prime_BERT_RGAT_train2.pkl', 'embed/' + dataset_path + '/h_prime_BERT_RGAT_train3.pkl', 'embed/' + dataset_path + '/h_prime_BERT_RGAT_train4.pkl', 'embed/' + dataset_path + '/h_prime_BERT_RGAT_train5.pkl']\n",
    "file_paths2 = ['embed/' + dataset_path + '/h_prime_BERT_RGAT_test1.pkl', 'embed/' + dataset_path + '/h_prime_BERT_RGAT_test2.pkl']\n",
    "file_paths3 = ['embed/' + dataset_path + '/h_prime_BERT_RGAT_dev1.pkl']\n",
    "output_file_path1 = 'embed/' + dataset_path + '/h_prime_BERT_RGAT_train.pkl'\n",
    "output_file_path2 = 'embed/' + dataset_path + '/h_prime_BERT_RGAT_test.pkl'\n",
    "output_file_path3 = 'embed/' + dataset_path + '/h_prime_BERT_RGAT_dev.pkl'\n",
    "\n",
    "combinePartitionedData(file_paths1, output_file_path1)\n",
    "combinePartitionedData(file_paths2, output_file_path2)\n",
    "combinePartitionedData(file_paths3, output_file_path3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016df95",
   "metadata": {},
   "source": [
    "end of encoding train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8da874fb",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # change D_m into\n",
    "# D_m = 100\n",
    "# D_g = 150\n",
    "# D_p = 150\n",
    "# D_e = 100\n",
    "# D_h = 100\n",
    "# D_a = 100\n",
    "# graph_h=100\n",
    "# seed_everything()\n",
    "# model = DialogueGCN_MELDModel(\n",
    "#                                D_m, D_g, D_p, D_e, D_h, D_a, graph_h,\n",
    "#                                n_speakers=2,\n",
    "#                                max_seq_len=110,\n",
    "#                                window_past=0,\n",
    "#                                window_future=5,\n",
    "#                                n_classes=7,\n",
    "#                                listener_state=False,\n",
    "#                                context_attention='general',\n",
    "#                                dropout=0.5,\n",
    "#                                nodal_attention=False,\n",
    "#                                no_cuda=False\n",
    "#                                )\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e759176",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# num_features = 200\n",
    "# num_classes = 7\n",
    "# num_relations = 3  # Example number of relations\n",
    "# max_seq_len = 30\n",
    "# hidden_size = 64\n",
    "# num_heads = 8\n",
    "# dropout = 0.5\n",
    "# no_cuda = False\n",
    "\n",
    "# model = GraphNetwork4WithRGAT(num_features, num_classes, num_relations, max_seq_len, hidden_size, num_heads, dropout, no_cuda)\n",
    "\n",
    "# # Dummy inputs for testing\n",
    "# x = torch.randn((14, num_features))  # Example feature matrix with 14 nodes\n",
    "# edge_index = torch.randint(0, 14, (2, 20))  # Example edge index with 20 edges\n",
    "# edge_type = torch.randint(0, num_relations, (20,))  # Example edge types\n",
    "# edge_attr = torch.randn((20, 1))  # Example edge features\n",
    "\n",
    "# # Forward pass\n",
    "# out = model(x, edge_index, edge_type=edge_type, edge_attr=edge_attr)\n",
    "# print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18936917",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def train_or_eval_graph_model(model, loss_function, dataloader, epoch, cuda, optimizer=None, train=False):\n",
    "#     losses, preds, labels = [], [], []\n",
    "#     scores, vids = [], []\n",
    "    \n",
    "#     ei, et, en, el = torch.empty(0).type(torch.LongTensor), torch.empty(0).type(torch.LongTensor), torch.empty(0), []\n",
    "    \n",
    "#     assert not train or optimizer != None\n",
    "#     if train:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "        \n",
    "#     seed_everything()\n",
    "#     for data in dataloader:\n",
    "#         if train:\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#         labels, features, edge_index, edge_type, edge_index_lengths, umask, _ = data\n",
    "#         print(\"features: \", features[0].shape)\n",
    "#         print(\"edge_index: \", edge_index[0].shape)\n",
    "# #         print(edge_norm[0].shape)\n",
    "#         print(\"edge_type: \", edge_type[0].shape)\n",
    "#         print(\"edge_index_lengths: \", edge_index_lengths[0])\n",
    "#         print(\"umask: \", umask[0].shape)\n",
    "#         print(\"-------------------------------\")\n",
    "#         log_prob, e_i, e_n, e_t, e_l = model(features, edge_index,\n",
    "# #                                              edge_norm,\n",
    "#                                              edge_type, edge_index_lengths, umask)\n",
    "#         label = torch.cat([label[j][:lengths[j]] for j in range(len(label))])\n",
    "#         loss = loss_function(log_prob, label)\n",
    "\n",
    "#         ei = torch.cat([ei, e_i], dim=1)\n",
    "#         et = torch.cat([et, e_t])\n",
    "#         en = torch.cat([en, e_n])\n",
    "#         el += e_l\n",
    "\n",
    "#         preds.append(torch.argmax(log_prob, 1).cpu().numpy())\n",
    "#         labels.append(label.cpu().numpy())\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         if train:\n",
    "#             loss.backward()\n",
    "#             if args.tensorboard:\n",
    "#                 for param in model.named_parameters():\n",
    "#                     writer.add_histogram(param[0], param[1].grad, epoch)\n",
    "#             optimizer.step()\n",
    "            \n",
    "#     if preds != []:\n",
    "#         preds = np.concatenate(preds)\n",
    "#         labels = np.concatenate(labels)\n",
    "#     else:\n",
    "#         return float('nan'), float('nan'), [], [], float('nan'), [], [], [], [], []\n",
    "    \n",
    "#     ei = ei.data.cpu().numpy()\n",
    "#     et = et.data.cpu().numpy()\n",
    "#     en = en.data.cpu().numpy()\n",
    "#     el = np.array(el)\n",
    "#     labels = np.array(labels)\n",
    "#     preds = np.array(preds)\n",
    "\n",
    "#     avg_loss = round(np.sum(losses) / len(losses), 4)\n",
    "#     avg_accuracy = round(accuracy_score(labels, preds) * 100, 2)\n",
    "#     avg_fscore = round(f1_score(labels, preds, average='macro') * 100, 2)\n",
    "#     # Add precision and recall\n",
    "#     precision = round(precision_score(labels, preds, average='macro') * 100, 2)\n",
    "#     recall = round(recall_score(labels, preds, average='macro') * 100, 2)\n",
    "#     return avg_loss, avg_accuracy, labels, preds, avg_fscore, ei, et, en, el, precision, recall\n",
    "    \n",
    "    \n",
    "# for e in range(60):\n",
    "#     train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_graph_model(model, loss_function,\n",
    "#                                                                           dataloader, e,\n",
    "#                                                                           optimizer, True)\n",
    "#     print(\n",
    "#     'epoch: {}, train_loss: {}, train_acc: {}, train_fscore: {}, train_precision: {}, train_recall: {}, '. \\\n",
    "#     format(e + 1, train_loss, train_acc, train_fscore, train_precision, train_recall))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871b0b4",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data (NO NEED FOR THIS SECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de3baa3e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/speaker_encoder_test.pkl\")\n",
    "# encodedSpeakersTest = []\n",
    "# rangesTest = []\n",
    "\n",
    "# if not checkFile:\n",
    "#     print(\"Run first the contextEncoder2 to generate this file\")\n",
    "# else:\n",
    "#     with open('data/dump/speaker_encoder_test.pkl', \"rb\") as file:\n",
    "#         encodedSpeakersTest, rangesTest = pickle.load(file)\n",
    "\n",
    "# checkFile = os.path.isfile(\"data/dump/adjListTest.pkl\")\n",
    "# adjacencyListTest = []\n",
    "\n",
    "# if key:\n",
    "#     adjacencyListTest = get_all_adjacency_list(rangesTest)\n",
    "# else:\n",
    "#     with open('data/dump/adjListTest', \"rb\") as file:\n",
    "#         adjacencyListTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0b9fb93",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# file_path = 'embed/u_prime_CNNBiLSTM_test.pkl'\n",
    "\n",
    "# # Load the list from the file using pickle\n",
    "# with open(file_path, 'rb') as file:\n",
    "#     contextualEmbeddingsTest = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29deca27",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# edgeIndicesTest = get_all_adjacency_list(rangesTest)\n",
    "# edgeTypesTest = get_all_edge_type_list(edgeIndicesTest, encodedSpeakersTest)\n",
    "# edgeIndicesTest = get_all_adjacency_list(rangesTest, key=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e5796",
   "metadata": {},
   "source": [
    "<h4> Creating graph features from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0634150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO repeat the one above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36c94f",
   "metadata": {},
   "source": [
    "<h3> Get GAT output from each set of data (DISCONTINUED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f024704",
   "metadata": {},
   "source": [
    "<h4> Instantiating the GAT (1st implementation) for 1 sample train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cacafa9a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# num_in_features = len(contextualEmbeddingsTrain[0][0])\n",
    "# num_out_features = len(contextualEmbeddingsTrain[0][0])\n",
    "# num_heads = 4\n",
    "# num_edge_types = 3\n",
    "# gat_layer = GATLayerWithEdgeType(num_in_features, num_out_features, num_heads, num_edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98a8775c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# i = 0  # dialogue id\n",
    "# relationalEmbedding, attentionCoef = gat_layer((contextualEmbeddingsTrain[i], edgeIndicesTrain[i]), edgeTypesTrain[i])\n",
    "# print(\"h_prime shape: \", relationalEmbedding.shape, \"attention_coef shape: \", attentionCoef.shape)\n",
    "\n",
    "# targetNodes = edgeIndicesTrain[i][1].tolist()\n",
    "\n",
    "# sample = {}\n",
    "# sampleEdgetypes = []\n",
    "\n",
    "# for target_i in sorted(set(targetNodes)):\n",
    "#     sample[target_i] = []\n",
    "\n",
    "# for targetNode, idx in zip(targetNodes, range(len(targetNodes))):\n",
    "#     sample[targetNode].append([idx, relationalEmbedding[targetNode][idx].tolist()])\n",
    "\n",
    "# listEdgeIdxTrain, inferredEdgeTypes = get_inferred_edgetypes_GAT(sample, edgeTypesTrain[i])\n",
    "# sampleEdgetypes.append(inferredEdgeTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc9c28c3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/dump/' + dataset_path + '/label_decoder.pkl', 'rb')\n",
    "label_decoder = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "label_decoder = list(label_decoder.values())\n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ca6003c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/labels_train.pkl\")\n",
    "\n",
    "# if checkFile is False:\n",
    "#     print(\"Please run the contextEncoder1 notebook to save the label file\")\n",
    "# else:\n",
    "#     file = open('data/dump/labels_train.pkl', 'rb')\n",
    "#     y_train = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c02f789e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# y_train[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6f3e8e1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# checkFile = os.path.isfile(\"data/dump/labels_test.pkl\")\n",
    "\n",
    "# if checkFile is False:\n",
    "#     print(\"Please run the contextEncoder2 notebook to save the label file\")\n",
    "# else:\n",
    "#     file = open('data/dump/labels_test.pkl', 'rb')\n",
    "#     y_test = pickle.load(file)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223125",
   "metadata": {},
   "source": [
    "<h5>Unsupervised Visualizarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1a41a39",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming h_prime contains the node embeddings\n",
    "# utt_size = 13\n",
    "# labels = torch.tensor(y_train[:utt_size + 1])\n",
    "\n",
    "# cherrypicked_nodes = []\n",
    "# for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#     cherrypicked_nodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "# cherrypicked_nodes = torch.tensor(cherrypicked_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47a97231",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# h_prime_np = cherrypicked_nodes.detach().numpy()\n",
    "\n",
    "# # Perform dimensionality reduction using t-SNE\n",
    "# tsne = TSNE(n_components=3, perplexity=5, random_state=42)\n",
    "# h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# # Plot the node embeddings with different colors for each label\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#     indices = (labels == label).nonzero().squeeze()\n",
    "#     plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "# plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "# plt.xlabel('Dimension 1', color=\"white\")\n",
    "# plt.ylabel('Dimension 2', color=\"white\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8043a6a",
   "metadata": {},
   "source": [
    "<h4> Now get new representations of all train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "588776d9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # filePath = data/dump/h_prime_BERT-GAT_train.pkl\n",
    "# #            data/dump/h_prime_BERT-GAT_test.pkl\n",
    "# #            data/dump/h_prime_BERT-GAT_valid.pkl\n",
    "\n",
    "# def get_GAT_representation(filePath, contextualEmbeddings, edgeIndices, edgeTypes):\n",
    "# #     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-GAT_train.pkl\") #replace it with key when deployed\n",
    "#     if key:\n",
    "#         print(\"Start of getting output of 1st GAT\")\n",
    "#         allInferredEdgetypes = []\n",
    "#         listAllEdgeIdx = []\n",
    "#         cherrypickedNodes = []\n",
    "#         for dialog, dialog_id in zip(contextualEmbeddings, range(len(contextualEmbeddings))):\n",
    "#             h_prime, attention_coef = gat_layer((dialog, edgeIndices[dialog_id]), edgeTypes[dialog_id])\n",
    "#             target_nodes = edgeIndices[dialog_id][1].tolist() # first idx represents dialogue id\n",
    "\n",
    "#             sample_edgetypes = {}\n",
    "#             for i in set(target_nodes):\n",
    "#                 sample_edgetypes[i] = []\n",
    "\n",
    "#             for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "#                 sample_edgetypes[target_node].append([edge_idx, h_prime[target_node][edge_idx].tolist()])\n",
    "\n",
    "#             list_edge_idx, inferred_edgetypes = get_inferred_edgetypes_GAT(sample_edgetypes,  edgeTypes[dialog_id])\n",
    "#             listAllEdgeIdx.append(list_edge_idx)\n",
    "#             allInferredEdgetypes.append(inferred_edgetypes)\n",
    "\n",
    "#             for src_idx, edge_idx in zip(range(len(list_edge_idx)), list_edge_idx):\n",
    "#                 cherrypickedNodes.append(attention_coef[src_idx, :, edge_idx].tolist())\n",
    "\n",
    "#         cherrypickedNodes = torch.tensor(cherrypickedNodes)\n",
    "#         cherrypickedNodes.shape\n",
    "#         print(\"End of getting output of 1st GAT\")\n",
    "\n",
    "#         pickle.dump([cherrypickedNodes, allInferredEdgetypes],\n",
    "#                     open(filePath, 'wb'))\n",
    "\n",
    "#     else:\n",
    "#         file = open(filePath, 'rb')\n",
    "#         cherrypickedNodes, allInferredEdgetypes = pickle.load(file)\n",
    "#         file.close()\n",
    "\n",
    "#     return cherrypickedNodes, allInferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2ef42f2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # train data\n",
    "# cherrypickedNodesTrain, allInferredEdgetypesTrain = get_GAT_representation(\n",
    "#                                                     \"embed/h_prime_CNNBiLSTM-GAT_train.pkl\",\n",
    "#                                                     contextualEmbeddingsTrain,\n",
    "#                                                     edgeIndicesTrain,\n",
    "#                                                     edgeTypesTrain)\n",
    "# # only save the pickle data for test and validation\n",
    "# _, _ = get_GAT_representation(\"embed/h_prime_CNNBiLSTM-GAT_test.pkl\",\n",
    "#                         contextualEmbeddingsTest,\n",
    "#                         edgeIndicesTest,\n",
    "#                         edgeTypesTest)\n",
    "# # TODO add valid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de95f55",
   "metadata": {},
   "source": [
    "<h5> Visualize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e56aa1c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# labels = torch.tensor(trainLabels)\n",
    "# h_prime_np = cherrypickedNodesTrain.detach().numpy() (discontinued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "548c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dcd5bd42",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# if runTSNE:\n",
    "#     # List of perplexity values to loop over\n",
    "#     perplexity_values = [30, 100]\n",
    "\n",
    "#     # Loop over each perplexity value\n",
    "#     for perplexity in perplexity_values:\n",
    "#         # Initialize t-SNE with the current perplexity value\n",
    "#         tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "#         # Fit and transform the data using t-SNE\n",
    "#         h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "#         # Plot the node embeddings with different colors for each label\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "#             indices = (labels == label).nonzero().squeeze()\n",
    "#             plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "#         plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "#         plt.xlabel('Dimension 1', color=\"white\")\n",
    "#         plt.ylabel('Dimension 2', color=\"white\")\n",
    "#         plt.legend()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caea68",
   "metadata": {},
   "source": [
    "<h4> Analyze the edgetypes of all train nodes in the context of a dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c236b455",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming `all_inferred_edgetypes` and `y_train` are defined\n",
    "# df_eda = pd.DataFrame(\n",
    "#     {'edgetype': flatten_extend(allInferredEdgetypesTrain),\n",
    "#      'label': y_train,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46b7f357",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Assuming `df_eda` and `CrosstabResult` are defined\n",
    "# CrosstabResult = pd.crosstab(index=df_eda['edgetype'], columns=df_eda['label'])\n",
    "\n",
    "# print(\"Crosstab Result:\")\n",
    "# print(CrosstabResult)\n",
    "# print()\n",
    "\n",
    "# # Performing Chi-squared test\n",
    "# ChiSqResult = chi2_contingency(CrosstabResult)\n",
    "\n",
    "# # P-Value is the Probability of H0 being True\n",
    "# # If P-Value > 0.05 then only we Accept the assumption(H0)\n",
    "# # H0: The variables are not correlated with each other.\n",
    "\n",
    "# print('The P-Value of the Chi-Squared Test is:', ChiSqResult[1])\n",
    "\n",
    "# if ChiSqResult[1] > 0.05:\n",
    "#     print(\"Variables are not correlated with each other\")\n",
    "# else:\n",
    "#     print(\"Two variables are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb0a7",
   "metadata": {},
   "source": [
    "<h3> Get EGAT output from each set of data (train, test, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60061c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "egat = EGATConv(in_node_feats=len(contextualEmbeddingsTrain[0][0]),\n",
    "                    in_edge_feats=3,\n",
    "                    out_node_feats=64,\n",
    "                    out_edge_feats=3,\n",
    "                    num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc9167ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EGAT_representations(filePath, contextualEmbeddings, edgeIndices, edgeTypes, ranges):\n",
    "#     checkFile = os.path.isfile(\"data/dump/h_prime_BERT-EGAT_train.pkl\")\n",
    "    if key:\n",
    "#         print(\"Start of getting output of 2nd GAT\")\n",
    "        inferredEdgetypes = []\n",
    "        allNodeFeats = []\n",
    "\n",
    "        # Iterate over each dialogue\n",
    "        for dialog_id in tqdm(range(len(edgeIndices)), desc=\"Encoding Progress\", unit=\"batch\"):\n",
    "            startIdx, endIdx = ranges[dialog_id][0], ranges[dialog_id][1]\n",
    "            # Create a DGL graph\n",
    "            graph = dgl.graph((edgeIndices[dialog_id][0], edgeIndices[dialog_id][1]))\n",
    "\n",
    "            # Get one-hot encoded edge features\n",
    "            edge_feats = get_ohe(edgeTypes[dialog_id])\n",
    "\n",
    "            # Get outputs from the second GAT layer\n",
    "            egat_output = egat(graph, contextualEmbeddings[startIdx: endIdx+1], edge_feats)\n",
    "            new_node_feats, new_edge_feats = egat_output\n",
    "\n",
    "            # Compute mean edge features\n",
    "            mean_edge_feats = new_edge_feats.mean(dim=1)\n",
    "            allNodeFeats.append(new_node_feats.mean(dim=1).tolist())\n",
    "\n",
    "            # Prepare edge features for inference\n",
    "            target_nodes = edgeIndices[dialog_id][1].tolist()\n",
    "            sample_edgetypes = {}\n",
    "            for i in set(target_nodes):\n",
    "                sample_edgetypes[i] = []\n",
    "            for target_node, edge_idx in zip(target_nodes, range(len(target_nodes))):\n",
    "                sample_edgetypes[target_node].append([edge_idx, \n",
    "                                                      mean_edge_feats[edge_idx].tolist()])\n",
    "\n",
    "            # Infer edge types\n",
    "            sample_edgetypes = get_inferred_edgetypes_EGAT(sample_edgetypes, edgeTypes[dialog_id], \n",
    "                                                           len(contextualEmbeddings[startIdx: endIdx+1]), \n",
    "                                                           dialog_id)\n",
    "            inferredEdgetypes.append(sample_edgetypes)\n",
    "\n",
    "        # Flatten and convert node features to tensor\n",
    "        allNodeFeats = torch.tensor(flatten_extend(allNodeFeats))\n",
    "\n",
    "#         print(\"End of getting output of 2nd GAT\")\n",
    "\n",
    "        # Save the data to a pickle file\n",
    "        pickle.dump([allNodeFeats, inferredEdgetypes], open(filePath, 'wb'))\n",
    "    else:\n",
    "        # Load data from the exiflatten_extendsting pickle file\n",
    "        file = open(filePath, 'rb')\n",
    "        all_node_feats, inferredEdgetypes = pickle.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return allNodeFeats, inferredEdgetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0afe0b4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# tensor_squeezedTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a62f0ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 768])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (contextualEmbeddingsTrain)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "938645cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Progress: 100%|███████████████████████████████████████████████████████| 2160/2160 [00:11<00:00, 185.49batch/s]\n",
      "Encoding Progress: 100%|█████████████████████████████████████████████████████████| 577/577 [00:02<00:00, 240.99batch/s]\n",
      "Encoding Progress: 100%|█████████████████████████████████████████████████████████| 270/270 [00:01<00:00, 258.50batch/s]\n"
     ]
    }
   ],
   "source": [
    "contextualEmbeddingsTrain_stacked = torch.cat(contextualEmbeddingsTrain, dim=0)\n",
    "allNodeFeatsTrain, inferredEdgetypesTrain = get_EGAT_representations(\n",
    "                                        \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_train.pkl\",\n",
    "                                        contextualEmbeddingsTrain_stacked,\n",
    "                                        edgeIndicesTrain,\n",
    "                                        edgeTypesTrain,\n",
    "                                        rangesTrain\n",
    "                                )\n",
    "\n",
    "contextualEmbeddingsTest_stacked = torch.cat(contextualEmbeddingsTest, dim=0)\n",
    "_, _ = get_EGAT_representations(\n",
    "        \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_test.pkl\",\n",
    "        contextualEmbeddingsTest_stacked,\n",
    "        edgeIndicesTest,\n",
    "        edgeTypesTest, \n",
    "        rangesTest\n",
    ")\n",
    "\n",
    "contextualEmbeddingsDev_stacked = torch.cat(contextualEmbeddingsDev, dim=0)\n",
    "_, _ = get_EGAT_representations(\n",
    "        \"embed/\" + dataset_path + \"/h_prime_BERT-EGAT_dev.pkl\",\n",
    "        contextualEmbeddingsDev_stacked,\n",
    "        edgeIndicesDev,\n",
    "        edgeTypesDev, \n",
    "        rangesDev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d83d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda2 = pd.DataFrame(\n",
    "    {'edgetype': flatten_extend(inferredEdgetypesTrain),\n",
    "     'label': trainLabels,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf36e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosstab Result:\n",
      " label        0    1    2     3     4    5     6\n",
      "edgetype                                       \n",
      "0            9    0    1     6     8    3     0\n",
      "1         1491  364  337  2304  5946  873  1490\n",
      "2            0    0    0     2     6    0     0\n",
      "The P-Value of the ChiSq Test is: 0.03765198295764648\n",
      "Two variables are correlated\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from your data (df_eda2)\n",
    "# Assuming df_eda2 is already defined\n",
    "\n",
    "# Crosstabulation\n",
    "CrosstabResult2 = pd.crosstab(index=df_eda2['edgetype'], columns=df_eda2['label'])\n",
    "print(\"Crosstab Result:\\n\", CrosstabResult2)\n",
    "\n",
    "# Performing Chi-squared test\n",
    "ChiSqResult2 = chi2_contingency(CrosstabResult2)\n",
    "\n",
    "# Print the p-value of the Chi-squared test\n",
    "print('The P-Value of the ChiSq Test is:', ChiSqResult2[1])\n",
    "\n",
    "# Interpret the p-value\n",
    "if ChiSqResult2[1] > 0.05:\n",
    "    print(\"Variables are not correlated with each other\")\n",
    "else:\n",
    "    print(\"Two variables are correlated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213c68a",
   "metadata": {},
   "source": [
    "Testing on 1 dialog data before scaling up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "84641d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Node Features Shape: torch.Size([14, 4, 64])\n",
      "New Edge Features Shape: torch.Size([50, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "dialog_id = 0\n",
    "\n",
    "# Create a DGL graph\n",
    "graph = dgl.graph((edgeIndicesTrain[dialog_id][0], edgeIndicesTrain[dialog_id][1]))\n",
    "\n",
    "# Obtain one-hot encoded edge features\n",
    "edge_feats = get_ohe(edgeTypesTrain[dialog_id])\n",
    "\n",
    "# Pass the graph, node representations, and edge features through the EGAT model\n",
    "contextualEmbeddingsTrain_stacked = torch.cat(contextualEmbeddingsTrain, dim=0)\n",
    "newNodeFeats, newEdgeFeats = egat(graph, contextualEmbeddingsTrain_stacked[0:14], edge_feats)\n",
    "\n",
    "# Print the shapes of the new node and edge features\n",
    "print(\"New Node Features Shape:\", newNodeFeats.shape)\n",
    "print(\"New Edge Features Shape:\", newEdgeFeats.shape)\n",
    "\n",
    "# Calculate the mean of node features along the second dimension (number of nodes)\n",
    "h_prime_mean = newNodeFeats.mean(dim=1)\n",
    "\n",
    "# Assuming you want to select only a subset of labels for visualization\n",
    "utt_size = 13\n",
    "labels = torch.tensor(trainLabels[:utt_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0bbcf4d9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "h_prime_np = h_prime_mean.detach().numpy()\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "\n",
    "# Plot the node embeddings with different colors for each label\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "    indices = (labels == label).nonzero().squeeze()\n",
    "    plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "plt.title('Node Embeddings Visualization (t-SNE)', color=\"white\")\n",
    "plt.xlabel('Dimension 1', color=\"white\")\n",
    "plt.ylabel('Dimension 2', color=\"white\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTSNE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a250c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if runTSNE:\n",
    "    # Convert tensor to numpy array\n",
    "    h_prime_np = allNodeFeatsTrain.detach().numpy()\n",
    "    labels = torch.tensor(trainLabels)\n",
    "    \n",
    "    # List of perplexity values to loop over\n",
    "    perplexity_values = [30, 100]\n",
    "\n",
    "    # Loop over each perplexity value\n",
    "    for perplexity in perplexity_values:\n",
    "        # Initialize t-SNE with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\n",
    "        # Fit and transform the data using t-SNE\n",
    "        h_prime_tsne = tsne.fit_transform(h_prime_np)\n",
    "        print(h_prime_tsne.shape)\n",
    "        \n",
    "        # Plot the node embeddings with different colors for each label\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for label, emotion in zip(range(len(label_decoder)), label_decoder): \n",
    "            indices = (labels == label).nonzero().squeeze()\n",
    "            plt.scatter(h_prime_tsne[indices, 0], h_prime_tsne[indices, 1], label=f'{emotion}')\n",
    "        plt.title(f'Node Embeddings Visualization (t-SNE) - Perplexity {perplexity}', color=\"white\")\n",
    "        plt.xlabel('Dimension 1', color=\"white\")\n",
    "        plt.ylabel('Dimension 2', color=\"white\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
